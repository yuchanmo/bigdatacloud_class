{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [[HW 1_1 SparkSQL 숙제]]\n",
    "\n",
    "# HDFS 실행 (MASTER_IP:50070로 실행 확인)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting namenodes on [master]\n",
      "master: namenode is running as process 11617.  Stop it first.\n",
      "Starting datanodes\n",
      "worker2: datanode is running as process 9439.  Stop it first.\n",
      "worker1: datanode is running as process 9439.  Stop it first.\n",
      "worker3: datanode is running as process 9422.  Stop it first.\n",
      "Starting secondary namenodes [master]\n",
      "master: secondarynamenode is running as process 11867.  Stop it first.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "start-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia data가 HDFS에 올라와 있는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ls: `hdfs://master:9000/wiki': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# List the uploaded files in the HDFS directory\n",
    "hdfs dfs -ls \"hdfs://master:9000/wiki\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 만약 올라와 있지 않다면, 데이터 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Upload the files from the local disk to HDFS\n",
    "hdfs dfs -put \"/home/ubuntu/spark_inputs/\" hdfs://master:9000/wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (혹시 실수로 데이터 업로드 두번 하셨다면, 다음 커맨드를 사용해서 중복 데이터를 제거해주세요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# hdfs dfs -rm -R \"hdfs://master:9000/wiki/spark_inputs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 스파크 분산환경 셋팅 (MASTER_IP:8080으로 실행 확인)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting org.apache.spark.deploy.master.Master, logging to /home/ubuntu/spark/logs/spark-ubuntu-org.apache.spark.deploy.master.Master-1-master.out\n",
      "[1] 09:37:23 [SUCCESS] worker1\n",
      "starting org.apache.spark.deploy.worker.Worker, logging to /home/ubuntu/spark/logs/spark-ubuntu-org.apache.spark.deploy.worker.Worker-1-worker1.out\n",
      "[2] 09:37:23 [SUCCESS] worker3\n",
      "starting org.apache.spark.deploy.worker.Worker, logging to /home/ubuntu/spark/logs/spark-ubuntu-org.apache.spark.deploy.worker.Worker-1-worker3.out\n",
      "[3] 09:37:23 [SUCCESS] worker2\n",
      "starting org.apache.spark.deploy.worker.Worker, logging to /home/ubuntu/spark/logs/spark-ubuntu-org.apache.spark.deploy.worker.Worker-1-worker2.out\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo -e 'worker1\\nworker2\\nworker3' > /home/ubuntu/spark_scripts/workers\n",
    "/home/ubuntu/spark_scripts/start_cluster.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 스파크 실행 (MASTER_IP:4040으로 실행 확인)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "\n",
    "# Required to import pyspark\n",
    "findspark.init('/home/ubuntu/spark')\n",
    "\n",
    "import pyspark\n",
    "\n",
    "# Set executor configurations\n",
    "sparkconf = pyspark.SparkConf().set('spark.executor.memory', '2g')\n",
    "\n",
    "# Deploy Spark executors!!\n",
    "ss = pyspark.sql.SparkSession.builder.appName(\"DS2\").master(\"spark://master:7077\").config(conf=sparkconf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 스파크 Dataframe 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SparkContext for mapreduce (not SparkSession)\n",
    "sc = ss.sparkContext\n",
    "\n",
    "# Read the data from HDFS\n",
    "lines = sc.textFile(\"hdfs://master:9000/wiki\")\n",
    "\n",
    "# Split each 'line' into columns\n",
    "columns = lines.map(lambda line: tuple(line.split(\" \")))\n",
    "\n",
    "# Create a Spark DataFrame (equivalent of a 'SQL table' in Spark)\n",
    "df = ss.createDataFrame(columns, ['project', 'title', 'count', 'size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 숙제 SQL query 실행 및 결과물 콘솔에 프린트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----+\n",
      "|project|               title|count|\n",
      "+-------+--------------------+-----+\n",
      "|     de|Wikipedia:Auskunf...|  900|\n",
      "|     de|Spezial:Beobachtu...|  882|\n",
      "|     de|Spezial:Beobachtu...|  804|\n",
      "+-------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.createOrReplaceTempView(\"....\")\n",
    "# selected = ss.sql(\"....\")\n",
    "# selected.show()\n",
    "df.createOrReplaceTempView('wiki') # create temp view\n",
    "no1 = ss.sql(\"select project,title,count \\\n",
    "            from wiki \\\n",
    "            where project='de' and title<>'Woodkid' and count >=800 and count<1000 \\\n",
    "            order by count desc\")\n",
    "no1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|           title|owner|\n",
      "+----------------+-----+\n",
      "|         Woodkid| Lila|\n",
      "|             Sia| Jane|\n",
      "|Ryuichi_Sakamoto|  Sam|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ownertable_raw = [('Woodkid','Lila'),('Sia','Jane'),('Ryuichi_Sakamoto','Sam')]\n",
    "ownerrdd = sc.parallelize(ownertable_raw)\n",
    "ownerdf = ss.createDataFrame(ownerrdd,['title','owner'])\n",
    "ownerdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|owner|avg_count|\n",
      "+-----+---------+\n",
      "| Lila|      2.3|\n",
      "|  Sam|     14.0|\n",
      "| Jane|     10.0|\n",
      "+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ownerdf.createOrReplaceTempView('owner')\n",
    "no2 = ss.sql('select owner,avg(count) as avg_count\\\n",
    "             from wiki\\\n",
    "             natural join owner\\\n",
    "             group by owner')\n",
    "no2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
