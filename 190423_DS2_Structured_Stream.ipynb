{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DS2_Structured_Stream.ipynb의 사본","version":"0.3.2","provenance":[{"file_id":"1o7FeyDCBy_xP1NKD-73ako0wcEYSocYD","timestamp":1556002094275},{"file_id":"148Wx-KfiLP2b0_ImpQvLPedEOcC-AK5X","timestamp":1539321816893}],"collapsed_sections":[]},"kernelspec":{"name":"python2","display_name":"Python 2"}},"cells":[{"metadata":{"id":"857s7aGUv88v","colab_type":"text"},"cell_type":"markdown","source":["Copyright (C) 2019 Software Platform Lab, Seoul National University\n","\n","Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."]},{"metadata":{"id":"7BZ8CyFiv_iD","colab_type":"text"},"cell_type":"markdown","source":["# Part 0: Environment setup"]},{"metadata":{"id":"J-TdhkUDbGfc","colab_type":"code","outputId":"48852980-f95d-42b7-869d-ba22ab6751d1","executionInfo":{"status":"ok","timestamp":1556002154843,"user_tz":-540,"elapsed":42408,"user":{"displayName":"yuchan Micky","photoUrl":"","userId":"10387316815296127801"}},"colab":{"base_uri":"https://localhost:8080/","height":500}},"cell_type":"code","source":["# Setting up spark\n","!rm -rf /content/*\n","!apt-get update\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!wget -q https://archive.apache.org/dist/spark/spark-2.3.2/spark-2.3.2-bin-hadoop2.7.tgz\n","!tar -xf /content/spark-2.3.2-bin-hadoop2.7.tgz\n","!pip install -q findspark\n","# Download necessary dependency file for Kafka\n","!wget -q http://central.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.11/2.3.2/spark-sql-kafka-0-10_2.11-2.3.2.jar\n","!wget -q http://central.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-10-assembly_2.11/2.3.2/spark-streaming-kafka-0-10-assembly_2.11-2.3.2.jar\n","!ls /content/"],"execution_count":1,"outputs":[{"output_type":"stream","text":["\r0% [Working]\r            \rIgn:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","\r0% [Connecting to archive.ubuntu.com (91.189.88.149)] [Connecting to security.u\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,609 B]\n","Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n","Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Hit:6 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Get:8 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [60.4 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Get:11 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n","Get:13 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Get:15 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,639 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,103 kB]\n","Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [301 kB]\n","Get:18 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [410 kB]\n","Get:19 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [788 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [7,238 B]\n","Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [752 kB]\n","Get:22 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [4,171 B]\n","Fetched 5,335 kB in 4s (1,192 kB/s)\n","Reading package lists... Done\n","spark-2.3.2-bin-hadoop2.7\n","spark-2.3.2-bin-hadoop2.7.tgz\n","spark-sql-kafka-0-10_2.11-2.3.2.jar\n","spark-streaming-kafka-0-10-assembly_2.11-2.3.2.jar\n"],"name":"stdout"}]},{"metadata":{"id":"vzzBCUnufKQs","colab_type":"code","colab":{}},"cell_type":"code","source":["# Setting the environment variable\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.2-bin-hadoop2.7\"\n","os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--jars /content/spark-sql-kafka-0-10_2.11-2.3.2.jar,/content/spark-streaming-kafka-0-10-assembly_2.11-2.3.2.jar pyspark-shell\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"vm4P6Vyqjcob","colab_type":"text"},"cell_type":"markdown","source":["# Part 1: Running a simple wordcount query"]},{"metadata":{"id":"LNPNeB_-t5Yp","colab_type":"text"},"cell_type":"markdown","source":["We will implement a simple continuous wordcount query. This query will\n","* Read the text sentence from a Kafka\n","* Split the sentence into words\n","* Continuously aggregate the counts for each word\n","\n","Firstly, we need to start from making a simple TCP server on the master server which produces random sentences to its clients. In this class, we will use `nc (netcat)` program. You need to setup your own TCP server by following the processes\n","\n","* Open the new terminal from jupyter\n","* Start a TCP server by entering \"nc -lk 20332\"\n","* Enter arbitrary texts to send events\n","\n","After running the simple TCP server, run the scripts below. The query will continuously run in a background thread."]},{"metadata":{"id":"xukqxaea84jW","colab_type":"code","colab":{}},"cell_type":"code","source":["## socket 으로 전달한 데이터는 peer to peer 라 한명만 \n","import findspark\n","import os\n","\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import *\n","\n","spark = SparkSession.builder \\\n","    .appName(\"DS2\") \\\n","    .master('local[*]') \\\n","    .getOrCreate()\n","\n","# Get the text data stream from TCP server\n","lines = spark \\\n","    .readStream \\\n","    .format(\"socket\") \\\n","    .option(\"host\", \"147.46.216.122\") \\\n","    .option(\"port\", 20332) \\\n","    .load()\n","\n","#explode and split are SQL functions. \n","#Both operate on SQL Column. \n","#split takes a Java regular expression as a second argument. \n","#If you want to separate data on arbitrary whitespace\n","#you'll need something like this:\n","words = lines.select(\n","    explode(\n","        split(lines.value, \" \")\n","    ).alias(\"word\")\n",")\n","\n","wordCounts = words.groupBy(\"word\").count()\n","\n","#outputMode('complete') : 누적결과를 보여줌\n","# streaming 에서 중간중간 \n","# (1)차이나는 부분만 보여주는 방법\n","# (2)누적된 결과를 보여주는 방법\n","query = wordCounts \\\n","    .writeStream \\\n","    .queryName(\"wordcount_simple\") \\\n","    .outputMode(\"complete\") \\\n","    .format(\"memory\") \\\n","    .start()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tyTwaYpG_S2n","colab_type":"code","colab":{}},"cell_type":"code","source":["#"],"execution_count":0,"outputs":[]},{"metadata":{"id":"v_s5-HqM-ch3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":36},"outputId":"ad8acb03-5f53-4c45-ec07-f7a9ec2a42ab","executionInfo":{"status":"ok","timestamp":1556002839518,"user_tz":-540,"elapsed":695,"user":{"displayName":"yuchan Micky","photoUrl":"","userId":"10387316815296127801"}}},"cell_type":"code","source":["wordCounts"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DataFrame[word: string, count: bigint]"]},"metadata":{"tags":[]},"execution_count":20}]},{"metadata":{"id":"KMf0lR4bF0_l","colab_type":"text"},"cell_type":"markdown","source":["The running query will process the incoming text data from the TCP server. You may see the result by running the script below. Execute the script repeatedly, and you will see the evolving result as the data arrives."]},{"metadata":{"id":"phjKn1FGFilV","colab_type":"code","outputId":"f2ada169-292c-4016-d642-f126353dcdf6","executionInfo":{"status":"ok","timestamp":1556003128651,"user_tz":-540,"elapsed":686,"user":{"displayName":"yuchan Micky","photoUrl":"","userId":"10387316815296127801"}},"colab":{"base_uri":"https://localhost:8080/","height":110}},"cell_type":"code","source":["result = spark.table(\"wordcount_simple\")\n","result.show()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["+----+-----+\n","|word|count|\n","+----+-----+\n","+----+-----+\n","\n"],"name":"stdout"}]},{"metadata":{"id":"PVyLyZ1KPiWy","colab_type":"text"},"cell_type":"markdown","source":["You may stop the running query with the following script"]},{"metadata":{"id":"AAlapIZvRjUh","colab_type":"code","colab":{}},"cell_type":"code","source":["query.stop()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AgTNYiQJUn-3","colab_type":"text"},"cell_type":"markdown","source":["# Part 2: Running a stream application from Kafka source\n","\n","Apache Kafka is a distributed streaming platform which supports messaging, processing, and storing of the stream data. In this practice session, we will focus on leveraging Kafka as a message brokering system.\n","\n","Kafka supports high-throughput & fault-tolerant messaging via publish-subscribe model. In publish-subscribe model, stream events are managed in **topics**. A **Producer** consistently generates a data, whereas **Consumer** receives the data events. Each topic is partitioned into multiple \"partitions\", and partitions are distributed and stored in the secondary storage to guarantee fault tolerance.\n","\n","As we can guess from the information above, we need the server address and topic name to fetch the data from a Kafka broker. Kafka server and producers are already set up by TAs. We will review the Producer code firstly.\n","\n","After revewing the code, we will implement the same word count application from the Kafka source. The broker address is **147.46.216.122:9092** and the topic is **wc**."]},{"metadata":{"id":"FZIjjH08ZAiy","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"Kdh3ei3hU7Fa","colab_type":"code","colab":{}},"cell_type":"code","source":["import findspark\n","import os\n","\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import *\n","\n","spark = SparkSession.builder \\\n","    .appName(\"DS2\") \\\n","    .master('local[*]') \\\n","    .getOrCreate()\n","    \n","# Get the text data stream from Kafka\n","lines = spark \\\n","    .readStream \\\n","    .format(\"kafka\") \\\n","    .option(\"kafka.bootstrap.servers\", \"147.46.216.122:9092\") \\\n","    .option(\"subscribe\", \"wc\") \\\n","    .load()\n","    \n","words = lines.select(\n","    explode(\n","        split(lines.value, \" \")\n","    ).alias(\"word\")\n",")\n","\n","wordCounts = words.groupBy(\"word\").count()\n","\n","query = wordCounts \\\n","    .writeStream \\\n","    .queryName(\"wordcount_kafka\") \\\n","    .outputMode(\"complete\") \\\n","    .format(\"memory\") \\\n","    .start()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"A2bnqbgJX8QQ","colab_type":"text"},"cell_type":"markdown","source":["You may see the result and stop the query with the following scripts."]},{"metadata":{"id":"uo13lpL3YAuR","colab_type":"code","outputId":"bcda87d6-8e77-4725-de9b-63cc13a85bd5","executionInfo":{"status":"ok","timestamp":1556003394758,"user_tz":-540,"elapsed":2418,"user":{"displayName":"yuchan Micky","photoUrl":"","userId":"10387316815296127801"}},"colab":{"base_uri":"https://localhost:8080/","height":388}},"cell_type":"code","source":["result = spark.table(\"wordcount_kafka\")\n","result.show()"],"execution_count":13,"outputs":[{"output_type":"stream","text":["+--------+-----+\n","|    word|count|\n","+--------+-----+\n","|    \"DS2|   11|\n","|practice|   12|\n","|      is|   11|\n","|    day\"|   15|\n","|   \"Have|   15|\n","|session\"|   12|\n","|    fun\"|   11|\n","|\"Welcome|   12|\n","|   class|   11|\n","|\"Session|   13|\n","|       a|   15|\n","|    nice|   15|\n","|expired\"|   13|\n","|      to|   12|\n","|     DS2|   12|\n","+--------+-----+\n","\n"],"name":"stdout"}]},{"metadata":{"id":"-sFDvpyAZA08","colab_type":"code","colab":{}},"cell_type":"code","source":["query.stop()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ehQ5l-_-Ze1e","colab_type":"text"},"cell_type":"markdown","source":["# Quiz 0: Simple Filtered Aggregation\n","\n","In the following cell, implement the word count example which gets the data from the **\"wc\"** topic according to the following condition.\n","* Do not count the words whose lengths are shorter than 3\n","\n","Hint: Use *length()* function"]},{"metadata":{"id":"LPhBE8prDxik","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"VoOXB_r9fBIR","colab_type":"code","colab":{}},"cell_type":"code","source":["import findspark\n","import os\n","\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import *\n","\n","spark = SparkSession.builder \\\n","    .appName(\"DS2\") \\\n","    .master('local[*]') \\\n","    .getOrCreate()\n","    \n","# Get the text data stream from Kafka\n","lines = spark \\\n","    .readStream \\\n","    .format(\"kafka\") \\\n","    .option(\"kafka.bootstrap.servers\", \"147.46.216.122:9092\") \\\n","    .option(\"subscribe\", \"wc\") \\\n","    .load()\n","    \n","# words = lines.select(\n","#     explode(\n","#         split(lines.value, \" \")\n","#     ).alias(\"word\")\n","# )\n","\n","words = lines.select(\n","   \n","        split(lines.value, \" \").alias(\"word\")\n",")\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-IKZpwDwDyuE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":349},"outputId":"3dab8e5f-8627-47eb-d74b-1221d3d5c1cd","executionInfo":{"status":"error","timestamp":1556004503576,"user_tz":-540,"elapsed":672,"user":{"displayName":"yuchan Micky","photoUrl":"","userId":"10387316815296127801"}}},"cell_type":"code","source":["words.writeStream \\\n","    .queryName(\"quiz_10\") \\\n","    .outputMode(\"complete\") \\\n","    .format(\"memory\") \\\n","    .start()"],"execution_count":39,"outputs":[{"output_type":"error","ename":"AnalysisException","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mAnalysisException\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-39-778487124ae8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"quiz_10\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"complete\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/spark-2.3.2-bin-hadoop2.7/python/pyspark/sql/streaming.pyc\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m    895\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-2.3.2-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-2.3.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAnalysisException\u001b[0m: u'Complete output mode not supported when there are no streaming aggregations on streaming DataFrames/Datasets;;\\nProject [split(cast(value#4159 as string),  ) AS word#4172]\\n+- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@1361ba1a, kafka, Map(subscribe -> wc, kafka.bootstrap.servers -> 147.46.216.122:9092), [key#4158, value#4159, topic#4160, partition#4161, offset#4162L, timestamp#4163, timestampType#4164], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@20eb8366,kafka,List(),None,List(),None,Map(subscribe -> wc, kafka.bootstrap.servers -> 147.46.216.122:9092),None), kafka, [key#4151, value#4152, topic#4153, partition#4154, offset#4155L, timestamp#4156, timestampType#4157]\\n'"]}]},{"metadata":{"id":"GVc4GQ51DHw5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":367},"outputId":"8c6fbb56-1ccf-4c57-8044-a10b75f0a909","executionInfo":{"status":"error","timestamp":1556004463737,"user_tz":-540,"elapsed":707,"user":{"displayName":"yuchan Micky","photoUrl":"","userId":"10387316815296127801"}}},"cell_type":"code","source":["filteredWordCounts = words.where(length(\"word\")>=3).groupBy('word').count()\n","# Implement your code here!"],"execution_count":37,"outputs":[{"output_type":"error","ename":"AnalysisException","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mAnalysisException\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-37-737ec41d20a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfilteredWordCounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Implement your code here!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-2.3.2-bin-hadoop2.7/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   1240\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1242\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"condition should be string or Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-2.3.2-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-2.3.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAnalysisException\u001b[0m: u\"cannot resolve 'length(`word`)' due to data type mismatch: argument 1 requires (string or binary) type, however, '`word`' is of array<string> type.;;\\n'Filter (length(word#4172) >= 3)\\n+- Project [split(cast(value#4159 as string),  ) AS word#4172]\\n   +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@1361ba1a, kafka, Map(subscribe -> wc, kafka.bootstrap.servers -> 147.46.216.122:9092), [key#4158, value#4159, topic#4160, partition#4161, offset#4162L, timestamp#4163, timestampType#4164], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@20eb8366,kafka,List(),None,List(),None,Map(subscribe -> wc, kafka.bootstrap.servers -> 147.46.216.122:9092),None), kafka, [key#4151, value#4152, topic#4153, partition#4154, offset#4155L, timestamp#4156, timestampType#4157]\\n\""]}]},{"metadata":{"id":"Kq12E8QtDGQg","colab_type":"code","colab":{}},"cell_type":"code","source":["query = filteredWordCounts \\\n","    .writeStream \\\n","    .queryName(\"quiz_0\") \\\n","    .outputMode(\"complete\") \\\n","    .format(\"memory\") \\\n","    .start()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"E6vfetuafm-I","colab_type":"code","outputId":"d20933b8-a488-44f6-cec5-62886d80ac4c","executionInfo":{"status":"ok","timestamp":1556004047039,"user_tz":-540,"elapsed":6431,"user":{"displayName":"yuchan Micky","photoUrl":"","userId":"10387316815296127801"}},"colab":{"base_uri":"https://localhost:8080/","height":333}},"cell_type":"code","source":["result = spark.table(\"quiz_0\")\n","result.show()"],"execution_count":35,"outputs":[{"output_type":"stream","text":["+--------+-----+\n","|    word|count|\n","+--------+-----+\n","|    \"DS2|   15|\n","|practice|   11|\n","|    day\"|   13|\n","|   \"Have|   13|\n","|session\"|   11|\n","|    fun\"|   15|\n","|\"Welcome|   11|\n","|   class|   15|\n","|\"Session|   12|\n","|    nice|   13|\n","|expired\"|   12|\n","|     DS2|   11|\n","+--------+-----+\n","\n"],"name":"stdout"}]},{"metadata":{"id":"ogDg0GYNfosW","colab_type":"code","colab":{}},"cell_type":"code","source":["query.stop()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9IyfQL0NkmUw","colab_type":"text"},"cell_type":"markdown","source":["# Part 3: Processing JSON-formatted data\n","\n","Until now, we processed only the simple plain texts. From this time, we will process JSON-formatted data events which are widely used for data transfer. Here, we will get the json-formatted movie datasets from the kafka server. We may get the data from the **movie** topic.\n","\n","To parse json-formatted data into spark dataframe, you need to use `from_json()` function and type casting.\n","\n","The script below parses the json events to dataframe and filters out the events whose \"year\" is less than 2000."]},{"metadata":{"id":"n5w_q9J8n9Po","colab_type":"code","colab":{}},"cell_type":"code","source":["#parsing 후 데이터 처리 방법에 대해서 나올 예정\n","\n","import os\n","import findspark\n","\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import *\n","\n","## Make a spark sql session\n","spark = SparkSession.builder \\\n","    .appName(\"DS2\") \\\n","    .master(\"local[*]\") \\\n","    .getOrCreate()\n","\n","## Get the json-formatted data from Kafka stream\n","kafka_movies = spark \\\n","    .readStream \\\n","    .format(\"kafka\") \\\n","    .option(\"kafka.bootstrap.servers\", \"147.46.216.122:9092\") \\\n","    .option(\"subscribe\", \"movie\") \\\n","    .load()\n","\n","## Change the JSON events into relational tuples with string types\n","relational_movies = kafka_movies.select([get_json_object(col(\"value\").cast(\"string\"), \"$.{}\".format(c)).alias(c)\n","    for c in [\"title\", \"genre\", \"year\", \"time\"]])\n","\n","## Change the type of year from string to integer,\n","## and change the type of time from string to timestamp\n","relational_movies = relational_movies.select(col(\"title\"), col(\"genre\"),\n","        relational_movies.year.cast('integer').alias('year'),\n","        col(\"time\").cast('double').cast('timestamp')\n","    )\n","\n","## Select the movie with year < 2000\n","filtered_movies = relational_movies.select(\"*\").where(\"year >= 1900\")\n","\n","query = filtered_movies \\\n","    .writeStream \\\n","    .queryName(\"twentyfirstcentury_movies\") \\\n","    .outputMode(\"append\") \\\n","    .format(\"memory\") \\\n","    .start()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8WsS3MNWoa2m","colab_type":"code","outputId":"28042576-470b-4cce-d2b4-6555d90befc4","executionInfo":{"status":"ok","timestamp":1556004955849,"user_tz":-540,"elapsed":4664,"user":{"displayName":"yuchan Micky","photoUrl":"","userId":"10387316815296127801"}},"colab":{"base_uri":"https://localhost:8080/","height":147}},"cell_type":"code","source":["results = spark.table(\"twentyfirstcentury_movies\")\n","results.show()"],"execution_count":48,"outputs":[{"output_type":"stream","text":["+--------------------+------+----+--------------------+\n","|               title| genre|year|                time|\n","+--------------------+------+----+--------------------+\n","|          Easy Rider| Drama|1969|2019-04-23 07:35:...|\n","|Delta Force 2: Th...|Action|1990|2019-04-23 07:35:...|\n","+--------------------+------+----+--------------------+\n","\n"],"name":"stdout"}]},{"metadata":{"id":"jksiSB0YpXG5","colab_type":"code","colab":{}},"cell_type":"code","source":["query.stop()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ljUOsYxltmMW","colab_type":"text"},"cell_type":"markdown","source":["# Quiz 1: Filtered Aggregation on JSON-formatted data\n","\n","In the following cell, implement the stream application which receives the **movie** topic from the Kafka stream and filters out all the movies which does not contain the word \"the\" in their titles (cases are ignored). \n","\n","After that, count the number of movies within the filtered data stream.\n","\n","**Input**: {\"title\": \"The titanic\", \"genre\": \"drama\", ...}, {\"title\": \"Titanic\", \"genre\": \"drama, ...\"}, {\"title\": \"Flintheart Glomgold\", \"genre\": \"comedy\", ...}\n","\n","**Output**: (\"drama\", 1), (\"comedy\", 1)\n","\n","Hint: Use *lower()* function and *like* clauses with *wildcard (%)*"]},{"metadata":{"id":"mqyLcqsuv_3A","colab_type":"code","colab":{}},"cell_type":"code","source":["import os\n","import findspark\n","\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import explode\n","from pyspark.sql.functions import split\n","from pyspark.sql.functions import col, from_json, get_json_object\n","from pyspark.sql.types import *\n","\n","## Make a spark sql session\n","spark = SparkSession.builder \\\n","    .appName(\"IAB\") \\\n","    .master(\"local[*]\") \\\n","    .getOrCreate()\n","\n","## Get the json-formatted data from Kafka stream\n","kafka_movies = spark \\\n","    .readStream \\\n","    .format(\"kafka\") \\\n","    .option(\"kafka.bootstrap.servers\", \"147.46.216.122:9092\") \\\n","    .option(\"subscribe\", \"movie\") \\\n","    .load()\n","\n","## Change the JSON events into relational tuples with string types\n","relational_movies = kafka_movies.select([get_json_object(col(\"value\").cast(\"string\"), \"$.{}\".format(c)).alias(c)\n","    for c in [\"title\", \"genre\", \"year\", \"time\"]])\n","\n","## Change the type of year from string to integer,\n","## and change the type of time from string to timestamp\n","relational_movies = relational_movies.select(col(\"title\"), col(\"genre\"),\n","        relational_movies.year.cast('integer').alias('year'),\n","        col(\"time\").cast('double').cast('timestamp')\n","    )\n","\n","# Implement your code here!\n","aggregated_counts = relational_movies.where(\"lower(title) like '%the%'\") #.groupBy('genre').count()\n","query = aggregated_counts \\\n","    .writeStream \\\n","    .queryName(\"quiz_1\") \\\n","    .outputMode(\"append\") \\\n","    .format(\"memory\") \\\n","    .start()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LLT8YkmYxQMl","colab_type":"code","outputId":"aed4fb80-86d5-4640-9c19-4bc5e882bcef","executionInfo":{"status":"ok","timestamp":1556005536525,"user_tz":-540,"elapsed":1678,"user":{"displayName":"yuchan Micky","photoUrl":"","userId":"10387316815296127801"}},"colab":{"base_uri":"https://localhost:8080/","height":166}},"cell_type":"code","source":["result = spark.table(\"quiz_1\")\n","result.show()"],"execution_count":71,"outputs":[{"output_type":"stream","text":["+--------------------+---------------+----+--------------------+\n","|               title|          genre|year|                time|\n","+--------------------+---------------+----+--------------------+\n","|The Girl with the...|Drama, Thriller|2011|2019-04-23 07:45:...|\n","|            The Heat|      Buddy cop|2013|2019-04-23 07:45:...|\n","|            The Hand|         Horror|1981|2019-04-23 07:45:...|\n","+--------------------+---------------+----+--------------------+\n","\n"],"name":"stdout"}]},{"metadata":{"id":"Egf4EFLfxQpQ","colab_type":"code","colab":{}},"cell_type":"code","source":["query.stop()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rsTml2E5xzfZ","colab_type":"text"},"cell_type":"markdown","source":["# Part 4: Running a windowed stream application\n","\n","By windowing, we can continuously get the set of recent data. A time-based **sliding window** can be defined by **window size** and **sliding interval**. For example, the window of `(window size = 5 seconds, sliding interval = 1 seconds)` consistently emits the data events in recent five seconds for every one second. For the special cases when the window size and the sliding interval are same, we call them as **tumbling windows**. In structured stream, it is possible to make windows from **event times**.\n","\n","Let's make a windowed movie aggregation query, which counts the frequency of each genre within a sliding window (size = 30 seconds, interval = 5 seconds)."]},{"metadata":{"id":"RJuMcXvjzUn0","colab_type":"code","colab":{}},"cell_type":"code","source":["import os\n","import findspark\n","\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import *\n","\n","## Make a spark sql session\n","spark = SparkSession.builder \\\n","    .appName(\"DS2\") \\\n","    .master(\"local[*]\") \\\n","    .getOrCreate()\n","\n","## Get the json-formatted data from Kafka stream\n","kafka_movies = spark \\\n","    .readStream \\\n","    .format(\"kafka\") \\\n","    .option(\"kafka.bootstrap.servers\", \"147.46.216.122:9092\") \\\n","    .option(\"subscribe\", \"movie\") \\\n","    .load()\n","\n","## Change the JSON events into relational tuples with string types\n","relational_movies = kafka_movies.select([get_json_object(col(\"value\").cast(\"string\"), \"$.{}\".format(c)).alias(c)\n","    for c in [\"title\", \"genre\", \"year\", \"time\"]])\n","\n","## Change the type of year from string to integer,\n","## and change the type of time from string to timestamp\n","relational_movies = relational_movies.select(col(\"title\"), col(\"genre\"),\n","        relational_movies.year.cast('integer').alias('year'),\n","        col(\"time\").cast('double').cast('timestamp')\n","    )\n","\n","## Make windows and aggregate\n","windowed_movies = relational_movies.groupBy(\n","    window(relational_movies.time, \"30 seconds\", \"5 seconds\"),\n","    \"genre\"\n","  ).count()\n","\n","query = windowed_movies \\\n","    .writeStream \\\n","    .queryName(\"windowed_movies\") \\\n","    .outputMode(\"complete\") \\\n","    .format(\"memory\") \\\n","    .start()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zYZiXhzE0AQw","colab_type":"code","outputId":"b8b5ed97-72a9-48dc-86ca-dc4fbbb150b4","executionInfo":{"status":"ok","timestamp":1555993581816,"user_tz":-540,"elapsed":962,"user":{"displayName":"Gyewon Lee","photoUrl":"","userId":"06066203904995074978"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"cell_type":"code","source":["result = spark.table(\"windowed_movies\")\n","result.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["+--------------------+-------+-----+\n","|              window|  genre|count|\n","+--------------------+-------+-----+\n","|[2019-04-23 04:25...|Western|    1|\n","|[2019-04-23 04:25...|Western|    1|\n","|[2019-04-23 04:25...|Western|    1|\n","|[2019-04-23 04:25...|Western|    1|\n","|[2019-04-23 04:25...|Western|    1|\n","|[2019-04-23 04:26...|Western|    1|\n","+--------------------+-------+-----+\n","\n"],"name":"stdout"}]},{"metadata":{"id":"sTPXgRXY0AvE","colab_type":"code","colab":{}},"cell_type":"code","source":["query.stop()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"a3YZ_MsQo_vN","colab_type":"text"},"cell_type":"markdown","source":["# Quiz 2: Windowed aggregation by the first character\n","\n","In this quiz, you will implement a sliding-window aggregation query with *movie* stream query.\n","\n","Implement a query which counts the number of movies according to the first characters of their *titles* (cases are ignored) within a sliding window (size = 30 seconds, interval = 5 seconds)\n","\n","Example:\n","\n","**Input**: {\"title\": \"The titanic\", ...}, {\"title\": \"Avengers\", ...}, {\"title\": \"a little boy\", ...}\n","\n","**Output**: (\"t\", 1), (\"a\", 2)\n","\n","Hint: Use substring() function"]},{"metadata":{"id":"26PthwgNp56R","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":366},"outputId":"194199de-7d0c-44c0-887a-b4f11775bd7a","executionInfo":{"status":"error","timestamp":1556006878175,"user_tz":-540,"elapsed":1179,"user":{"displayName":"yuchan Micky","photoUrl":"","userId":"10387316815296127801"}}},"cell_type":"code","source":["import os\n","import findspark\n","\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import *\n","\n","## Make a spark sql session\n","spark = SparkSession.builder \\\n","    .appName(\"DS2\") \\\n","    .master(\"local[*]\") \\\n","    .getOrCreate()\n","\n","## Get the json-formatted data from Kafka stream\n","kafka_movies = spark \\\n","    .readStream \\\n","    .format(\"kafka\") \\\n","    .option(\"kafka.bootstrap.servers\", \"147.46.216.122:9092\") \\\n","    .option(\"subscribe\", \"movie\") \\\n","    .load()\n","\n","## Change the JSON events into relational tuples with string types\n","relational_movies = kafka_movies.select([get_json_object(col(\"value\").cast(\"string\"), \"$.{}\".format(c)).alias(c)\n","    for c in [\"title\", \"genre\", \"year\", \"time\"]])\n","\n","## Change the type of year from string to integer,\n","## and change the type of time from string to timestamp\n","relational_movies = relational_movies.select(col(\"title\"), col(\"genre\"),\n","        relational_movies.year.cast('integer').alias('year'),\n","        col(\"time\").cast('double').cast('timestamp')\n","    )\n","\n","first_characters = relational_movies.select('time',substring('title',0,1).alias('first_character'))\n","\n","# Implement your code here!\n","## Extract the lower-cased first characters from relational_movie\n","\n","\n","## Make windows and aggregate\n","windowed_movies = first_characters.groupBy(\n","    window(first_characters.time, \"30 seconds\", \"5 seconds\"),\n","    first_characters.first_character\n","  ).count()\n","\n","query = windowed_movies \\\n","    .writeStream \\\n","    .queryName(\"quiz_2\") \\\n","    .outputMode(\"complete\") \\\n","    .format(\"memory\") \\\n","    .start()"],"execution_count":79,"outputs":[{"output_type":"error","ename":"IllegalArgumentException","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mIllegalArgumentException\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-79-c8bc19f1fa65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m   ).count()\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindowed_movies\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"quiz_2\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"complete\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/spark-2.3.2-bin-hadoop2.7/python/pyspark/sql/streaming.pyc\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m    895\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-2.3.2-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-2.3.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIllegalArgumentException\u001b[0m: u'Cannot start query with name quiz_2 as a query with that name is already active'"]}]},{"metadata":{"id":"CU5Wunfiqp5t","colab_type":"code","outputId":"75410c9c-92ce-4dd6-96ed-1ff8ea369dd1","executionInfo":{"status":"ok","timestamp":1556006882197,"user_tz":-540,"elapsed":1634,"user":{"displayName":"yuchan Micky","photoUrl":"","userId":"10387316815296127801"}},"colab":{"base_uri":"https://localhost:8080/","height":500}},"cell_type":"code","source":["result = spark.table(\"quiz_2\")\n","result.show()"],"execution_count":80,"outputs":[{"output_type":"stream","text":["+--------------------+---------------+-----+\n","|              window|first_character|count|\n","+--------------------+---------------+-----+\n","|[2019-04-23 08:07...|              D|    3|\n","|[2019-04-23 08:07...|              S|    1|\n","|[2019-04-23 08:07...|              W|    1|\n","|[2019-04-23 08:07...|              T|    3|\n","|[2019-04-23 08:07...|              D|    2|\n","|[2019-04-23 08:07...|              S|    2|\n","|[2019-04-23 08:07...|              S|    2|\n","|[2019-04-23 08:07...|              D|    3|\n","|[2019-04-23 08:07...|              D|    3|\n","|[2019-04-23 08:07...|              S|    1|\n","|[2019-04-23 08:07...|              T|    3|\n","|[2019-04-23 08:07...|              W|    1|\n","|[2019-04-23 08:07...|              T|    3|\n","|[2019-04-23 08:07...|              D|    1|\n","|[2019-04-23 08:07...|              S|    2|\n","|[2019-04-23 08:07...|              S|    2|\n","|[2019-04-23 08:07...|              F|    1|\n","|[2019-04-23 08:07...|              F|    1|\n","|[2019-04-23 08:07...|              F|    1|\n","|[2019-04-23 08:07...|              F|    1|\n","+--------------------+---------------+-----+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"metadata":{"id":"yGAkzQaJqsGB","colab_type":"code","colab":{}},"cell_type":"code","source":["query.stop()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gtd9DPlkPDzM","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}