{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "190417_SparkSQL.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "K9JzWDhUXkh6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Software Platform Lab, Seoul National University\n",
        "\n",
        "## Colab 101\n",
        "\n",
        "Colab is a free Jupyter notebook environment by Google Research. Unlike AWS cluster (which is charged every hour it is up and running), you can run experiments on your own environment.\n",
        "\n",
        "## Colab Spark Setup"
      ]
    },
    {
      "metadata": {
        "id": "w82WXCR2VCcV",
        "colab_type": "code",
        "outputId": "4576cbe4-6022-4450-d420-7e7732637230",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless\n",
        "!curl http://mirror.cogentco.com/pub/apache/spark/spark-2.4.1/spark-2.4.1-bin-hadoop2.7.tgz --output spark-2.4.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rIgn:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.18\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.18\r0% [2 InRelease gpgv 3,609 B] [Connecting to archive.ubuntu.com] [Connecting to\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [2 InRelease gpgv 3,609 B] [Connecting to archive.ubuntu.com (91.189.88.162)\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "\r0% [2 InRelease gpgv 3,609 B] [Connecting to archive.ubuntu.com (91.189.88.162)\r                                                                               \rHit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "\r0% [2 InRelease gpgv 3,609 B] [Connecting to archive.ubuntu.com (91.189.88.162)\r                                                                               \rGet:6 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [2 InRelease gpgv 3,609 B] [Connecting to archive.ubuntu.com (91.189.88.162)\r                                                                               \rHit:7 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Fetched 252 kB in 4s (63.7 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "openjdk-8-jdk-headless is already the newest version (8u191-b12-2ubuntu0.18.04.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  220M  100  220M    0     0  29.0M      0  0:00:07  0:00:07 --:--:-- 39.2M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sSL4Z8LJHp4F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.1-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X-Pvn8rkXuHD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Wikipedia dataset sample\n",
        "\n",
        "This time we're not using HDFS to load the data. Sample data are loaded by Python code directly.\n",
        "\n",
        "The data has four fields: project, title, pageview count and size."
      ]
    },
    {
      "metadata": {
        "id": "HTYHQ4OCU4LD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wikipedia_data_sample = [\"commons.m File:Gemblong.JPG 1 9717\"\n",
        ",\"pl Beata_Tyszkiewicz 10 207378\"\n",
        ",\"en Special:RecentChangesLinked/Roswell_(TV_series) 1 14617\"\n",
        ",\"de Grafische_Benutzeroberfl%C3%A4che 1 22549\"\n",
        ",\"en Simeon_I_of_Bulgaria 5 385793\"\n",
        ",\"en Rainbow_Six_(novel) 8 122792\"\n",
        ",\"es Pediatr%C3%ADa 5 73598\"\n",
        ",\"sv Ett_uts%C3%B6kt_universum 1 9499\"\n",
        ",\"en Video_game_content_rating_system 4 112324\"\n",
        ",\"es Yuno_Gasai 2 55260\"\n",
        ",\"en File:Georg_Wilhelm_Friedrich_Hegel00.jpg 1 43395\"\n",
        ",\"en Anestia_ombrophanes 1 8881\"\n",
        ",\"et Seitse 2 84874\"\n",
        ",\"en And_I_Am_Telling_You_I%27m_Not_Going 4 85690\"\n",
        ",\"he %D7%A4%D7%A8%D7%93%D7%99%D7%92%D7%9E%D7%94 1 13887\"\n",
        ",\"zh File:Pictogram_voting_keep-green.svg 1 15106\"\n",
        ",\"sv Special:Senaste_relaterade_%C3%A4ndringar/Homestead,_Florida 1 7677\"\n",
        ",\"pt Categoria:Ambientes_de_desenvolvimento_integrado_livres 1 8151\"\n",
        ",\"de.voy Plattensee 1 43748\"\n",
        ",\"en Independent_Chip_Model 1 8938\"\n",
        ",\"en Category:Toronto_Toros_players 2 0\"\n",
        ",\"en Special:Export/Helsinki_Accords 1 19899\"\n",
        ",\"xh Special:Contributions/Kpeterzell 1 5883\"\n",
        ",\"nl 4_mei 1 0\"\n",
        ",\"no Carlos_Keller_Rueff 5 87075\"\n",
        ",\"en Special:Contributions/2.31.218.202 1 7402\"\n",
        ",\"es Placa_Yangtze 1 10329\"\n",
        ",\"de Datei:BSicon_uhKBHFe.svg 1 9786\"\n",
        ",\"en Randolph_County,_Alabama 1 21431\"\n",
        ",\"es S%C3%A9neca 3 70494\"\n",
        ",\"en Tu_Bishvat 3 56438\"\n",
        ",\"cs Radiohead 1 14325\"\n",
        ",\"es Naturaleza_sangre 1 9286\"\n",
        ",\"en Anatolia_(disambiguation) 1 7980\"\n",
        ",\"pt Queima_de_suti%C3%A3s 1 8982\"\n",
        ",\"pt Titanoboa_cerrejonensis 5 64540\"\n",
        ",\"commons.m Category:People_of_Ireland 1 19278\"\n",
        ",\"fi Matti_Inkinen 1 10138\"\n",
        ",\"ja %E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB:Esfahan_(Iran)_Emam_Mosque.JPG 1 33168\"\n",
        ",\"en Psicobloc 1 12739\"\n",
        ",\"en Macael,_Spain 1 12658\"\n",
        ",\"fa %DA%A9%D9%87%D8%AA%D9%88%DB%8C%D9%87 1 22855\"\n",
        ",\"fr Sp%C3%A9cial:Pages_li%C3%A9es/Fichier:Wiki-ezokuroten5.jpg 1 21955\"\n",
        ",\"nl Overleg_gebruiker:82.171.157.232 1 0\"\n",
        ",\"en Thomas_%26_Mack_Center 2 41010\"\n",
        ",\"en Warren_Beatty 49 2631986\"\n",
        ",\"uz Auberville 1 11401\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TG9kePIjX6A5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Spark RDD Transforms\n",
        "\n",
        "Now we'll try several Spark RDD transforms using the sample wikipedia dataset."
      ]
    },
    {
      "metadata": {
        "id": "6n8go8ojHp_E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "ss = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = ss.sparkContext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nl1fp34_UGB3",
        "colab_type": "code",
        "outputId": "c9daba67-c275-4eb3-eaf0-1265e49f3f74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "type(ss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.session.SparkSession"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "hZx1MpvJcdI1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "# Parallelize the data and split into columns\n",
        "lines = sc.parallelize(wikipedia_data_sample)\n",
        "columns = lines.map(lambda line: ((explode(line.split(\" \")))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vQKeR2F7KPGM",
        "colab_type": "code",
        "outputId": "d4fd2178-fd09-443a-b9c6-68f5e867edd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "cell_type": "code",
      "source": [
        "lines.take(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['commons.m File:Gemblong.JPG 1 9717',\n",
              " 'pl Beata_Tyszkiewicz 10 207378',\n",
              " 'en Special:RecentChangesLinked/Roswell_(TV_series) 1 14617',\n",
              " 'de Grafische_Benutzeroberfl%C3%A4che 1 22549',\n",
              " 'en Simeon_I_of_Bulgaria 5 385793']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "CwdHYDUEK4WW",
        "colab_type": "code",
        "outputId": "d6ae98e1-7f78-4723-eb7a-c12d93891597",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2595
        }
      },
      "cell_type": "code",
      "source": [
        "columns.take(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-0fa7e7ff62eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-2.4.1-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.1-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 5, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/content/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/content/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/content/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 393, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/content/spark-2.4.1-bin-hadoop2.7/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/content/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-17-7380b1f0b37b>\", line 4, in <lambda>\n  File \"/content/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 2103, in explode\n    jc = sc._jvm.functions.explode(_to_java_column(col))\nAttributeError: 'NoneType' object has no attribute '_jvm'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/content/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/content/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/content/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 393, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/content/spark-2.4.1-bin-hadoop2.7/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/content/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-17-7380b1f0b37b>\", line 4, in <lambda>\n  File \"/content/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 2103, in explode\n    jc = sc._jvm.functions.explode(_to_java_column(col))\nAttributeError: 'NoneType' object has no attribute '_jvm'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "YPcVG0I33j4J",
        "colab_type": "code",
        "outputId": "66ebff31-1b9e-407c-b9cf-ec0e8ecbef0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 844
        }
      },
      "cell_type": "code",
      "source": [
        "# Element-Wise Transformation: Map Transform\n",
        "\n",
        "# Create (project, count) tuples(Be mindful of 'long()'!)\n",
        "project_count_tuples = columns.map(lambda column: (column[0], long(column[2])))\n",
        "project_count_tuples.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('commons.m', 1L),\n",
              " ('pl', 10L),\n",
              " ('en', 1L),\n",
              " ('de', 1L),\n",
              " ('en', 5L),\n",
              " ('en', 8L),\n",
              " ('es', 5L),\n",
              " ('sv', 1L),\n",
              " ('en', 4L),\n",
              " ('es', 2L),\n",
              " ('en', 1L),\n",
              " ('en', 1L),\n",
              " ('et', 2L),\n",
              " ('en', 4L),\n",
              " ('he', 1L),\n",
              " ('zh', 1L),\n",
              " ('sv', 1L),\n",
              " ('pt', 1L),\n",
              " ('de.voy', 1L),\n",
              " ('en', 1L),\n",
              " ('en', 2L),\n",
              " ('en', 1L),\n",
              " ('xh', 1L),\n",
              " ('nl', 1L),\n",
              " ('no', 5L),\n",
              " ('en', 1L),\n",
              " ('es', 1L),\n",
              " ('de', 1L),\n",
              " ('en', 1L),\n",
              " ('es', 3L),\n",
              " ('en', 3L),\n",
              " ('cs', 1L),\n",
              " ('es', 1L),\n",
              " ('en', 1L),\n",
              " ('pt', 1L),\n",
              " ('pt', 5L),\n",
              " ('commons.m', 1L),\n",
              " ('fi', 1L),\n",
              " ('ja', 1L),\n",
              " ('en', 1L),\n",
              " ('en', 1L),\n",
              " ('fa', 1L),\n",
              " ('fr', 1L),\n",
              " ('nl', 1L),\n",
              " ('en', 2L),\n",
              " ('en', 49L),\n",
              " ('uz', 1L)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "SADcMkDj3OVc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Element-Wise Transformation: Filter Transform\n",
        "\n",
        "# Filter project containing name 'de'\n",
        "project_de_filtered = project_count_tuples.filter(lambda t: 'de' in t[0])\n",
        "# project_de_filtered.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dlUGGF-hbzeu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Quiz 1\n",
        "Sample wikipedia data에서 project 의 count column 값이 5 보다 큰 경우를 filter 하시오.\n",
        "- 결과값: project, count 로 구성된 tuple"
      ]
    },
    {
      "metadata": {
        "id": "hYlIksygDDjD",
        "colab_type": "code",
        "outputId": "48624903-5f5c-45b8-eda5-a1e14c2597f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# FIXME\n",
        "project_count_tuples.filter(lambda x : x[1]>5L).collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('pl', 10L), ('en', 8L), ('en', 49L)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "NhZ3zYpqWiy2",
        "colab_type": "code",
        "outputId": "5c609876-2ea7-46af-cadb-4220f4fcb316",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# Transformations on one Pair RDD: ReduceByKey Transform\n",
        "\n",
        "# Compute the sum of pageview counts per project\n",
        "project_sum_tuples = project_count_tuples.reduceByKey(lambda left, right: left + right) \n",
        "filtered_tuple = project_sum_tuples.filter(lambda r : r[1] > 5L)\n",
        "project_sum_tuples.collect()\n",
        "print('filtered date')\n",
        "filtered_tuple.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "filtered date\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('pt', 7L), ('es', 12L), ('pl', 10L), ('en', 87L)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "IVcZtTxoJvGm",
        "colab_type": "code",
        "outputId": "96d0cd53-6b5f-40f6-e319-065fdd61695a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "# Transformations on two Pair RDDs: Join Transform\n",
        "\n",
        "# Declare another two sample data\n",
        "wikipedia_sample_singer = [\"en Steve_Jobs 49 2631986\"\n",
        ",\"en WoodKid 1 12739\"\n",
        ",\"en Honne 100 12658\"\n",
        ",\"fa Eminem 1 22855\"\n",
        ",\"en Sia 49 2631986\"]\n",
        "\n",
        "singer_to_ranking = [\"WoodKid 1\"\n",
        ",\"Honne 2\"\n",
        ",\"Eminem 3\"\n",
        ",\"Sia 4\"]\n",
        "\n",
        "# Parallelize the data and split into columns\n",
        "lines2 = sc.parallelize(wikipedia_sample_singer)\n",
        "lines3 = sc.parallelize(singer_to_ranking)\n",
        "\n",
        "wikipedia_sample_singer_tuples = lines2.map(lambda line: tuple(line.split(\" \")))\n",
        "singer_to_ranking_tuples = lines3.map(lambda line: tuple(line.split(\" \")))\n",
        "\n",
        "# Create (title, count) tuples and join via title name.\n",
        "title_count_tuples = wikipedia_sample_singer_tuples.map(lambda column: (column[1], long(column[2])))\n",
        "title_count_tuples.join(singer_to_ranking_tuples).collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Honne', (100L, '2')),\n",
              " ('WoodKid', (1L, '1')),\n",
              " ('Sia', (49L, '4')),\n",
              " ('Eminem', (1L, '3'))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "KgEM6fA2YaPz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## SparkSQL"
      ]
    },
    {
      "metadata": {
        "id": "Z650qgjHTFLM",
        "colab_type": "code",
        "outputId": "3fbeaff9-c35e-4f96-d4cf-1f4df437d77e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "cell_type": "code",
      "source": [
        "# Create a Spark DataFrame from wikipedia_data_sample (equivalent of a 'SQL table' in Spark)\n",
        "df = ss.createDataFrame(columns, ['project', 'title', 'count', 'size'])\n",
        "\n",
        "# Create a table view called \"WikipediaTable\"\n",
        "df.createOrReplaceTempView(\"WikipediaTable\")\n",
        "\n",
        "# Run an SQL query that selects project equals to 'en' with count greater than 5\n",
        "selected = ss.sql(\"SELECT project, count FROM WikipediaTable \\\n",
        "                   WHERE project='en' AND count >= 5\")\n",
        "\n",
        "# Print the results in this console (top 20 results will be shown)\n",
        "selected.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-----+\n",
            "|project|count|\n",
            "+-------+-----+\n",
            "|     en|    5|\n",
            "|     en|    8|\n",
            "|     en|   49|\n",
            "+-------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Aw3J1GToUxl7",
        "colab_type": "code",
        "outputId": "02feeb00-d99a-4517-b623-5da63c9e590d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        }
      },
      "cell_type": "code",
      "source": [
        "# Run an SQL query that orders projects by the number of titles each project has\n",
        "selected = ss.sql(\"SELECT project, COUNT(title) as num_of_title FROM WikipediaTable \\\n",
        "                  GROUP BY project \\\n",
        "                  ORDER BY num_of_title DESC\")\n",
        "\n",
        "# Print the results in this console (top 20 results will be shown)\n",
        "selected.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+------------+\n",
            "|  project|num_of_title|\n",
            "+---------+------------+\n",
            "|       en|          18|\n",
            "|       es|           5|\n",
            "|       pt|           3|\n",
            "|       nl|           2|\n",
            "|       de|           2|\n",
            "|commons.m|           2|\n",
            "|       sv|           2|\n",
            "|       uz|           1|\n",
            "|       cs|           1|\n",
            "|       pl|           1|\n",
            "|   de.voy|           1|\n",
            "|       xh|           1|\n",
            "|       no|           1|\n",
            "|       zh|           1|\n",
            "|       et|           1|\n",
            "|       ja|           1|\n",
            "|       he|           1|\n",
            "|       fa|           1|\n",
            "|       fr|           1|\n",
            "|       fi|           1|\n",
            "+---------+------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iN-tZTf_RasG",
        "colab_type": "code",
        "outputId": "56bc9808-0a62-4476-9c90-5e49d08591b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "cell_type": "code",
      "source": [
        "# Create a Spark DataFrame from singer_to_ranking and wikipedia_sample_singer\n",
        "df = ss.createDataFrame(singer_to_ranking_tuples, ['title', 'ranking'])\n",
        "df1 = ss.createDataFrame(wikipedia_sample_singer_tuples, ['project', 'title', 'count', 'size'])\n",
        "\n",
        "# Create a table view of them, called \"RankingTable\" and \"SingerTable\"\n",
        "df.createOrReplaceTempView(\"RankingTable\")\n",
        "df1.createOrReplaceTempView(\"SingerTable\")\n",
        "\n",
        "# Run an SQL query that joins the two tables.\n",
        "# The result will show 'ranking' of RankingTable and 'title', 'count' of SingerTable.\n",
        "# Join will be performed on rows with common 'title' in both tables.\n",
        "selected = ss.sql(\"SELECT RankingTable.ranking, SingerTable.title, SingerTable.count FROM SingerTable \\\n",
        "                   INNER JOIN RankingTable ON RankingTable.title=SingerTable.title \\\n",
        "                   ORDER BY RankingTable.ranking\")\n",
        "\n",
        "selected.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-------+-----+\n",
            "|ranking|  title|count|\n",
            "+-------+-------+-----+\n",
            "|      1|WoodKid|    1|\n",
            "|      2|  Honne|  100|\n",
            "|      3| Eminem|    1|\n",
            "|      4|    Sia|   49|\n",
            "+-------+-------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e2vgvCH5STk9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FzKCclPWYmG-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Quiz 2. \n",
        "'WikipediaTable'에서, 각 project 당 count column 값의 총합이 20 이상인 (project, sum_of_count)를 구하시오\n",
        "- 결과값: project, sum_of_count 2개의 column 을 갖는 테이블"
      ]
    },
    {
      "metadata": {
        "id": "k5x2qGd5VIyz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as sf\n",
        "from pyspark.sql.types import StructType,StructField"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NLlqBfmKVLIC",
        "colab_type": "code",
        "outputId": "b23053ac-fa5b-47e7-d988-260c48f328e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "# Create a Spark DataFrame from wikipedia_data_sample (equivalent of a 'SQL table' in Spark)\n",
        "wkdf = ss.createDataFrame(columns, ['project', 'title', 'count', 'size'])\n",
        "\n",
        "# Create a table view called \"WikipediaTable\"\n",
        "wkdf.createOrReplaceTempView(\"Wiki\")\n",
        "\n",
        "# Run an SQL query that selects project equals to 'en' with count greater than 5\n",
        "selected = ss.sql(\"SELECT project, sum(count) FROM Wiki \\\n",
        "                   group by project \\\n",
        "                   having sum(count)>=20\")\n",
        "\n",
        "# Print the results in this console (top 20 results will be shown)\n",
        "selected.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+--------------------------+\n",
            "|project|sum(CAST(count AS DOUBLE))|\n",
            "+-------+--------------------------+\n",
            "|     en|                      87.0|\n",
            "+-------+--------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LKjC_xq5XWQs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import IntegerType,StringType,ArrayType,LongType\n",
        "from pyspark.sql.functions import max,sum"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vcs2nQw-Wq3j",
        "colab_type": "code",
        "outputId": "87a6b456-a1b5-449e-a41c-81712ba7c9c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "agg_df = wkdf.withColumn('cnt',wkdf['count'].cast(IntegerType())).groupBy('project').agg(sum('cnt').alias('sumofcnt'))\n",
        "agg_df.where(agg_df['sumofcnt']>50).show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+--------+\n",
            "|project|sumofcnt|\n",
            "+-------+--------+\n",
            "|     en|      87|\n",
            "+-------+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sp9V8qXNY5CS",
        "colab_type": "code",
        "outputId": "662ab3e2-425c-4538-f36d-a2aefe898d73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "## FIXME\n",
        "# Create a Spark DataFrame from wikipedia_data_sample (equivalent of a 'SQL table' in Spark)\n",
        "\n",
        "\n",
        "wkdf = ss.createDataFrame(columns, ['project', 'title', 'count', 'size'])\n",
        "wkdf.head(5)\n",
        "wkdf.printSchema()\n",
        "# wkdf.groupBy('project').agg(sum('count')).head(10)\n",
        "# Create a table view called \"WikipediaTable\"\n",
        "# wkdf.createOrReplaceTempView(\"WikTbl\")\n",
        "\n",
        "# # Run an SQL query that selects project equals to 'en' with count greater than 5\n",
        "# wkselected = ss.sql(\"SELECT project, count FROM WikiTbl \\\n",
        "#                    WHERE project='en' AND count >= 5\")\n",
        "\n",
        "# # Print the results in this console (top 20 results will be shown)\n",
        "# wkselected.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- project: string (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- count: string (nullable = true)\n",
            " |-- size: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OLQZBBAiY_UN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Quiz 3.\n",
        "다음의 table을 'WikipediaTable'과 Join하여, grade가 'C'에 해당하는 project에 속하는 title들을 구하시오\n",
        "- 결과값: title 1개의 column 을 갖는 테이블"
      ]
    },
    {
      "metadata": {
        "id": "Zm7cN8MuZNPk",
        "colab_type": "code",
        "outputId": "540c2c6c-ee7e-4a42-e864-6efac4a30019",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "cell_type": "code",
      "source": [
        "cols = ['project', 'grade']\n",
        "vals = [\n",
        "     ('en', 'C'),\n",
        "     ('he', 'A'),\n",
        "     ('zh', 'B'),    \n",
        "     ('no', 'A')\n",
        "]\n",
        "\n",
        "title_grade = ss.createDataFrame(vals, cols)\n",
        "title_grade.show()\n",
        "title_grade.createOrReplaceTempView(\"TitleGradeTable\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-----+\n",
            "|project|grade|\n",
            "+-------+-----+\n",
            "|     en|    C|\n",
            "|     he|    A|\n",
            "|     zh|    B|\n",
            "|     no|    A|\n",
            "+-------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9EnpkIUQbTKo",
        "colab_type": "code",
        "outputId": "2cb287ee-205c-46ce-eec7-65ae36b820d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        }
      },
      "cell_type": "code",
      "source": [
        "## FIXME\n",
        "ss.sql(\"select wk.title,tg.grade from Wiki as wk \\\n",
        "        inner join TitleGradeTable as tg \\\n",
        "        on wk.project = tg.project \\\n",
        "        where tg.grade='C'\").show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-----+\n",
            "|               title|grade|\n",
            "+--------------------+-----+\n",
            "|Special:RecentCha...|    C|\n",
            "|Simeon_I_of_Bulgaria|    C|\n",
            "| Rainbow_Six_(novel)|    C|\n",
            "|Video_game_conten...|    C|\n",
            "|File:Georg_Wilhel...|    C|\n",
            "| Anestia_ombrophanes|    C|\n",
            "|And_I_Am_Telling_...|    C|\n",
            "|Independent_Chip_...|    C|\n",
            "|Category:Toronto_...|    C|\n",
            "|Special:Export/He...|    C|\n",
            "|Special:Contribut...|    C|\n",
            "|Randolph_County,_...|    C|\n",
            "|          Tu_Bishvat|    C|\n",
            "|Anatolia_(disambi...|    C|\n",
            "|           Psicobloc|    C|\n",
            "|       Macael,_Spain|    C|\n",
            "|Thomas_%26_Mack_C...|    C|\n",
            "|       Warren_Beatty|    C|\n",
            "+--------------------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IfTZ9a3pbYOU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}