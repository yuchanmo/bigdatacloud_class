{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlowPracticeSession.ipynb의 사본",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "UwlPAA8ZJUsr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Copyright (C) 2018 Software Platform Lab, Seoul National University\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); \n",
        "\n",
        "you may not use this file except in compliance with the License. \n",
        "\n",
        "You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 \n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software \n",
        "\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS, \n",
        "\n",
        "\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n",
        "\n",
        "\n",
        "See the License for the specific language governing permissions and\n",
        "\n",
        "\n",
        "limitations under the License."
      ]
    },
    {
      "metadata": {
        "id": "BDNSc2_nJex1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Colab 101**\n",
        "\n",
        "Colab is a free Jupyter notebook environment offered by Google Research. "
      ]
    },
    {
      "metadata": {
        "id": "pNB9n-BkhIEC",
        "colab_type": "toc"
      },
      "cell_type": "markdown",
      "source": [
        "**TensorFlow Practice Session**\n",
        "\n",
        ">[3-1. TensorFlow Ops](#scrollTo=Ki_RHIwPJvyn)\n",
        "\n",
        ">>[Graph and Session](#scrollTo=s97fwEOoiXk3)\n",
        "\n",
        ">>[Constant op](#scrollTo=V5N1npMVQqJz)\n",
        "\n",
        ">>[Math ops](#scrollTo=xrE4WkZNUn9o)\n",
        "\n",
        ">>[*Quiz 1](#scrollTo=cnNhG69dgWsA)\n",
        "\n",
        ">>[Variables](#scrollTo=a4UKKwwsZa4I)\n",
        "\n",
        ">>>[Creating variables](#scrollTo=2TubsFPeZ0Rz)\n",
        "\n",
        ">>>>[Usage of TF Variables](#scrollTo=2TubsFPeZ0Rz)\n",
        "\n",
        ">>>[Initialize variables](#scrollTo=r9Xymzuia0j3)\n",
        "\n",
        ">>>[Evaluate the values of variables](#scrollTo=239nTB8ScEMD)\n",
        "\n",
        ">>[*Quiz 2](#scrollTo=lJ3XeX_Gn4a3)\n",
        "\n",
        ">[3-2. Feeding Input Data into a Tensorflow Graph](#scrollTo=rgFhwdh4kJlB)\n",
        "\n",
        ">>[Placeholder](#scrollTo=JNTEqevyzz2X)\n",
        "\n",
        ">>[Dataset](#scrollTo=9mHYbZmS06zb)\n",
        "\n",
        ">>[Create dataset from files using Dataset API](#scrollTo=hV-LVSuslOLE)\n",
        "\n",
        ">>>[Create dummy binary files](#scrollTo=mCt7x4ByogNm)\n",
        "\n",
        ">>>[FixedLengthRecordDataset : each fixed-length record is a data instance.](#scrollTo=0rBvsoONqBSl)\n",
        "\n",
        ">>>[Create dummy text files](#scrollTo=L5FwnpyyneZQ)\n",
        "\n",
        ">>>[TextLineDataset : each text line is a data instance.](#scrollTo=Jv2Vc8gt9WbO)\n",
        "\n",
        ">>[Transform dataset](#scrollTo=S8eqeLjir-ar)\n",
        "\n",
        ">>[Speed up Dataset processing](#scrollTo=gXqUUZeo5a9E)\n",
        "\n",
        ">>[*Quiz 3](#scrollTo=fEs3qEJszYgu)\n",
        "\n",
        ">[3-3. Logistic Regression](#scrollTo=z8oia-fDJ4cx)\n",
        "\n",
        ">>[Download and Read MNIST data using the dataset API](#scrollTo=WYPbCvJxL8fH)\n",
        "\n",
        ">>>[Build an input pipeline using the dataset API](#scrollTo=lq_2ypB720KU)\n",
        "\n",
        ">>[Create weights and bias](#scrollTo=8H5Yn3pxNVOH)\n",
        "\n",
        ">>[Build a logistic regression model](#scrollTo=wkJhz1tCNcih)\n",
        "\n",
        ">>[Define a loss function](#scrollTo=6h4HwTbMNkx6)\n",
        "\n",
        ">>[Define a training op](#scrollTo=nZDF4B7jN5r9)\n",
        "\n",
        ">>[Train and calculate accuracy](#scrollTo=0BjGaGU4OM_O)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Ki_RHIwPJvyn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#3-1. TensorFlow Ops"
      ]
    },
    {
      "metadata": {
        "id": "s97fwEOoiXk3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Graph and Session\n",
        "**Graph** : It contains a set of Operations (units of computation) and Tensors (units of data between operations).\n",
        "\n",
        "**Session** : It encapsulates an execution environment such as which operations are executed and what is the current values of Tensor objects. "
      ]
    },
    {
      "metadata": {
        "id": "8p7tgEPijeSd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's learn Graph and Session using examples presented below."
      ]
    },
    {
      "metadata": {
        "id": "V5N1npMVQqJz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Constant Op\n",
        "\n",
        "Let's create a constant in TensorFlow.\n",
        "\n",
        "**```tf.constant(value, dtype = None, shape = None, name = 'Const', verify_shape = False)```**"
      ]
    },
    {
      "metadata": {
        "id": "EFZ3bfVsQz_p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.contrib.eager as tfe\n",
        "tfe.enable_eager_execution()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "45Jqb8tpmE7x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3a57166e-24e8-45de-cac5-1cfbf22f7011"
      },
      "cell_type": "code",
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  #constant of 1d tensor, or a vector\n",
        "  a = tf.constant([2,2], name = 'vector')\n",
        "\n",
        "  #constant of 2x2 tensor, or a matrix\n",
        "  b = tf.constant([[0,2], [1,3]], name = 'matrix')\n",
        "\n",
        "print(a)\n",
        "print(b)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"vector:0\", shape=(2,), dtype=int32)\n",
            "Tensor(\"matrix:0\", shape=(2, 2), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tqBA4_VcXCBe",
        "colab_type": "code",
        "outputId": "f2b5168b-d302-47ad-d264-fc9c99455004",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "cell_type": "code",
      "source": [
        "# Get values of a and b\n",
        "with tf.Session(graph=graph) as sess:\n",
        "  print('a')\n",
        "  print(sess.run(a))\n",
        "  print('b')\n",
        "  print(sess.run(b))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a\n",
            "[2 2]\n",
            "b\n",
            "[[0 2]\n",
            " [1 3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xrE4WkZNUn9o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Math Ops\n",
        "TensorFlow math ops are pretty standard. The following example shows a matrix division op."
      ]
    },
    {
      "metadata": {
        "id": "drceRvn4VGec",
        "colab_type": "code",
        "outputId": "b721e225-f490-4191-b38a-413ab45570b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        }
      },
      "cell_type": "code",
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  # Create constant a and b\n",
        "  a = tf.constant([2,2], name = 'a', dtype = tf.float32)\n",
        "  b = tf.constant([[0,1], [2,3]], name = 'b', dtype = tf.float32)\n",
        "  \n",
        "  # Create divide operation using b and a\n",
        "  div = tf.div(b, a)\n",
        "\n",
        "print('Print information of div op')\n",
        "print(div.op)\n",
        "\n",
        "print('\\nPrint div')\n",
        "print(div)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-38de3a2d07a7>:8: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Print information of div op\n",
            "name: \"div\"\n",
            "op: \"RealDiv\"\n",
            "input: \"b\"\n",
            "input: \"a\"\n",
            "attr {\n",
            "  key: \"T\"\n",
            "  value {\n",
            "    type: DT_FLOAT\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "Print div\n",
            "Tensor(\"div:0\", shape=(2, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jthhyswKkisO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run div operations."
      ]
    },
    {
      "metadata": {
        "id": "kkmWJN_pY8Xf",
        "colab_type": "code",
        "outputId": "5f61d541-de08-4c88-d3b7-088d609c5a24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session(graph=graph) as sess:\n",
        "  print('Print div.op')\n",
        "  print(sess.run(div.op))\n",
        "  \n",
        "  print('\\nPrint div')\n",
        "  print(sess.run(div))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Print div.op\n",
            "None\n",
            "\n",
            "Print div\n",
            "[[0.  0.5]\n",
            " [1.  1.5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "cnNhG69dgWsA"
      },
      "cell_type": "markdown",
      "source": [
        "## Quiz 1\n",
        "**Define a graph with two constants with shape=[2, 2] and a matmul operation. Print the values of c.\n",
        "(X: matrix multiplication, HINT: use tf.matmul) **"
      ]
    },
    {
      "metadata": {
        "id": "5-VXmMoJvRaJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c900fd96-5e86-4a08-efc9-cc1f1977e596"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "graph = tf.Graph()\n",
        "t = tf.random_normal([2,2],mean=3)\n",
        "sess = tf.Session()\n",
        "sess.run(t)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4.610576 , 2.0455003],\n",
              "       [3.715271 , 3.11921  ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "metadata": {
        "id": "_HY_ygacw-Jt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d5d12877-295e-4b64-85aa-762360133f32"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.random(size=(2,2))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.59187775, 0.39972936],\n",
              "       [0.85847551, 0.35101929]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "QZyw665-gWsE",
        "outputId": "c40972d0-d836-4fd3-8960-8bb9ed7f82b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  ############# Write here. ################\n",
        "  a = tf.constant(np.random.random(size=(2,2)))\n",
        "  b = tf.constant(np.random.random(size=(2,2)))\n",
        "  c = tf.matmul(a,b)\n",
        "  ##########################################\n",
        "\n",
        "with tf.Session(graph=graph) as sess:\n",
        "  print(sess.run(c))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.23130782 0.69591677]\n",
            " [0.0959227  0.25833004]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a4UKKwwsZa4I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Variables\n",
        "\n",
        "TensorFlow object to store mutable data (e.g., model parameters)."
      ]
    },
    {
      "metadata": {
        "id": "2TubsFPeZ0Rz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Creating variables\n",
        "\n",
        "To declare a variable, you create an instance of the class tf.Variable. tf.constant is written as lowercase because it's an op, and tf.Variable is written with a capital \"V\" because it's a class with multiple ops.\n",
        "\n",
        "#### Usage of TF Variables\n",
        "\n",
        "\n",
        "```\n",
        "x = tf.Variable(...)\n",
        "x.initializer   #init\n",
        "x.value         #read op\n",
        "x.assign(...)   #write op\n",
        "x.assign_add(...) # x+=...\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "zcriAnGbaCKk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The old way to create a variable is: \n",
        "\n",
        "**```tf.Variable(< initial-value >, name = < optional-name >)```**"
      ]
    },
    {
      "metadata": {
        "id": "DsaTk0M3hr0p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create three variables using `tf.Variable`"
      ]
    },
    {
      "metadata": {
        "id": "32t-sxgsaFXh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "outputId": "da3e39c2-7fba-4e45-bf7d-4a2ba02614f2"
      },
      "cell_type": "code",
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  # Create scalar variable\n",
        "  s = tf.Variable(2, name = 'scalar')\n",
        "  # Create matrix variable\n",
        "  m = tf.Variable([[0,1], [2,3]], name = 'matrix')\n",
        "  # Create zero matrix using tf.zeros\n",
        "  W = tf.Variable(tf.zeros([784,10]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-Us76wF-aTaf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The new and encouraged way is **`tf.get_variable`**, which allows us to provide the variable's internal name, shape, type, and initializer to give the variable its initial value.\n",
        "\n",
        "```\n",
        "tf.get_variable(\n",
        "    name,\n",
        "    shape=None,\n",
        "    dtype=None,\n",
        "    initializer=None,\n",
        "    regularizer=None,\n",
        "    trainable=True,\n",
        "    collections=None,\n",
        "    caching_device=None,\n",
        "    partitioner=None,\n",
        "    validate_shape=True,\n",
        "    use_resource=None,\n",
        "    custom_getter=None,\n",
        "    constraint=None\n",
        ")\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "54ABBw1ljKqB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create three variables using `tf.get_variable`"
      ]
    },
    {
      "metadata": {
        "id": "TV2kmEqUaeF8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  s = tf.get_variable('scalar', initializer=tf.constant(2))\n",
        "  m = tf.get_variable('matrix', initializer=tf.constant([[0,1], [2,3]]))\n",
        "  W = tf.get_variable('big_matrix', shape=(784, 10), initializer=tf.zeros_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9iVhrjR-aptK",
        "colab_type": "code",
        "outputId": "39446e29-3751-43b6-f755-4583a7e3bee3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session(graph=graph) as sess:\n",
        "  #필수로 Variable 에 대해서는 initialize를 명시적으로 해줘야 실행 가능함\n",
        "  init = tf.global_variables_initializer()\n",
        "  sess.run(init)\n",
        "  print(sess.run(s))\n",
        "  print(sess.run(m))\n",
        "  print(sess.run(W))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "[[0 1]\n",
            " [2 3]]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r9Xymzuia0j3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Initialize variables\n",
        "\n",
        "Before using a variable, you must initialize it, or else you'll run into an error.\n",
        "\n",
        "To initiliaze them all at once: use **`tf.global_variables_initializer()`**"
      ]
    },
    {
      "metadata": {
        "id": "IFFDV5ujbjX9",
        "colab_type": "code",
        "outputId": "d192f68f-d1f3-4cbb-8b6e-11eaaedb78b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session(graph=graph) as sess:\n",
        "  # Initialize variables\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  print('print s')\n",
        "  print(sess.run(s))\n",
        "  print('\\nprint m')\n",
        "  print(sess.run(m))\n",
        "  print('\\nprint W')\n",
        "  print(sess.run(W))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "print s\n",
            "2\n",
            "\n",
            "print m\n",
            "[[0 1]\n",
            " [2 3]]\n",
            "\n",
            "print W\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "239nTB8ScEMD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluate values of variables\n",
        "\n",
        "To get the value of a variable, we need to fetch it within a session.\n",
        "\n",
        "This example shows how to evaluate the value (`sess.run` and `eval`)."
      ]
    },
    {
      "metadata": {
        "id": "StmIpynRoJ9S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "outputId": "8aa2e58c-6cdd-497b-b461-87b1cddebbbe"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  v = tf.get_variable('normal_matrix', shape=(784,10), initializer=tf.truncated_normal_initializer())\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  v.eval()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-dbd354b883e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'normal_matrix'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated_normal_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1477\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1218\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    545\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    497\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0;31m# Set trainable value based on synchronization value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"tensorflow/python\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" % (err_msg, \"\".join(\n\u001b[0;32m--> 848\u001b[0;31m             traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    849\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Variable normal_matrix already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-17-c04ce2c663c3>\", line 2, in <module>\n    v = tf.get_variable('normal_matrix', shape=(784,10), initializer=tf.truncated_normal_initializer())\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Ezu6kFnScVd8",
        "colab_type": "code",
        "outputId": "641feb41-9632-49ff-cdbf-3f162b4ceb11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        }
      },
      "cell_type": "code",
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  # v is a 784 x 10 variable of random values\n",
        "  v = tf.get_variable('normal_matrix', shape=(784,10), initializer=tf.truncated_normal_initializer())\n",
        "\n",
        "with tf.Session(graph=graph) as sess:\n",
        "  # Initialize variables\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  \n",
        "  # Get value with sess.run()\n",
        "  v_sess = sess.run(v)\n",
        "  print('v value with sess.run')\n",
        "  print(v_sess)\n",
        "\n",
        "  # Get value with v.eval()\n",
        "  v_eval = v.eval()\n",
        "  print('\\nv value with v.eval()')\n",
        "  print(v_eval)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "v value with sess.run\n",
            "[[ 1.2321249  -0.43968326  0.81738865 ... -0.5795666   0.15272485\n",
            "   1.454538  ]\n",
            " [ 1.4339306   0.60545444  1.0215849  ...  0.7473966  -0.97005785\n",
            "  -0.43987107]\n",
            " [ 1.7107915   0.59194946 -0.09696774 ...  1.3537883  -0.83571017\n",
            "   0.37570116]\n",
            " ...\n",
            " [-0.11873318  1.0012013   0.95490146 ... -1.2887901  -0.6996174\n",
            "   1.390105  ]\n",
            " [-0.482495   -0.23953633 -0.7638849  ... -0.6663543   0.16483837\n",
            "  -1.4218045 ]\n",
            " [ 0.3522759  -0.8085719  -1.115115   ... -0.18757783 -0.79786646\n",
            "   0.2702269 ]]\n",
            "\n",
            "v value with v.eval()\n",
            "[[ 1.2321249  -0.43968326  0.81738865 ... -0.5795666   0.15272485\n",
            "   1.454538  ]\n",
            " [ 1.4339306   0.60545444  1.0215849  ...  0.7473966  -0.97005785\n",
            "  -0.43987107]\n",
            " [ 1.7107915   0.59194946 -0.09696774 ...  1.3537883  -0.83571017\n",
            "   0.37570116]\n",
            " ...\n",
            " [-0.11873318  1.0012013   0.95490146 ... -1.2887901  -0.6996174\n",
            "   1.390105  ]\n",
            " [-0.482495   -0.23953633 -0.7638849  ... -0.6663543   0.16483837\n",
            "  -1.4218045 ]\n",
            " [ 0.3522759  -0.8085719  -1.115115   ... -0.18757783 -0.79786646\n",
            "   0.2702269 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ldhC62kpdZIm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To change the values of variables, use `tf.assign`.\n",
        "\n",
        "You can see variable v changes after an *assign* operation is executed."
      ]
    },
    {
      "metadata": {
        "id": "bOpHtM4cdTeT",
        "colab_type": "code",
        "outputId": "8b46a039-7954-4719-9755-5002a4e478a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "cell_type": "code",
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  # Create variable v.\n",
        "  v = tf.get_variable(\"a\", shape=(2), initializer=tf.ones_initializer())\n",
        "  \n",
        "  # Create two assign operations.\n",
        "  assign_2 = tf.assign(v, [2, 2])\n",
        "  assign_5 = tf.assign(v, [5, 5])\n",
        "  \n",
        "with tf.Session(graph=graph) as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  \n",
        "  # Before applying assign op.\n",
        "  v_val = sess.run(v)\n",
        "  print('Initial value: %s' % (v_val))\n",
        "  \n",
        "  # Run assign_2\n",
        "  sess.run(assign_2)\n",
        "  print('After assign_2: %s' % sess.run(v))\n",
        "  \n",
        "  # Run assign_5\n",
        "  sess.run(assign_5)\n",
        "  print('After assign_5: %s' % sess.run(v))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial value: [1. 1.]\n",
            "After assign_2: [2. 2.]\n",
            "After assign_5: [5. 5.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lJ3XeX_Gn4a3"
      },
      "cell_type": "markdown",
      "source": [
        "## Quiz 2\n",
        "** Define two variables (name : \"variable1\", \"varaiable2\") with shape = [2, 3]. Initialize the variables as zeros first. **\n",
        "\n",
        "**Then, change the values of \"variable1\" as [[0,1,2], [3,4,5]]. Print final values of two variables. **"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "N6VmIhrin4a4",
        "outputId": "fbddea7e-0b7a-46cd-f140-f97d9a9c53cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "cell_type": "code",
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  ############# Write here. ################\n",
        "  v1 = tf.Variable(tf.zeros([2,3]),name='variable1')\n",
        "  v2 = tf.Variable(tf.zeros([2,3]),name='variable2')\n",
        "  \n",
        "  v1_assign = tf.assign(v1,[[0,1,2],[3,4,5]])\n",
        "  ##########################################\n",
        "\n",
        "with tf.Session(graph=graph) as sess:\n",
        "  ############# Write here. ################\n",
        "  init = tf.global_variables_initializer()\n",
        "  sess.run(init)\n",
        "  sess.run(v1_assign)\n",
        "  \n",
        "  print(sess.run([v1,v2]))\n",
        "  ##########################################"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[0., 1., 2.],\n",
            "       [3., 4., 5.]], dtype=float32), array([[0., 0., 0.],\n",
            "       [0., 0., 0.]], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rgFhwdh4kJlB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3-2. Feeding Input Data into a Tensorflow Graph\n",
        "\n",
        "How to feed data into a TensorFlow program?\n",
        "\n",
        "Using `tf.Placeholder` or\n",
        "\n",
        "**Using TensorFlow Dataset API (recommended)**"
      ]
    },
    {
      "metadata": {
        "id": "n0B6Y4_kzmXH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Create dummy array: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]**"
      ]
    },
    {
      "metadata": {
        "id": "eacvC1TMxChE",
        "colab_type": "code",
        "outputId": "473aeb26-ae9a-457e-972f-b5e49f369f22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "# Create 0~9 array\n",
        "input_constant = [i for i in range(10)]\n",
        "print('input_constant: %s' % input_constant)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_constant: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JNTEqevyzz2X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Placeholder\n",
        "\n",
        "TensorFlow provides a **placeholder** operation that must be fed with data on execution.\n",
        "\n",
        "TensorFlow's feed mechanism lets you inject data into any Tensor in a computation graph.\n",
        "\n",
        "A Python computation can thus feed data directly into the graph.\n",
        "\n",
        "Supply feed data through the feed_dict argument to a run() or eval() call that initiates computation.\n",
        "\n",
        "A placeholder exists solely to serve as the target of feeds. It is not initialized and contains no data.\n",
        "\n",
        "A placeholder generates an error if it is executed without a feed, so you won't forget to feed it."
      ]
    },
    {
      "metadata": {
        "id": "7l66JRS_03Xc",
        "colab_type": "code",
        "outputId": "0023fd1e-6036-4c42-ede0-cada688c616e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "cell_type": "code",
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  # Create scalar placeholder with integer data type.\n",
        "  a = tf.placeholder(shape=[], dtype=tf.int32)\n",
        "  \n",
        "  # Create add operation using placeholder.\n",
        "  add_1 = tf.add(a, 1)\n",
        "  \n",
        "  with tf.Session(graph=graph) as sess:\n",
        "    for i in range(10):\n",
        "      # Make feed_dict where the key is placeholder and the value is the input data.\n",
        "      feed_dict = {a: input_constant[i]}\n",
        "      \n",
        "      # Run operation with feeding data.\n",
        "      print(sess.run(add_1, feed_dict=feed_dict))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9mHYbZmS06zb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "The `tf.data` API is the most advanced API for writing TensorFlow input pipelines.\n",
        "\n",
        "It allows you to build complex pipelines by composing simple building blocks. \n",
        "\n",
        "Two main abstractions introduced by the `tf.data` API are:\n",
        "* `tf.data.Dataset`: contains a sequence of items (each item represents one or more `tf.Tensor`s)\n",
        "* `tf.data.Iterator`: provides interface to iterate through the dataset\n",
        "\n",
        "Users can create new Datasets from existing `tf.Tensor`s by using static methods like `Dataset.from_tensor_slices()`. \n",
        "\n",
        "For example, you can create a Dataset of string Tensors that represents input file names. \n",
        "\n",
        "Transformation of exisiting Datasets is another way of creating new dataset. \n",
        "\n",
        "TensorFlow provides frequently-used Dataset transformations such as `Dataset.batch` or `Dataset.shuffle` (please refer to https://www.tensorflow.org/api_docs/python/tf/data/Dataset). \n",
        "\n",
        "An Iterator is associated with a particular Dataset and we can retrieve the next element by executing an operation returned by `Iterator.get_next()`. \n",
        "\n",
        "This typically acts as an interface between your Dataset input pipline and your model.\n",
        "\n",
        "The simplest way to construct an Iterator is using `Dataset.make_one_shot_iterator()`.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "G2N-qVWB4VNW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**`tf.data.Dataset.from_tensor_slices()`**\n",
        "\n",
        "Creates a Dataset whose elements are slices of the given *python array* or *numpy array* or *tensors*."
      ]
    },
    {
      "metadata": {
        "id": "eJLs3wvl4VnE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "6940a175-fa06-4e0e-8b6a-040605c5c13c"
      },
      "cell_type": "code",
      "source": [
        "ds = tf.data.Dataset.from_tensor_slices(input_constant)\n",
        "ds"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<DatasetV1Adapter shapes: (), types: tf.int32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "knXKv93E4drK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create iterator using Dataset and run operation.\n",
        "\n",
        "Using Dataset API, you do not need to feed input data."
      ]
    },
    {
      "metadata": {
        "id": "7lVbbwCfzGbx",
        "colab_type": "code",
        "outputId": "09d1f3db-7f44-4e36-f248-fb1b64db8384",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "cell_type": "code",
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  \n",
        "  # Create iterator for dataset\n",
        "  iterator = ds.make_one_shot_iterator()\n",
        "  print(iterator)\n",
        "  # Retrieve next element from iterator\n",
        "  data_getter = iterator.get_next()\n",
        "  print(data_getter)\n",
        "  \n",
        "  # Create add operation\n",
        "  add_1 = tf.add(data_getter, 1)\n",
        "  \n",
        "  with tf.Session(graph=graph) as sess:\n",
        "    for i in range(10):\n",
        "      print(sess.run(add_1))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The graph (<tensorflow.python.framework.ops.Graph object at 0x7f2d9bd6f048>) of the iterator is different from the graph (<tensorflow.python.framework.ops.Graph object at 0x7f2da3790630>) the dataset: <DatasetV1Adapter shapes: (), types: tf.int32> was created in. If you are using the Estimator API, make sure that no part of the dataset returned by the `input_fn` function is defined outside the `input_fn` function.Please ensure that all datasets in the pipeline are created in the same graph as the iterator. NOTE: This warning will become an error in future versions of TensorFlow.\n",
            "<tensorflow.python.data.ops.iterator_ops.Iterator object at 0x7f2d9bd91550>\n",
            "Tensor(\"IteratorGetNext:0\", shape=(), dtype=int32)\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hV-LVSuslOLE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create a dataset from files using the Dataset API"
      ]
    },
    {
      "metadata": {
        "id": "jo1fuNuS61Cm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define a helper function for reading the first three dataset instances."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "og18NE7x6zUZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_dataset_instance(ds, count=3):\n",
        "  graph = tf.Graph()\n",
        "  with graph.as_default():\n",
        "    iterator = ds.make_one_shot_iterator()\n",
        "    data_getter = iterator.get_next()\n",
        "\n",
        "  with tf.Session(graph=graph) as sess:\n",
        "    for i in range(count):\n",
        "      data_instance = sess.run(data_getter)\n",
        "      print('step %d data_instance_length: %d' % (i, len(data_instance)))\n",
        "      print('step %d data_instance: %s' % (i, data_instance))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mCt7x4ByogNm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create dummy binary files"
      ]
    },
    {
      "metadata": {
        "id": "MTFhI2kBomkO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def create_bin_file(index):\n",
        "  with open('binary_file_%d'%index, 'wb') as f:\n",
        "    f.write(np.full((100), index))\n",
        "    \n",
        "bin_filenames = []\n",
        "for i in range(3):\n",
        "  create_bin_file(i)\n",
        "  bin_filenames.append('binary_file_%d'% i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0rBvsoONqBSl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### FixedLengthRecordDataset : each fixed-length record is a data instance.\n",
        "\n",
        "In this example, each data instance is a 8-byte integer."
      ]
    },
    {
      "metadata": {
        "id": "48JvZOGtqEwY",
        "colab_type": "code",
        "outputId": "eb9065e4-f02c-430e-bc17-c1afb5504b89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "cell_type": "code",
      "source": [
        "#어떤 길이로 데이터를 잘라서 읽어들일것인지 parameter로 넘겨줌\n",
        "ds = tf.data.FixedLengthRecordDataset(bin_filenames, 8)\n",
        "read_dataset_instance(ds)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The graph (<tensorflow.python.framework.ops.Graph object at 0x7f2d9bd6f128>) of the iterator is different from the graph (<tensorflow.python.framework.ops.Graph object at 0x7f2da3790630>) the dataset: <FixedLengthRecordDatasetV1 shapes: (), types: tf.string> was created in. If you are using the Estimator API, make sure that no part of the dataset returned by the `input_fn` function is defined outside the `input_fn` function.Please ensure that all datasets in the pipeline are created in the same graph as the iterator. NOTE: This warning will become an error in future versions of TensorFlow.\n",
            "step 0 data_instance_length: 8\n",
            "step 0 data_instance: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n",
            "step 1 data_instance_length: 8\n",
            "step 1 data_instance: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n",
            "step 2 data_instance_length: 8\n",
            "step 2 data_instance: b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L5FwnpyyneZQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create dummy text files"
      ]
    },
    {
      "metadata": {
        "id": "BdSZO50jlSbB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_text_file(index):\n",
        "  with open('text_file_%d'%index, 'w') as f:\n",
        "    f.write('hello_%d\\n'%index)\n",
        "    f.write('TensorFlow_%d\\n'%index)\n",
        "\n",
        "text_filenames = []\n",
        "for i in range(3):\n",
        "  create_text_file(i)\n",
        "  text_filenames.append('text_file_%d'% i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P53idkRbqFE6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "f91cdcfe-4f5e-400c-b5b0-de8afb5373b0"
      },
      "cell_type": "code",
      "source": [
        "for f in text_filenames:\n",
        "  with open(f,'r') as fr:\n",
        "    print(fr.read())"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello_0\n",
            "TensorFlow_0\n",
            "\n",
            "hello_1\n",
            "TensorFlow_1\n",
            "\n",
            "hello_2\n",
            "TensorFlow_2\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Jv2Vc8gt9WbO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### TextLineDataset : each text line is a data instance.\n"
      ]
    },
    {
      "metadata": {
        "id": "hYY0TdipmvoU",
        "colab_type": "code",
        "outputId": "10867c4b-a84e-4a48-defa-fb1dd24ae194",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "cell_type": "code",
      "source": [
        "ds = tf.data.TextLineDataset(text_filenames)\n",
        "\n",
        "read_dataset_instance(ds)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The graph (<tensorflow.python.framework.ops.Graph object at 0x7f2d9954c6d8>) of the iterator is different from the graph (<tensorflow.python.framework.ops.Graph object at 0x7f2da3790630>) the dataset: <TextLineDatasetV1 shapes: (), types: tf.string> was created in. If you are using the Estimator API, make sure that no part of the dataset returned by the `input_fn` function is defined outside the `input_fn` function.Please ensure that all datasets in the pipeline are created in the same graph as the iterator. NOTE: This warning will become an error in future versions of TensorFlow.\n",
            "step 0 data_instance_length: 7\n",
            "step 0 data_instance: b'hello_0'\n",
            "step 1 data_instance_length: 12\n",
            "step 1 data_instance: b'TensorFlow_0'\n",
            "step 2 data_instance_length: 7\n",
            "step 2 data_instance: b'hello_1'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S8eqeLjir-ar",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Transform dataset"
      ]
    },
    {
      "metadata": {
        "id": "GinK40Kp5jQA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**ds.shffule(buffer_size)**\n",
        "\n",
        "shuffle: shuffle data instances randomly. buffer size represents the number of data instances to be sampled.\n"
      ]
    },
    {
      "metadata": {
        "id": "CzImNht48XKW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "ds.shuffle with N > 1 can pick data instances randomly within N samples."
      ]
    },
    {
      "metadata": {
        "id": "b3oaSOHVsGLc",
        "colab_type": "code",
        "outputId": "cb936961-8b99-4b2d-e76b-5f5eed478d66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        }
      },
      "cell_type": "code",
      "source": [
        "ds = tf.data.TextLineDataset(text_filenames)\n",
        "ds = ds.shuffle(5)\n",
        "\n",
        "read_dataset_instance(ds)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The graph (<tensorflow.python.framework.ops.Graph object at 0x7f2d9954ccf8>) of the iterator is different from the graph (<tensorflow.python.framework.ops.Graph object at 0x7f2da3790630>) the dataset: <DatasetV1Adapter shapes: (), types: tf.string> was created in. If you are using the Estimator API, make sure that no part of the dataset returned by the `input_fn` function is defined outside the `input_fn` function.Please ensure that all datasets in the pipeline are created in the same graph as the iterator. NOTE: This warning will become an error in future versions of TensorFlow.\n",
            "WARNING:tensorflow:The graph (<tensorflow.python.framework.ops.Graph object at 0x7f2d9954ccf8>) of the iterator is different from the graph (<tensorflow.python.framework.ops.Graph object at 0x7f2da3790630>) the dataset: <TextLineDatasetV1 shapes: (), types: tf.string> was created in. If you are using the Estimator API, make sure that no part of the dataset returned by the `input_fn` function is defined outside the `input_fn` function.Please ensure that all datasets in the pipeline are created in the same graph as the iterator. NOTE: This warning will become an error in future versions of TensorFlow.\n",
            "step 0 data_instance_length: 12\n",
            "step 0 data_instance: b'TensorFlow_1'\n",
            "step 1 data_instance_length: 7\n",
            "step 1 data_instance: b'hello_1'\n",
            "step 2 data_instance_length: 7\n",
            "step 2 data_instance: b'hello_2'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bP-MH37I8s8z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "ds.shuffle with N == 1 has the same effect with no shuffling."
      ]
    },
    {
      "metadata": {
        "id": "c8Q9W2pG6nxF",
        "colab_type": "code",
        "outputId": "26cd9cf8-0853-49ad-f204-79764b2ad9f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "cell_type": "code",
      "source": [
        "ds = tf.data.TextLineDataset(text_filenames)\n",
        "ds = ds.shuffle(1)\n",
        "\n",
        "read_dataset_instance(ds)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0 data_instance_length: 7\n",
            "step 0 data_instance: b'hello_0'\n",
            "step 1 data_instance_length: 12\n",
            "step 1 data_instance: b'TensorFlow_0'\n",
            "step 2 data_instance_length: 7\n",
            "step 2 data_instance: b'hello_1'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9i-rJwnE6Vdj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**ds.repeat(count)**\n",
        "\n",
        "Repeat the data instances count times. "
      ]
    },
    {
      "metadata": {
        "id": "WM9xpfjr72Qo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Without ds.repeat(), an error is raised after reading all the data instances once."
      ]
    },
    {
      "metadata": {
        "id": "KUzyWsUt6WQ_",
        "colab_type": "code",
        "outputId": "c2f1280a-9fea-45dc-e17b-5f8244253d0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1852
        }
      },
      "cell_type": "code",
      "source": [
        "ds = tf.data.TextLineDataset(text_filenames)\n",
        "#100개의 instance 가 없어서 에러발생\n",
        "read_dataset_instance(ds, 100)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The graph (<tensorflow.python.framework.ops.Graph object at 0x7f2d99532f98>) of the iterator is different from the graph (<tensorflow.python.framework.ops.Graph object at 0x7f2da3790630>) the dataset: <TextLineDatasetV1 shapes: (), types: tf.string> was created in. If you are using the Estimator API, make sure that no part of the dataset returned by the `input_fn` function is defined outside the `input_fn` function.Please ensure that all datasets in the pipeline are created in the same graph as the iterator. NOTE: This warning will become an error in future versions of TensorFlow.\n",
            "step 0 data_instance_length: 7\n",
            "step 0 data_instance: b'hello_0'\n",
            "step 1 data_instance_length: 12\n",
            "step 1 data_instance: b'TensorFlow_0'\n",
            "step 2 data_instance_length: 7\n",
            "step 2 data_instance: b'hello_1'\n",
            "step 3 data_instance_length: 12\n",
            "step 3 data_instance: b'TensorFlow_1'\n",
            "step 4 data_instance_length: 7\n",
            "step 4 data_instance: b'hello_2'\n",
            "step 5 data_instance_length: 12\n",
            "step 5 data_instance: b'TensorFlow_2'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "OutOfRangeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfRangeError\u001b[0m: End of sequence\n\t [[{{node IteratorGetNext}}]]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-0228159e21ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextLineDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_filenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mread_dataset_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-5146be67aea2>\u001b[0m in \u001b[0;36mread_dataset_instance\u001b[0;34m(ds, count)\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m       \u001b[0mdata_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'step %d data_instance_length: %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_instance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'step %d data_instance: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_instance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfRangeError\u001b[0m: End of sequence\n\t [[node IteratorGetNext (defined at <ipython-input-25-5146be67aea2>:5) ]]\n\nCaused by op 'IteratorGetNext', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2828, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-32-0228159e21ca>\", line 2, in <module>\n    read_dataset_instance(ds, 100)\n  File \"<ipython-input-25-5146be67aea2>\", line 5, in read_dataset_instance\n    data_getter = iterator.get_next()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 414, in get_next\n    output_shapes=self._structure._flat_shapes, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1685, in iterator_get_next\n    output_shapes=output_shapes, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nOutOfRangeError (see above for traceback): End of sequence\n\t [[node IteratorGetNext (defined at <ipython-input-25-5146be67aea2>:5) ]]\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Z_CwPo5I76G9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "ds.repeat() can solve the error."
      ]
    },
    {
      "metadata": {
        "id": "FzqJY23X8Hg0",
        "colab_type": "code",
        "outputId": "fe021dec-5865-4f74-9528-5afaa00103b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3786
        }
      },
      "cell_type": "code",
      "source": [
        "ds = tf.data.TextLineDataset(text_filenames)\n",
        "ds = ds.repeat(100)\n",
        "\n",
        "read_dataset_instance(ds, 100)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The graph (<tensorflow.python.framework.ops.Graph object at 0x7f2d99532550>) of the iterator is different from the graph (<tensorflow.python.framework.ops.Graph object at 0x7f2da3790630>) the dataset: <DatasetV1Adapter shapes: (), types: tf.string> was created in. If you are using the Estimator API, make sure that no part of the dataset returned by the `input_fn` function is defined outside the `input_fn` function.Please ensure that all datasets in the pipeline are created in the same graph as the iterator. NOTE: This warning will become an error in future versions of TensorFlow.\n",
            "WARNING:tensorflow:The graph (<tensorflow.python.framework.ops.Graph object at 0x7f2d99532550>) of the iterator is different from the graph (<tensorflow.python.framework.ops.Graph object at 0x7f2da3790630>) the dataset: <TextLineDatasetV1 shapes: (), types: tf.string> was created in. If you are using the Estimator API, make sure that no part of the dataset returned by the `input_fn` function is defined outside the `input_fn` function.Please ensure that all datasets in the pipeline are created in the same graph as the iterator. NOTE: This warning will become an error in future versions of TensorFlow.\n",
            "step 0 data_instance_length: 7\n",
            "step 0 data_instance: b'hello_0'\n",
            "step 1 data_instance_length: 12\n",
            "step 1 data_instance: b'TensorFlow_0'\n",
            "step 2 data_instance_length: 7\n",
            "step 2 data_instance: b'hello_1'\n",
            "step 3 data_instance_length: 12\n",
            "step 3 data_instance: b'TensorFlow_1'\n",
            "step 4 data_instance_length: 7\n",
            "step 4 data_instance: b'hello_2'\n",
            "step 5 data_instance_length: 12\n",
            "step 5 data_instance: b'TensorFlow_2'\n",
            "step 6 data_instance_length: 7\n",
            "step 6 data_instance: b'hello_0'\n",
            "step 7 data_instance_length: 12\n",
            "step 7 data_instance: b'TensorFlow_0'\n",
            "step 8 data_instance_length: 7\n",
            "step 8 data_instance: b'hello_1'\n",
            "step 9 data_instance_length: 12\n",
            "step 9 data_instance: b'TensorFlow_1'\n",
            "step 10 data_instance_length: 7\n",
            "step 10 data_instance: b'hello_2'\n",
            "step 11 data_instance_length: 12\n",
            "step 11 data_instance: b'TensorFlow_2'\n",
            "step 12 data_instance_length: 7\n",
            "step 12 data_instance: b'hello_0'\n",
            "step 13 data_instance_length: 12\n",
            "step 13 data_instance: b'TensorFlow_0'\n",
            "step 14 data_instance_length: 7\n",
            "step 14 data_instance: b'hello_1'\n",
            "step 15 data_instance_length: 12\n",
            "step 15 data_instance: b'TensorFlow_1'\n",
            "step 16 data_instance_length: 7\n",
            "step 16 data_instance: b'hello_2'\n",
            "step 17 data_instance_length: 12\n",
            "step 17 data_instance: b'TensorFlow_2'\n",
            "step 18 data_instance_length: 7\n",
            "step 18 data_instance: b'hello_0'\n",
            "step 19 data_instance_length: 12\n",
            "step 19 data_instance: b'TensorFlow_0'\n",
            "step 20 data_instance_length: 7\n",
            "step 20 data_instance: b'hello_1'\n",
            "step 21 data_instance_length: 12\n",
            "step 21 data_instance: b'TensorFlow_1'\n",
            "step 22 data_instance_length: 7\n",
            "step 22 data_instance: b'hello_2'\n",
            "step 23 data_instance_length: 12\n",
            "step 23 data_instance: b'TensorFlow_2'\n",
            "step 24 data_instance_length: 7\n",
            "step 24 data_instance: b'hello_0'\n",
            "step 25 data_instance_length: 12\n",
            "step 25 data_instance: b'TensorFlow_0'\n",
            "step 26 data_instance_length: 7\n",
            "step 26 data_instance: b'hello_1'\n",
            "step 27 data_instance_length: 12\n",
            "step 27 data_instance: b'TensorFlow_1'\n",
            "step 28 data_instance_length: 7\n",
            "step 28 data_instance: b'hello_2'\n",
            "step 29 data_instance_length: 12\n",
            "step 29 data_instance: b'TensorFlow_2'\n",
            "step 30 data_instance_length: 7\n",
            "step 30 data_instance: b'hello_0'\n",
            "step 31 data_instance_length: 12\n",
            "step 31 data_instance: b'TensorFlow_0'\n",
            "step 32 data_instance_length: 7\n",
            "step 32 data_instance: b'hello_1'\n",
            "step 33 data_instance_length: 12\n",
            "step 33 data_instance: b'TensorFlow_1'\n",
            "step 34 data_instance_length: 7\n",
            "step 34 data_instance: b'hello_2'\n",
            "step 35 data_instance_length: 12\n",
            "step 35 data_instance: b'TensorFlow_2'\n",
            "step 36 data_instance_length: 7\n",
            "step 36 data_instance: b'hello_0'\n",
            "step 37 data_instance_length: 12\n",
            "step 37 data_instance: b'TensorFlow_0'\n",
            "step 38 data_instance_length: 7\n",
            "step 38 data_instance: b'hello_1'\n",
            "step 39 data_instance_length: 12\n",
            "step 39 data_instance: b'TensorFlow_1'\n",
            "step 40 data_instance_length: 7\n",
            "step 40 data_instance: b'hello_2'\n",
            "step 41 data_instance_length: 12\n",
            "step 41 data_instance: b'TensorFlow_2'\n",
            "step 42 data_instance_length: 7\n",
            "step 42 data_instance: b'hello_0'\n",
            "step 43 data_instance_length: 12\n",
            "step 43 data_instance: b'TensorFlow_0'\n",
            "step 44 data_instance_length: 7\n",
            "step 44 data_instance: b'hello_1'\n",
            "step 45 data_instance_length: 12\n",
            "step 45 data_instance: b'TensorFlow_1'\n",
            "step 46 data_instance_length: 7\n",
            "step 46 data_instance: b'hello_2'\n",
            "step 47 data_instance_length: 12\n",
            "step 47 data_instance: b'TensorFlow_2'\n",
            "step 48 data_instance_length: 7\n",
            "step 48 data_instance: b'hello_0'\n",
            "step 49 data_instance_length: 12\n",
            "step 49 data_instance: b'TensorFlow_0'\n",
            "step 50 data_instance_length: 7\n",
            "step 50 data_instance: b'hello_1'\n",
            "step 51 data_instance_length: 12\n",
            "step 51 data_instance: b'TensorFlow_1'\n",
            "step 52 data_instance_length: 7\n",
            "step 52 data_instance: b'hello_2'\n",
            "step 53 data_instance_length: 12\n",
            "step 53 data_instance: b'TensorFlow_2'\n",
            "step 54 data_instance_length: 7\n",
            "step 54 data_instance: b'hello_0'\n",
            "step 55 data_instance_length: 12\n",
            "step 55 data_instance: b'TensorFlow_0'\n",
            "step 56 data_instance_length: 7\n",
            "step 56 data_instance: b'hello_1'\n",
            "step 57 data_instance_length: 12\n",
            "step 57 data_instance: b'TensorFlow_1'\n",
            "step 58 data_instance_length: 7\n",
            "step 58 data_instance: b'hello_2'\n",
            "step 59 data_instance_length: 12\n",
            "step 59 data_instance: b'TensorFlow_2'\n",
            "step 60 data_instance_length: 7\n",
            "step 60 data_instance: b'hello_0'\n",
            "step 61 data_instance_length: 12\n",
            "step 61 data_instance: b'TensorFlow_0'\n",
            "step 62 data_instance_length: 7\n",
            "step 62 data_instance: b'hello_1'\n",
            "step 63 data_instance_length: 12\n",
            "step 63 data_instance: b'TensorFlow_1'\n",
            "step 64 data_instance_length: 7\n",
            "step 64 data_instance: b'hello_2'\n",
            "step 65 data_instance_length: 12\n",
            "step 65 data_instance: b'TensorFlow_2'\n",
            "step 66 data_instance_length: 7\n",
            "step 66 data_instance: b'hello_0'\n",
            "step 67 data_instance_length: 12\n",
            "step 67 data_instance: b'TensorFlow_0'\n",
            "step 68 data_instance_length: 7\n",
            "step 68 data_instance: b'hello_1'\n",
            "step 69 data_instance_length: 12\n",
            "step 69 data_instance: b'TensorFlow_1'\n",
            "step 70 data_instance_length: 7\n",
            "step 70 data_instance: b'hello_2'\n",
            "step 71 data_instance_length: 12\n",
            "step 71 data_instance: b'TensorFlow_2'\n",
            "step 72 data_instance_length: 7\n",
            "step 72 data_instance: b'hello_0'\n",
            "step 73 data_instance_length: 12\n",
            "step 73 data_instance: b'TensorFlow_0'\n",
            "step 74 data_instance_length: 7\n",
            "step 74 data_instance: b'hello_1'\n",
            "step 75 data_instance_length: 12\n",
            "step 75 data_instance: b'TensorFlow_1'\n",
            "step 76 data_instance_length: 7\n",
            "step 76 data_instance: b'hello_2'\n",
            "step 77 data_instance_length: 12\n",
            "step 77 data_instance: b'TensorFlow_2'\n",
            "step 78 data_instance_length: 7\n",
            "step 78 data_instance: b'hello_0'\n",
            "step 79 data_instance_length: 12\n",
            "step 79 data_instance: b'TensorFlow_0'\n",
            "step 80 data_instance_length: 7\n",
            "step 80 data_instance: b'hello_1'\n",
            "step 81 data_instance_length: 12\n",
            "step 81 data_instance: b'TensorFlow_1'\n",
            "step 82 data_instance_length: 7\n",
            "step 82 data_instance: b'hello_2'\n",
            "step 83 data_instance_length: 12\n",
            "step 83 data_instance: b'TensorFlow_2'\n",
            "step 84 data_instance_length: 7\n",
            "step 84 data_instance: b'hello_0'\n",
            "step 85 data_instance_length: 12\n",
            "step 85 data_instance: b'TensorFlow_0'\n",
            "step 86 data_instance_length: 7\n",
            "step 86 data_instance: b'hello_1'\n",
            "step 87 data_instance_length: 12\n",
            "step 87 data_instance: b'TensorFlow_1'\n",
            "step 88 data_instance_length: 7\n",
            "step 88 data_instance: b'hello_2'\n",
            "step 89 data_instance_length: 12\n",
            "step 89 data_instance: b'TensorFlow_2'\n",
            "step 90 data_instance_length: 7\n",
            "step 90 data_instance: b'hello_0'\n",
            "step 91 data_instance_length: 12\n",
            "step 91 data_instance: b'TensorFlow_0'\n",
            "step 92 data_instance_length: 7\n",
            "step 92 data_instance: b'hello_1'\n",
            "step 93 data_instance_length: 12\n",
            "step 93 data_instance: b'TensorFlow_1'\n",
            "step 94 data_instance_length: 7\n",
            "step 94 data_instance: b'hello_2'\n",
            "step 95 data_instance_length: 12\n",
            "step 95 data_instance: b'TensorFlow_2'\n",
            "step 96 data_instance_length: 7\n",
            "step 96 data_instance: b'hello_0'\n",
            "step 97 data_instance_length: 12\n",
            "step 97 data_instance: b'TensorFlow_0'\n",
            "step 98 data_instance_length: 7\n",
            "step 98 data_instance: b'hello_1'\n",
            "step 99 data_instance_length: 12\n",
            "step 99 data_instance: b'TensorFlow_1'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "71CqxItI4_Rk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**ds.batch(batch_size)**\n",
        "\n",
        "Combines data instances of this dataset into batches. batch_size represents the number of data instances to combine."
      ]
    },
    {
      "metadata": {
        "id": "H9j2k0yC9C3C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Make batches with batch size of 3"
      ]
    },
    {
      "metadata": {
        "id": "8stEmpyxvT18",
        "colab_type": "code",
        "outputId": "967318cb-80cc-43f6-e70b-14dbd866720f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        }
      },
      "cell_type": "code",
      "source": [
        "ds = tf.data.TextLineDataset(text_filenames)\n",
        "ds = ds.shuffle(3)\n",
        "ds = ds.repeat()\n",
        "ds = ds.batch(3)\n",
        "\n",
        "read_dataset_instance(ds)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The graph (<tensorflow.python.framework.ops.Graph object at 0x7f2d97cd3080>) of the iterator is different from the graph (<tensorflow.python.framework.ops.Graph object at 0x7f2da3790630>) the dataset: <DatasetV1Adapter shapes: (?,), types: tf.string> was created in. If you are using the Estimator API, make sure that no part of the dataset returned by the `input_fn` function is defined outside the `input_fn` function.Please ensure that all datasets in the pipeline are created in the same graph as the iterator. NOTE: This warning will become an error in future versions of TensorFlow.\n",
            "WARNING:tensorflow:The graph (<tensorflow.python.framework.ops.Graph object at 0x7f2d97cd3080>) of the iterator is different from the graph (<tensorflow.python.framework.ops.Graph object at 0x7f2da3790630>) the dataset: <DatasetV1Adapter shapes: (), types: tf.string> was created in. If you are using the Estimator API, make sure that no part of the dataset returned by the `input_fn` function is defined outside the `input_fn` function.Please ensure that all datasets in the pipeline are created in the same graph as the iterator. NOTE: This warning will become an error in future versions of TensorFlow.\n",
            "WARNING:tensorflow:The graph (<tensorflow.python.framework.ops.Graph object at 0x7f2d97cd3080>) of the iterator is different from the graph (<tensorflow.python.framework.ops.Graph object at 0x7f2da3790630>) the dataset: <DatasetV1Adapter shapes: (), types: tf.string> was created in. If you are using the Estimator API, make sure that no part of the dataset returned by the `input_fn` function is defined outside the `input_fn` function.Please ensure that all datasets in the pipeline are created in the same graph as the iterator. NOTE: This warning will become an error in future versions of TensorFlow.\n",
            "WARNING:tensorflow:The graph (<tensorflow.python.framework.ops.Graph object at 0x7f2d97cd3080>) of the iterator is different from the graph (<tensorflow.python.framework.ops.Graph object at 0x7f2da3790630>) the dataset: <TextLineDatasetV1 shapes: (), types: tf.string> was created in. If you are using the Estimator API, make sure that no part of the dataset returned by the `input_fn` function is defined outside the `input_fn` function.Please ensure that all datasets in the pipeline are created in the same graph as the iterator. NOTE: This warning will become an error in future versions of TensorFlow.\n",
            "step 0 data_instance_length: 3\n",
            "step 0 data_instance: [b'hello_0' b'hello_1' b'hello_2']\n",
            "step 1 data_instance_length: 3\n",
            "step 1 data_instance: [b'TensorFlow_2' b'TensorFlow_0' b'TensorFlow_1']\n",
            "step 2 data_instance_length: 3\n",
            "step 2 data_instance: [b'TensorFlow_0' b'TensorFlow_1' b'hello_2']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "meX0yDAd4KCD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**ds.map(fn)**\n",
        "\n",
        "apply fn to each data instance in the dataset"
      ]
    },
    {
      "metadata": {
        "id": "BLy4cgXR9Gum",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Apply split_index function to the batched dataset"
      ]
    },
    {
      "metadata": {
        "id": "zITLEwgys7l8",
        "colab_type": "code",
        "outputId": "3ad666ea-1c48-45ff-c5a4-4f0337fa7f67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "cell_type": "code",
      "source": [
        "def split_index(data):\n",
        "  data = tf.split(data, [1, 1, 1])\n",
        "  return tf.strings.join(data, '+')\n",
        "\n",
        "ds = tf.data.TextLineDataset(text_filenames)\n",
        "ds = ds.shuffle(5)\n",
        "ds = ds.repeat()\n",
        "ds = ds.batch(3)\n",
        "ds = ds.map(split_index)\n",
        "\n",
        "read_dataset_instance(ds)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The graph (<tensorflow.python.framework.ops.Graph object at 0x7f2d99d21550>) of the iterator is different from the graph (<tensorflow.python.framework.ops.Graph object at 0x7f2da3790630>) the dataset: <DatasetV1Adapter shapes: (1,), types: tf.string> was created in. If you are using the Estimator API, make sure that no part of the dataset returned by the `input_fn` function is defined outside the `input_fn` function.Please ensure that all datasets in the pipeline are created in the same graph as the iterator. NOTE: This warning will become an error in future versions of TensorFlow.\n",
            "WARNING:tensorflow:The graph (<tensorflow.python.framework.ops.Graph object at 0x7f2d99d21550>) of the iterator is different from the graph (<tensorflow.python.framework.ops.Graph object at 0x7f2da3790630>) the dataset: <DatasetV1Adapter shapes: (?,), types: tf.string> was created in. If you are using the Estimator API, make sure that no part of the dataset returned by the `input_fn` function is defined outside the `input_fn` function.Please ensure that all datasets in the pipeline are created in the same graph as the iterator. NOTE: This warning will become an error in future versions of TensorFlow.\n",
            "WARNING:tensorflow:The graph (<tensorflow.python.framework.ops.Graph object at 0x7f2d99d21550>) of the iterator is different from the graph (<tensorflow.python.framework.ops.Graph object at 0x7f2da3790630>) the dataset: <DatasetV1Adapter shapes: (), types: tf.string> was created in. If you are using the Estimator API, make sure that no part of the dataset returned by the `input_fn` function is defined outside the `input_fn` function.Please ensure that all datasets in the pipeline are created in the same graph as the iterator. NOTE: This warning will become an error in future versions of TensorFlow.\n",
            "WARNING:tensorflow:The graph (<tensorflow.python.framework.ops.Graph object at 0x7f2d99d21550>) of the iterator is different from the graph (<tensorflow.python.framework.ops.Graph object at 0x7f2da3790630>) the dataset: <DatasetV1Adapter shapes: (), types: tf.string> was created in. If you are using the Estimator API, make sure that no part of the dataset returned by the `input_fn` function is defined outside the `input_fn` function.Please ensure that all datasets in the pipeline are created in the same graph as the iterator. NOTE: This warning will become an error in future versions of TensorFlow.\n",
            "WARNING:tensorflow:The graph (<tensorflow.python.framework.ops.Graph object at 0x7f2d99d21550>) of the iterator is different from the graph (<tensorflow.python.framework.ops.Graph object at 0x7f2da3790630>) the dataset: <TextLineDatasetV1 shapes: (), types: tf.string> was created in. If you are using the Estimator API, make sure that no part of the dataset returned by the `input_fn` function is defined outside the `input_fn` function.Please ensure that all datasets in the pipeline are created in the same graph as the iterator. NOTE: This warning will become an error in future versions of TensorFlow.\n",
            "step 0 data_instance_length: 1\n",
            "step 0 data_instance: [b'hello_0+TensorFlow_1+hello_1']\n",
            "step 1 data_instance_length: 1\n",
            "step 1 data_instance: [b'TensorFlow_2+TensorFlow_0+hello_2']\n",
            "step 2 data_instance_length: 1\n",
            "step 2 data_instance: [b'hello_2+TensorFlow_2+hello_0']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gXqUUZeo5a9E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##  Speed up Dataset processing\n"
      ]
    },
    {
      "metadata": {
        "id": "WplDJMqQ59im",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "**ds.interleave(map_func, cycle_length)**\n",
        "\n",
        "map _func : map function to apply to each data instance\n",
        "\n",
        "cycle_length : the number of data instances to process concurrently"
      ]
    },
    {
      "metadata": {
        "id": "elNSpQlF9Kkd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Read files concurrently "
      ]
    },
    {
      "metadata": {
        "id": "dlIOImd6KHHr",
        "colab_type": "code",
        "outputId": "b21a10d6-1487-4e78-98a9-adccaea265f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "cell_type": "code",
      "source": [
        "ds = tf.data.Dataset.from_tensor_slices(text_filenames)\n",
        "ds = ds.apply(\n",
        "      tf.contrib.data.parallel_interleave(\n",
        "          lambda filename: tf.data.TextLineDataset(filename),\n",
        "          cycle_length=len(text_filenames)))\n",
        "\n",
        "read_dataset_instance(ds)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-36-d3ab75a0b6ab>:5: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "WARNING:tensorflow:The graph (<tensorflow.python.framework.ops.Graph object at 0x7f2d99532898>) of the iterator is different from the graph (<tensorflow.python.framework.ops.Graph object at 0x7f2da3790630>) the dataset: <DatasetV1Adapter shapes: (), types: tf.string> was created in. If you are using the Estimator API, make sure that no part of the dataset returned by the `input_fn` function is defined outside the `input_fn` function.Please ensure that all datasets in the pipeline are created in the same graph as the iterator. NOTE: This warning will become an error in future versions of TensorFlow.\n",
            "WARNING:tensorflow:The graph (<tensorflow.python.framework.ops.Graph object at 0x7f2d99532898>) of the iterator is different from the graph (<tensorflow.python.framework.ops.Graph object at 0x7f2da3790630>) the dataset: <DatasetV1Adapter shapes: (), types: tf.string> was created in. If you are using the Estimator API, make sure that no part of the dataset returned by the `input_fn` function is defined outside the `input_fn` function.Please ensure that all datasets in the pipeline are created in the same graph as the iterator. NOTE: This warning will become an error in future versions of TensorFlow.\n",
            "step 0 data_instance_length: 7\n",
            "step 0 data_instance: b'hello_0'\n",
            "step 1 data_instance_length: 7\n",
            "step 1 data_instance: b'hello_1'\n",
            "step 2 data_instance_length: 7\n",
            "step 2 data_instance: b'hello_2'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mhp0R08AKxpy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**ds.prefetch(buffer_size)** \n",
        "\n",
        "prefetch elements from a dataset. buffer size represents the maximum buffer size"
      ]
    },
    {
      "metadata": {
        "id": "jap0MiIv9NXx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Prefetch five data instances"
      ]
    },
    {
      "metadata": {
        "id": "AWM2PH6SLHrb",
        "colab_type": "code",
        "outputId": "c9e116ab-a7d0-47cb-d094-94e3063c2f02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "ds = tf.data.Dataset.from_tensor_slices(text_filenames)\n",
        "ds = ds.apply(\n",
        "      tf.contrib.data.parallel_interleave(\n",
        "          lambda filename: tf.data.TextLineDataset(filename),\n",
        "          cycle_length=len(text_filenames)))\n",
        "ds = ds.prefetch(5)\n",
        "\n",
        "read_dataset_instance(ds)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The graph (<tensorflow.python.framework.ops.Graph object at 0x7f2d99d3a3c8>) of the iterator is different from the graph (<tensorflow.python.framework.ops.Graph object at 0x7f2da3790630>) the dataset: <DatasetV1Adapter shapes: (), types: tf.string> was created in. If you are using the Estimator API, make sure that no part of the dataset returned by the `input_fn` function is defined outside the `input_fn` function.Please ensure that all datasets in the pipeline are created in the same graph as the iterator. NOTE: This warning will become an error in future versions of TensorFlow.\n",
            "WARNING:tensorflow:The graph (<tensorflow.python.framework.ops.Graph object at 0x7f2d99d3a3c8>) of the iterator is different from the graph (<tensorflow.python.framework.ops.Graph object at 0x7f2da3790630>) the dataset: <DatasetV1Adapter shapes: (), types: tf.string> was created in. If you are using the Estimator API, make sure that no part of the dataset returned by the `input_fn` function is defined outside the `input_fn` function.Please ensure that all datasets in the pipeline are created in the same graph as the iterator. NOTE: This warning will become an error in future versions of TensorFlow.\n",
            "WARNING:tensorflow:The graph (<tensorflow.python.framework.ops.Graph object at 0x7f2d99d3a3c8>) of the iterator is different from the graph (<tensorflow.python.framework.ops.Graph object at 0x7f2da3790630>) the dataset: <DatasetV1Adapter shapes: (), types: tf.string> was created in. If you are using the Estimator API, make sure that no part of the dataset returned by the `input_fn` function is defined outside the `input_fn` function.Please ensure that all datasets in the pipeline are created in the same graph as the iterator. NOTE: This warning will become an error in future versions of TensorFlow.\n",
            "step 0 data_instance_length: 7\n",
            "step 0 data_instance: b'hello_0'\n",
            "step 1 data_instance_length: 7\n",
            "step 1 data_instance: b'hello_1'\n",
            "step 2 data_instance_length: 7\n",
            "step 2 data_instance: b'hello_2'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fEs3qEJszYgu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Quiz 3\n",
        "**Create a dataset following the instructions**\n",
        "1. Create a textline dataset using `ex_filenames`. \n",
        "2. Shuffle the dataset with buffer size 10.\n",
        "3. Repeat the dataset.\n",
        "4. Convert each data instance using the `cast` function.\n",
        "5. Make the data instances as a batch (batch size = 3).\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "C_TOoWhx1O2S",
        "colab_type": "code",
        "outputId": "a3939d06-f4d3-46ad-a839-2793946f6410",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def create_text_file(index):\n",
        "  with open('ex_file_%d'%index, 'w') as f:\n",
        "    for i in range(5):\n",
        "      f.write('%d\\n' % random.randrange(0,index+1))\n",
        "    \n",
        "ex_filenames = []\n",
        "for i in range(3):\n",
        "  create_text_file(i)\n",
        "  ex_filenames.append('ex_file_%d'% i)\n",
        "\n",
        "def cast(data):\n",
        "  data = tf.strings.to_number(data, out_type=tf.int32)\n",
        "  return data\n",
        "\n",
        "############# Write here. ################\n",
        "ds = tf.data.TextLineDataset(ex_filenames)\n",
        "ds = ds.shuffle(10)\n",
        "ds = ds.repeat()\n",
        "ds = ds.batch(3)\n",
        "ds = ds.map(cast)\n",
        "\n",
        "##########################################\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  iterator = ds.make_one_shot_iterator()\n",
        "  data_getter = iterator.get_next()\n",
        "\n",
        "with tf.Session(graph=graph) as sess:\n",
        "  data_instance = sess.run(data_getter)\n",
        "  print(data_instance)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The graph (<tensorflow.python.framework.ops.Graph object at 0x7f2d9bc23ef0>) of the iterator is different from the graph (<tensorflow.python.framework.ops.Graph object at 0x7f2da3790630>) the dataset: <DatasetV1Adapter shapes: (?,), types: tf.int32> was created in. If you are using the Estimator API, make sure that no part of the dataset returned by the `input_fn` function is defined outside the `input_fn` function.Please ensure that all datasets in the pipeline are created in the same graph as the iterator. NOTE: This warning will become an error in future versions of TensorFlow.\n",
            "WARNING:tensorflow:The graph (<tensorflow.python.framework.ops.Graph object at 0x7f2d9bc23ef0>) of the iterator is different from the graph (<tensorflow.python.framework.ops.Graph object at 0x7f2da3790630>) the dataset: <DatasetV1Adapter shapes: (?,), types: tf.string> was created in. If you are using the Estimator API, make sure that no part of the dataset returned by the `input_fn` function is defined outside the `input_fn` function.Please ensure that all datasets in the pipeline are created in the same graph as the iterator. NOTE: This warning will become an error in future versions of TensorFlow.\n",
            "WARNING:tensorflow:The graph (<tensorflow.python.framework.ops.Graph object at 0x7f2d9bc23ef0>) of the iterator is different from the graph (<tensorflow.python.framework.ops.Graph object at 0x7f2da3790630>) the dataset: <DatasetV1Adapter shapes: (), types: tf.string> was created in. If you are using the Estimator API, make sure that no part of the dataset returned by the `input_fn` function is defined outside the `input_fn` function.Please ensure that all datasets in the pipeline are created in the same graph as the iterator. NOTE: This warning will become an error in future versions of TensorFlow.\n",
            "WARNING:tensorflow:The graph (<tensorflow.python.framework.ops.Graph object at 0x7f2d9bc23ef0>) of the iterator is different from the graph (<tensorflow.python.framework.ops.Graph object at 0x7f2da3790630>) the dataset: <DatasetV1Adapter shapes: (), types: tf.string> was created in. If you are using the Estimator API, make sure that no part of the dataset returned by the `input_fn` function is defined outside the `input_fn` function.Please ensure that all datasets in the pipeline are created in the same graph as the iterator. NOTE: This warning will become an error in future versions of TensorFlow.\n",
            "WARNING:tensorflow:The graph (<tensorflow.python.framework.ops.Graph object at 0x7f2d9bc23ef0>) of the iterator is different from the graph (<tensorflow.python.framework.ops.Graph object at 0x7f2da3790630>) the dataset: <TextLineDatasetV1 shapes: (), types: tf.string> was created in. If you are using the Estimator API, make sure that no part of the dataset returned by the `input_fn` function is defined outside the `input_fn` function.Please ensure that all datasets in the pipeline are created in the same graph as the iterator. NOTE: This warning will become an error in future versions of TensorFlow.\n",
            "[0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z8oia-fDJ4cx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3-3. Logistic Regression\n",
        "\n",
        "Now, we can build a logistic regression example on TensorFlow by following the steps below.\n",
        "\n",
        "**1.Download and Read MNIST data using the dataset API**\n",
        "\n",
        "**2. Create weights and biases**\n",
        "\n",
        "**3. Build a logistic regression model**\n",
        "\n",
        "**4. Define a loss function**\n",
        "\n",
        "**5. Define a training op**\n",
        "\n",
        "**6. Train and calculate accuracy**"
      ]
    },
    {
      "metadata": {
        "id": "WYPbCvJxL8fH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download and Read MNIST data using the dataset API"
      ]
    },
    {
      "metadata": {
        "id": "m_Pt_YweMFTP",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "b9047b85-5c4a-4eb5-805a-7eda03e84c25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        }
      },
      "cell_type": "code",
      "source": [
        "#@title Download the MNIST dataset\n",
        "import tensorflow\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets('data/mnist', one_hot = True)    # if \"MNIST/data\" does not work, try \"data/mnist\"\n",
        "\n",
        "import os\n",
        "import gzip\n",
        "import shutil\n",
        "import struct\n",
        "import urllib\n",
        "import time\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def huber_loss(labels, predictions, delta=14.0):\n",
        "    residual = tf.abs(labels - predictions)\n",
        "    def f1(): return 0.5 * tf.square(residual)\n",
        "    def f2(): return delta * residual - 0.5 * tf.square(delta)\n",
        "    return tf.cond(residual < delta, f1, f2)\n",
        "\n",
        "def safe_mkdir(path):\n",
        "    \"\"\" Create a directory if there isn't one already. \"\"\"\n",
        "    try:\n",
        "        os.mkdir(path)\n",
        "    except OSError:\n",
        "        pass\n",
        "\n",
        "def read_birth_life_data(filename):\n",
        "    \"\"\"\n",
        "    Read in birth_life_2010.txt and return:\n",
        "    data in the form of NumPy array\n",
        "    n_samples: number of samples\n",
        "    \"\"\"\n",
        "    text = open(filename, 'r').readlines()[1:]\n",
        "    data = [line[:-1].split('\\t') for line in text]\n",
        "    births = [float(line[1]) for line in data]\n",
        "    lifes = [float(line[2]) for line in data]\n",
        "    data = list(zip(births, lifes))\n",
        "    n_samples = len(data)\n",
        "    data = np.asarray(data, dtype=np.float32)\n",
        "    return data, n_samples\n",
        "\n",
        "def download_one_file(download_url, \n",
        "                    local_dest, \n",
        "                    expected_byte=None, \n",
        "                    unzip_and_remove=False):\n",
        "    \"\"\" \n",
        "    Download the file from download_url into local_dest\n",
        "    if the file doesn't already exists.\n",
        "    If expected_byte is provided, check if \n",
        "    the downloaded file has the same number of bytes.\n",
        "    If unzip_and_remove is True, unzip the file and remove the zip file\n",
        "    \"\"\"\n",
        "    if os.path.exists(local_dest) or os.path.exists(local_dest[:-3]):\n",
        "        print('%s already exists' %local_dest)\n",
        "    else:\n",
        "        print('Downloading %s' %download_url)\n",
        "        local_file, _ = urllib.request.urlretrieve(download_url, local_dest)\n",
        "        file_stat = os.stat(local_dest)\n",
        "        if expected_byte:\n",
        "            if file_stat.st_size == expected_byte:\n",
        "                print('Successfully downloaded %s' %local_dest)\n",
        "                if unzip_and_remove:\n",
        "                    with gzip.open(local_dest, 'rb') as f_in, open(local_dest[:-3],'wb') as f_out:\n",
        "                        shutil.copyfileobj(f_in, f_out)\n",
        "                    os.remove(local_dest)\n",
        "            else:\n",
        "                print('The downloaded file has unexpected number of bytes')\n",
        "\n",
        "def download_mnist(path):\n",
        "    \"\"\" \n",
        "    Download and unzip the dataset mnist if it's not already downloaded \n",
        "    Download from http://yann.lecun.com/exdb/mnist\n",
        "    \"\"\"\n",
        "    safe_mkdir(path)\n",
        "    url = 'http://yann.lecun.com/exdb/mnist'\n",
        "    filenames = ['train-images-idx3-ubyte.gz',\n",
        "                'train-labels-idx1-ubyte.gz',\n",
        "                't10k-images-idx3-ubyte.gz',\n",
        "                't10k-labels-idx1-ubyte.gz']\n",
        "    expected_bytes = [9912422, 28881, 1648877, 4542]\n",
        "\n",
        "    for filename, byte in zip(filenames, expected_bytes):\n",
        "        download_url = os.path.join(url, filename)\n",
        "        local_dest = os.path.join(path, filename)\n",
        "        download_one_file(download_url, local_dest, byte, True)\n",
        "\n",
        "# Parse each MNIST data instance as images and labels\n",
        "def parse_data(path, dataset, flatten):\n",
        "    if dataset != 'train' and dataset != 't10k':\n",
        "        raise NameError('dataset must be train or t10k')\n",
        "\n",
        "    label_file = os.path.join(path, dataset + '-labels-idx1-ubyte')\n",
        "    with open(label_file, 'rb') as file:\n",
        "        _, num = struct.unpack(\">II\", file.read(8))\n",
        "        labels = np.fromfile(file, dtype=np.int8) #int8\n",
        "        new_labels = np.zeros((num, 10))\n",
        "        new_labels[np.arange(num), labels] = 1\n",
        "    \n",
        "    img_file = os.path.join(path, dataset + '-images-idx3-ubyte')\n",
        "    with open(img_file, 'rb') as file:\n",
        "        _, num, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
        "        imgs = np.fromfile(file, dtype=np.uint8).reshape(num, rows, cols) #uint8\n",
        "        imgs = imgs.astype(np.float32) / 255.0\n",
        "        if flatten:\n",
        "            imgs = imgs.reshape([num, -1])\n",
        "\n",
        "    return imgs, new_labels\n",
        "  \n",
        "# Read in the mnist dataset, given that the data is stored in path\n",
        "# Return two tuples of numpy arrays\n",
        "# ((train_imgs, train_labels), (test_imgs, test_labels))\n",
        "def read_mnist(path, flatten=True, num_train=55000):\n",
        "    imgs, labels = parse_data(path, 'train', flatten)\n",
        "    indices = np.random.permutation(labels.shape[0])\n",
        "    train_idx, val_idx = indices[:num_train], indices[num_train:]\n",
        "    train_img, train_labels = imgs[train_idx, :], labels[train_idx, :]\n",
        "    val_img, val_labels = imgs[val_idx, :], labels[val_idx, :]\n",
        "    test = parse_data(path, 't10k', flatten)\n",
        "    return (train_img, train_labels), (val_img, val_labels), test\n",
        "\n",
        "mnist_folder = \"data/mnist-\"\n",
        "download_mnist(mnist_folder)\n",
        "train, val, test = read_mnist(mnist_folder, flatten=True)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-41-78c056a78e9b>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting data/mnist/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting data/mnist/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting data/mnist/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting data/mnist/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded data/mnist-/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded data/mnist-/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded data/mnist-/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Successfully downloaded data/mnist-/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lq_2ypB720KU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Build an input pipeline using the dataset API\n",
        "Create datasets from numpy arrays created by reading the MNIST dataset files. \n",
        "\n",
        "Then, make them as batches with batch size of 128."
      ]
    },
    {
      "metadata": {
        "id": "pvuVMsrttDu6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "8cd9492d-d519-4ba7-93a6-0e6bdaec57be"
      },
      "cell_type": "code",
      "source": [
        "train"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
              " array([[1., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 1., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 1., 0., 0.],\n",
              "        [0., 1., 0., ..., 0., 0., 0.]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "metadata": {
        "id": "N1-_jKWwNGyC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  # data created above by the following method\n",
        "  # train, val, test = read_mnist(mnist_folder, flatten=True)\n",
        "  train_data = tf.data.Dataset.from_tensor_slices(train)\n",
        "  train_data = train_data.shuffle(10000)\n",
        "  train_data = train_data.batch(batch_size)\n",
        "\n",
        "  test_data = tf.data.Dataset.from_tensor_slices(test)\n",
        "  test_data = test_data.batch(batch_size)\n",
        "\n",
        "  iterator = tf.data.Iterator.from_structure(train_data.output_types, train_data.output_shapes)\n",
        "  img, label = iterator.get_next()\n",
        "\n",
        "  train_init = iterator.make_initializer(train_data) #initializer for train_data\n",
        "  test_init = iterator.make_initializer(test_data) #initializer for test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pn7eh950IlVn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "8H5Yn3pxNVOH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create weights and bias\n",
        "w is initialized to random normal distribution variables with mean 0 and standard deviation 0.01. b is initialized to 0's. The shape of w depends on the dimensions of X and Y so that Y = tf.matmul(X,w). The shape of b depends on Y."
      ]
    },
    {
      "metadata": {
        "id": "PKOmF9JfNZLc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with graph.as_default():\n",
        "  w = tf.get_variable(name='weights_dataset', shape=(784, 10), initializer=tf.random_normal_initializer(0, 0.01))\n",
        "  b = tf.get_variable(name='bias_dataset', shape=(1, 10), initializer=tf.zeros_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wkJhz1tCNcih",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build a logistic regression model\n",
        "\n",
        "This model returns logits that will be passed into the softmax layer."
      ]
    },
    {
      "metadata": {
        "id": "h_ETGSYdNhcz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with graph.as_default():\n",
        "  logits = tf.matmul(img, w) + b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6h4HwTbMNkx6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define a loss function\n",
        "\n",
        "The cross entropy of softmax of logits is our loss function."
      ]
    },
    {
      "metadata": {
        "id": "7hHviSd5Nqa5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with graph.as_default():\n",
        "  entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=label, name='entropy')\n",
        "  loss = tf.reduce_mean(entropy, name='loss') # average over all the examples in the batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nZDF4B7jN5r9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define a training op\n",
        "\n",
        "We'll use an Adam optimizer with a learning rate of 0.01 to minimize loss."
      ]
    },
    {
      "metadata": {
        "id": "hGPK-eWzN4yc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with graph.as_default():\n",
        "  optimizer = tf.train.AdamOptimizer(0.01)\n",
        "  train_op = optimizer.minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0BjGaGU4OM_O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train and calculate accuracy\n",
        "\n",
        "Finally, we train the model and use the test set to calculate the accuracy of our model."
      ]
    },
    {
      "metadata": {
        "id": "mfLm2JTjOOyy",
        "colab_type": "code",
        "outputId": "bfdfd41d-48bd-4f4c-b79e-142eab6977d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        }
      },
      "cell_type": "code",
      "source": [
        "with graph.as_default():\n",
        "  preds = tf.nn.softmax(logits)\n",
        "  correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(label, 1))\n",
        "  accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
        "\n",
        "with tf.Session(graph=graph) as sess:\n",
        "   \n",
        "    start_time = time.time()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # train the model n_epochs times\n",
        "    for i in range(30): \t\n",
        "        sess.run(train_init)\t# initialize training dataset iterator\n",
        "        total_loss = 0\n",
        "        n_batches = 0\n",
        "        try:\n",
        "            while True:\n",
        "                _, l = sess.run([train_op, loss])\n",
        "                total_loss += l\n",
        "                n_batches += 1\n",
        "        except tf.errors.OutOfRangeError:\n",
        "            pass\n",
        "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))        \n",
        "    print('Total time: {0} seconds'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "    # test the model\n",
        "    sess.run(test_init)\t\t\t# initialize test dataset iterator\n",
        "    total_correct_preds = 0\n",
        "    try:\n",
        "        while True:\n",
        "            accuracy_batch = sess.run(accuracy)\n",
        "            total_correct_preds += accuracy_batch\n",
        "    except tf.errors.OutOfRangeError:\n",
        "        pass\n",
        "    print('Accuracy {0}'.format(total_correct_preds/10000))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average loss epoch 0: 0.3668336667293726\n",
            "Average loss epoch 1: 0.29454850987639536\n",
            "Average loss epoch 2: 0.28413341130281605\n",
            "Average loss epoch 3: 0.27795317228450334\n",
            "Average loss epoch 4: 0.2754930011061735\n",
            "Average loss epoch 5: 0.2710635873294154\n",
            "Average loss epoch 6: 0.2696129959856355\n",
            "Average loss epoch 7: 0.2703531366967878\n",
            "Average loss epoch 8: 0.2632993358858796\n",
            "Average loss epoch 9: 0.2643631101001141\n",
            "Average loss epoch 10: 0.2641341299684935\n",
            "Average loss epoch 11: 0.2619624991056531\n",
            "Average loss epoch 12: 0.2590268860895966\n",
            "Average loss epoch 13: 0.26181176800367445\n",
            "Average loss epoch 14: 0.2575613180218741\n",
            "Average loss epoch 15: 0.25733905547233515\n",
            "Average loss epoch 16: 0.25472047051718066\n",
            "Average loss epoch 17: 0.25659440840746084\n",
            "Average loss epoch 18: 0.2555305506081082\n",
            "Average loss epoch 19: 0.2545956387422806\n",
            "Average loss epoch 20: 0.25689747330061224\n",
            "Average loss epoch 21: 0.25524225801576017\n",
            "Average loss epoch 22: 0.25486626663180284\n",
            "Average loss epoch 23: 0.253238767073598\n",
            "Average loss epoch 24: 0.25418776776208435\n",
            "Average loss epoch 25: 0.2521757761233075\n",
            "Average loss epoch 26: 0.25321996717952017\n",
            "Average loss epoch 27: 0.2544230682732061\n",
            "Average loss epoch 28: 0.24951384734622267\n",
            "Average loss epoch 29: 0.2527007594704628\n",
            "Total time: 25.239198446273804 seconds\n",
            "Accuracy 0.9183\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f95Z4ZA4trUL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}