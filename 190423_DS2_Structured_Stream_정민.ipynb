{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS2_Structured_Stream.ipynb의 사본",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "857s7aGUv88v",
        "colab_type": "text"
      },
      "source": [
        "Copyright (C) 2019 Software Platform Lab, Seoul National University\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BZ8CyFiv_iD",
        "colab_type": "text"
      },
      "source": [
        "# Part 0: Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-TdhkUDbGfc",
        "colab_type": "code",
        "outputId": "708a673e-bd03-4930-f5f9-b1f01ddb0267",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        }
      },
      "source": [
        "# Setting up spark\n",
        "!rm -rf /content/*\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.3.2/spark-2.3.2-bin-hadoop2.7.tgz  #자바설치\n",
        "!tar -xf /content/spark-2.3.2-bin-hadoop2.7.tgz #스파크 다운 받아서 \n",
        "!pip install -q findspark # findspark 가지고 스파크 찾아서 설치. \n",
        "# Download necessary dependency file for Kafka\n",
        "!wget -q http://central.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.11/2.3.2/spark-sql-kafka-0-10_2.11-2.3.2.jar\n",
        "!wget -q http://central.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-10-assembly_2.11/2.3.2/spark-streaming-kafka-0-10-assembly_2.11-2.3.2.jar\n",
        "!ls /content/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rIgn:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.88.162)] [W\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,609 B]\n",
            "Hit:3 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:9 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:11 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [60.4 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:15 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,639 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [410 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [4,171 B]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [301 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,103 kB]\n",
            "Get:20 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [788 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [752 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [7,238 B]\n",
            "Fetched 5,335 kB in 5s (1,137 kB/s)\n",
            "Reading package lists... Done\n",
            "spark-2.3.2-bin-hadoop2.7\n",
            "spark-2.3.2-bin-hadoop2.7.tgz\n",
            "spark-sql-kafka-0-10_2.11-2.3.2.jar\n",
            "spark-streaming-kafka-0-10-assembly_2.11-2.3.2.jar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzzBCUnufKQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting the environment variable\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.2-bin-hadoop2.7\"\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--jars /content/spark-sql-kafka-0-10_2.11-2.3.2.jar,/content/spark-streaming-kafka-0-10-assembly_2.11-2.3.2.jar pyspark-shell\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm4P6Vyqjcob",
        "colab_type": "text"
      },
      "source": [
        "# Part 1: Running a simple wordcount query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNPNeB_-t5Yp",
        "colab_type": "text"
      },
      "source": [
        "We will implement a simple continuous wordcount query. This query will\n",
        "* Read the text sentence from a Kafka\n",
        "* Split the sentence into words\n",
        "* Continuously aggregate the counts for each word\n",
        "\n",
        "Firstly, we need to start from making a simple TCP server on the master server which produces random sentences to its clients. In this class, we will use `nc (netcat)` program. You need to setup your own TCP server by following the processes\n",
        "\n",
        "* Open the new terminal from jupyter\n",
        "* Start a TCP server by entering \"nc -lk 20332\"\n",
        "* Enter arbitrary texts to send events\n",
        "\n",
        "After running the simple TCP server, run the scripts below. The query will continuously run in a background thread."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xukqxaea84jW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "import os\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DS2\") \\\n",
        "    .master('local[*]') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Get the text data stream from TCP server\n",
        "lines = spark \\\n",
        "    .readStream \\\n",
        "    .format(\"socket\") \\\n",
        "    .option(\"host\", \"147.46.216.122\") \\\n",
        "    .option(\"port\", 20332) \\\n",
        "    .load()\n",
        "    \n",
        "words = lines.select(\n",
        "    explode(\n",
        "        split(lines.value, \" \")\n",
        "    ).alias(\"word\")\n",
        ")\n",
        "\n",
        "wordCounts = words.groupBy(\"word\").count()\n",
        "\n",
        "query = wordCounts \\\n",
        "    .writeStream \\\n",
        "    .queryName(\"wordcount_simple\") \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMf0lR4bF0_l",
        "colab_type": "text"
      },
      "source": [
        "The running query will process the incoming text data from the TCP server. You may see the result by running the script below. Execute the script repeatedly, and you will see the evolving result as the data arrives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phjKn1FGFilV",
        "colab_type": "code",
        "outputId": "46935d11-2978-4545-efd1-df3a197bdb9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "result = spark.table(\"wordcount_simple\")\n",
        "result.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----+\n",
            "|word|count|\n",
            "+----+-----+\n",
            "+----+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVyLyZ1KPiWy",
        "colab_type": "text"
      },
      "source": [
        "You may stop the running query with the following script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAlapIZvRjUh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgTNYiQJUn-3",
        "colab_type": "text"
      },
      "source": [
        "# Part 2: Running a stream application from Kafka source\n",
        "\n",
        "Apache Kafka is a distributed streaming platform which supports messaging, processing, and storing of the stream data. In this practice session, we will focus on leveraging Kafka as a message brokering system.\n",
        "\n",
        "Kafka supports high-throughput & fault-tolerant messaging via publish-subscribe model. In publish-subscribe model, stream events are managed in **topics**. A **Producer** consistently generates a data, whereas **Consumer** receives the data events. Each topic is partitioned into multiple \"partitions\", and partitions are distributed and stored in the secondary storage to guarantee fault tolerance.\n",
        "\n",
        "As we can guess from the information above, we need the server address and topic name to fetch the data from a Kafka broker. Kafka server and producers are already set up by TAs. We will review the Producer code firstly.\n",
        "\n",
        "After revewing the code, we will implement the same word count application from the Kafka source. The broker address is **147.46.216.122:9092** and the topic is **wc**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZIjjH08ZAiy",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kdh3ei3hU7Fa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "import os\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DS2\") \\\n",
        "    .master('local[*]') \\\n",
        "    .getOrCreate()\n",
        "    \n",
        "# Get the text data stream from Kafka\n",
        "lines = spark \\\n",
        "    .readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", \"147.46.216.122:9092\") \\\n",
        "    .option(\"subscribe\", \"wc\") \\\n",
        "    .load()\n",
        "    \n",
        "words = lines.select(\n",
        "    explode(\n",
        "        split(lines.value, \" \")\n",
        "    ).alias(\"word\")\n",
        ")\n",
        "\n",
        "wordCounts = words.groupBy(\"word\").count()\n",
        "\n",
        "query = wordCounts \\\n",
        "    .writeStream \\\n",
        "    .queryName(\"wordcount_kafka\") \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2bnqbgJX8QQ",
        "colab_type": "text"
      },
      "source": [
        "You may see the result and stop the query with the following scripts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo13lpL3YAuR",
        "colab_type": "code",
        "outputId": "5f939575-c3ce-4a7e-9dba-1a6710f81bba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        }
      },
      "source": [
        "result = spark.table(\"wordcount_kafka\")\n",
        "result.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-----+\n",
            "|    word|count|\n",
            "+--------+-----+\n",
            "|    \"DS2|   24|\n",
            "|practice|   30|\n",
            "|      is|   24|\n",
            "|    day\"|   22|\n",
            "|   \"Have|   22|\n",
            "|session\"|   30|\n",
            "|    fun\"|   24|\n",
            "|\"Welcome|   30|\n",
            "|   class|   24|\n",
            "|\"Session|   32|\n",
            "|       a|   22|\n",
            "|    nice|   22|\n",
            "|expired\"|   32|\n",
            "|      to|   30|\n",
            "|     DS2|   30|\n",
            "+--------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sFDvpyAZA08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehQ5l-_-Ze1e",
        "colab_type": "text"
      },
      "source": [
        "# Quiz 0: Simple Filtered Aggregation\n",
        "\n",
        "In the following cell, implement the word count example which gets the data from the **\"wc\"** topic according to the following condition.\n",
        "* Do not count the words whose lengths are shorter than 3\n",
        "\n",
        "Hint: Use *length()* function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoOXB_r9fBIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "import os\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DS2\") \\\n",
        "    .master('local[*]') \\\n",
        "    .getOrCreate()\n",
        "    \n",
        "# Get the text data stream from Kafka\n",
        "lines = spark \\\n",
        "    .readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", \"147.46.216.122:9092\") \\\n",
        "    .option(\"subscribe\", \"wc\") \\\n",
        "    .load()\n",
        "    \n",
        "words = lines.select(\n",
        "    explode(\n",
        "        split(lines.value, \" \")\n",
        "    ).alias(\"word\")\n",
        ")\n",
        "\n",
        "# Implement your code here!\n",
        "filteredWords = words.select('*').where(length('word')>=3)#### quiz solution\n",
        "\n",
        "filteredWordCounts = filteredWords.groupBy(\"word\").count()#### quiz solution\n",
        "\n",
        "query = filteredWordCounts \\\n",
        "    .writeStream \\\n",
        "    .queryName(\"quiz_0\") \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6vfetuafm-I",
        "colab_type": "code",
        "outputId": "bc0eaf69-4ed2-4516-82bc-089d120d0757",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "result = spark.table(\"quiz_0\")\n",
        "result.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----+\n",
            "|word|count|\n",
            "+----+-----+\n",
            "+----+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogDg0GYNfosW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IyfQL0NkmUw",
        "colab_type": "text"
      },
      "source": [
        "# Part 3: Processing JSON-formatted data\n",
        "\n",
        "Until now, we processed only the simple plain texts. From this time, we will process JSON-formatted data events which are widely used for data transfer. Here, we will get the json-formatted movie datasets from the kafka server. We may get the data from the **movie** topic.\n",
        "\n",
        "To parse json-formatted data into spark dataframe, you need to use `from_json()` function and type casting.\n",
        "\n",
        "The script below parses the json events to dataframe and filters out the events whose \"year\" is less than 2000."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5w_q9J8n9Po",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import findspark\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "## Make a spark sql session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DS2\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "## Get the json-formatted data from Kafka stream\n",
        "kafka_movies = spark \\\n",
        "    .readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", \"147.46.216.122:9092\") \\\n",
        "    .option(\"subscribe\", \"movie\") \\\n",
        "    .load()\n",
        "\n",
        "## Change the JSON events into relational tuples with string types\n",
        "relational_movies = kafka_movies.select([get_json_object(col(\"value\").cast(\"string\"), \"$.{}\".format(c)).alias(c)\n",
        "    for c in [\"title\", \"genre\", \"year\", \"time\"]])\n",
        "\n",
        "## Change the type of year from string to integer,\n",
        "## and change the type of time from string to timestamp\n",
        "relational_movies = relational_movies.select(col(\"title\"), col(\"genre\"),\n",
        "        relational_movies.year.cast('integer').alias('year'),\n",
        "        col(\"time\").cast('double').cast('timestamp')\n",
        "    )\n",
        "\n",
        "## Select the movie with year < 2000\n",
        "filtered_movies = relational_movies.select(\"*\").where(\"year >= 2000\")\n",
        "\n",
        "query = filtered_movies \\\n",
        "    .writeStream \\\n",
        "    .queryName(\"twentyfirstcentury_movies\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WsS3MNWoa2m",
        "colab_type": "code",
        "outputId": "11c36868-6d8e-47ee-808c-edba9d2a0c42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "results = spark.table(\"twentyfirstcentury_movies\")\n",
        "results.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+----------------+----+--------------------+\n",
            "|          title|           genre|year|                time|\n",
            "+---------------+----------------+----+--------------------+\n",
            "|       Hercules|          Action|2014|2019-04-23 07:34:...|\n",
            "|Project Almanac|        Thriller|2015|2019-04-23 07:34:...|\n",
            "| Antwone Fisher|Biography, Drama|2002|2019-04-23 07:34:...|\n",
            "| Material Girls|          Comedy|2006|2019-04-23 07:34:...|\n",
            "|   My Big Break|     Documentary|2001|2019-04-23 07:34:...|\n",
            "|       Out Cold|          Comedy|2001|2019-04-23 07:34:...|\n",
            "|Monsoon Wedding|          Comedy|2001|2019-04-23 07:35:...|\n",
            "|Superhero Movie|  Comedy, parody|2008|2019-04-23 07:35:...|\n",
            "+---------------+----------------+----+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jksiSB0YpXG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljUOsYxltmMW",
        "colab_type": "text"
      },
      "source": [
        "# Quiz 1: Filtered Aggregation on JSON-formatted data\n",
        "\n",
        "In the following cell, implement the stream application which receives the **movie** topic from the Kafka stream and filters out all the movies which does not contain the word \"the\" in their titles (cases are ignored). \n",
        "\n",
        "After that, count the number of movies within the filtered data stream.\n",
        "\n",
        "**Input**: {\"title\": \"The titanic\", \"genre\": \"drama\", ...}, {\"title\": \"Titanic\", \"genre\": \"drama, ...\"}, {\"title\": \"Flintheart Glomgold\", \"genre\": \"comedy\", ...}\n",
        "\n",
        "**Output**: (\"drama\", 1), (\"comedy\", 1)\n",
        "\n",
        "Hint: Use *lower()* function and *like* clauses with *wildcard (%)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqyLcqsuv_3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import findspark\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode\n",
        "from pyspark.sql.functions import split\n",
        "from pyspark.sql.functions import col, from_json, get_json_object\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "## Make a spark sql session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"IAB\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "## Get the json-formatted data from Kafka stream\n",
        "kafka_movies = spark \\\n",
        "    .readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", \"147.46.216.122:9092\") \\\n",
        "    .option(\"subscribe\", \"movie\") \\\n",
        "    .load()\n",
        "\n",
        "## Change the JSON events into relational tuples with string types\n",
        "relational_movies = kafka_movies.select([get_json_object(col(\"value\").cast(\"string\"), \"$.{}\".format(c)).alias(c)\n",
        "    for c in [\"title\", \"genre\", \"year\", \"time\"]])\n",
        "\n",
        "## Change the type of year from string to integer,\n",
        "## and change the type of time from string to timestamp\n",
        "relational_movies = relational_movies.select(col(\"title\"), col(\"genre\"),\n",
        "        relational_movies.year.cast('integer').alias('year'),\n",
        "        col(\"time\").cast('double').cast('timestamp')\n",
        "    )\n",
        "\n",
        "# Implement your code here!\n",
        "filtered_movies = relational_movies.where(\"lower(title) like '%the%'\")\n",
        "aggregated_counts = filtered_movies.groupBy('genre').count()\n",
        "\n",
        "\n",
        "query = aggregated_counts \\\n",
        "    .writeStream \\\n",
        "    .queryName(\"quiz_1\") \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .start()\n",
        "\n",
        "# .outputMode(\"append\") 는 row data를 뿌려줄 때 사용. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLT8YkmYxQMl",
        "colab_type": "code",
        "outputId": "5533903f-9d78-4a4f-f70d-f8f23e40a4d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "result = spark.table(\"quiz_1\")\n",
        "result.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+-----+\n",
            "|genre|count|\n",
            "+-----+-----+\n",
            "+-----+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Egf4EFLfxQpQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsTml2E5xzfZ",
        "colab_type": "text"
      },
      "source": [
        "# Part 4: Running a windowed stream application\n",
        "\n",
        "By windowing, we can continuously get the set of recent data. A time-based **sliding window** can be defined by **window size** and **sliding interval**. For example, the window of `(window size = 5 seconds, sliding interval = 1 seconds)` consistently emits the data events in recent five seconds for every one second. For the special cases when the window size and the sliding interval are same, we call them as **tumbling windows**. In structured stream, it is possible to make windows from **event times**.\n",
        "\n",
        "Let's make a windowed movie aggregation query, which counts the frequency of each genre within a sliding window (size = 30 seconds, interval = 5 seconds)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJuMcXvjzUn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import findspark\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "## Make a spark sql session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DS2\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "## Get the json-formatted data from Kafka stream\n",
        "kafka_movies = spark \\\n",
        "    .readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", \"147.46.216.122:9092\") \\\n",
        "    .option(\"subscribe\", \"movie\") \\\n",
        "    .load()\n",
        "\n",
        "## Change the JSON events into relational tuples with string types\n",
        "relational_movies = kafka_movies.select([get_json_object(col(\"value\").cast(\"string\"), \"$.{}\".format(c)).alias(c)\n",
        "    for c in [\"title\", \"genre\", \"year\", \"time\"]])\n",
        "\n",
        "## Change the type of year from string to integer,\n",
        "## and change the type of time from string to timestamp\n",
        "relational_movies = relational_movies.select(col(\"title\"), col(\"genre\"),\n",
        "        relational_movies.year.cast('integer').alias('year'),\n",
        "        col(\"time\").cast('double').cast('timestamp')\n",
        "    )\n",
        "\n",
        "## Make windows and aggregate\n",
        "windowed_movies = relational_movies.groupBy(\n",
        "    window(relational_movies.time, \"30 seconds\", \"5 seconds\"),\n",
        "    \"genre\"\n",
        "  ).count()\n",
        "\n",
        "query = windowed_movies \\\n",
        "    .writeStream \\\n",
        "    .queryName(\"windowed_movies\") \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYZiXhzE0AQw",
        "colab_type": "code",
        "outputId": "b8b5ed97-72a9-48dc-86ca-dc4fbbb150b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "result = spark.table(\"windowed_movies\")\n",
        "result.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-------+-----+\n",
            "|              window|  genre|count|\n",
            "+--------------------+-------+-----+\n",
            "|[2019-04-23 04:25...|Western|    1|\n",
            "|[2019-04-23 04:25...|Western|    1|\n",
            "|[2019-04-23 04:25...|Western|    1|\n",
            "|[2019-04-23 04:25...|Western|    1|\n",
            "|[2019-04-23 04:25...|Western|    1|\n",
            "|[2019-04-23 04:26...|Western|    1|\n",
            "+--------------------+-------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTPXgRXY0AvE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3YZ_MsQo_vN",
        "colab_type": "text"
      },
      "source": [
        "# Quiz 2: Windowed aggregation by the first character\n",
        "\n",
        "In this quiz, you will implement a sliding-window aggregation query with *movie* stream query.\n",
        "\n",
        "Implement a query which counts the number of movies according to the first characters of their *titles* (cases are ignored) within a sliding window (size = 30 seconds, interval = 5 seconds)\n",
        "\n",
        "Example:\n",
        "\n",
        "**Input**: {\"title\": \"The titanic\", ...}, {\"title\": \"Avengers\", ...}, {\"title\": \"a little boy\", ...}\n",
        "\n",
        "**Output**: (\"t\", 1), (\"a\", 2)\n",
        "\n",
        "Hint: Use substring() function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26PthwgNp56R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import findspark\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "## Make a spark sql session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DS2\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "## Get the json-formatted data from Kafka stream\n",
        "kafka_movies = spark \\\n",
        "    .readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", \"147.46.216.122:9092\") \\\n",
        "    .option(\"subscribe\", \"movie\") \\\n",
        "    .load()\n",
        "\n",
        "## Change the JSON events into relational tuples with string types\n",
        "relational_movies = kafka_movies.select([get_json_object(col(\"value\").cast(\"string\"), \"$.{}\".format(c)).alias(c)\n",
        "    for c in [\"title\", \"genre\", \"year\", \"time\"]])\n",
        "\n",
        "## Change the type of year from string to integer,\n",
        "## and change the type of time from string to timestamp\n",
        "relational_movies = relational_movies.select(col(\"title\"), col(\"genre\"),\n",
        "        relational_movies.year.cast('integer').alias('year'),\n",
        "        col(\"time\").cast('double').cast('timestamp')\n",
        "    )\n",
        "\n",
        "# Implement your code here!\n",
        "## Extract the lower-cased first characters from relational_movie\n",
        "first_characters = relational_movies \\\n",
        "        .select(substring(lower(relational_movies.title), 1, 1).alias(\"first_character\"), \"time\")\n",
        "\n",
        "## Make windows and aggregate\n",
        "windowed_movies = first_characters.groupBy(\n",
        "    window(first_characters.time, \"30 seconds\", \"5 seconds\"),\n",
        "    first_characters.first_character\n",
        "  ).count()\n",
        "\n",
        "query = windowed_movies \\\n",
        "    .writeStream \\\n",
        "    .queryName(\"quiz_2\") \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU5Wunfiqp5t",
        "colab_type": "code",
        "outputId": "1fb959ef-b245-4672-9280-d6fe07592408",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "result = spark.table(\"quiz_2\")\n",
        "result.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+---------------+-----+\n",
            "|              window|first_character|count|\n",
            "+--------------------+---------------+-----+\n",
            "|[2019-04-23 04:26...|              b|    1|\n",
            "|[2019-04-23 04:26...|              b|    1|\n",
            "|[2019-04-23 04:26...|              j|    1|\n",
            "|[2019-04-23 04:26...|              m|    2|\n",
            "|[2019-04-23 04:26...|              d|    2|\n",
            "|[2019-04-23 04:26...|              b|    1|\n",
            "|[2019-04-23 04:26...|              b|    1|\n",
            "|[2019-04-23 04:26...|              d|    2|\n",
            "|[2019-04-23 04:26...|              m|    2|\n",
            "|[2019-04-23 04:26...|              j|    1|\n",
            "|[2019-04-23 04:26...|              t|    2|\n",
            "|[2019-04-23 04:26...|              p|    1|\n",
            "|[2019-04-23 04:26...|              t|    2|\n",
            "|[2019-04-23 04:26...|              p|    1|\n",
            "|[2019-04-23 04:26...|              p|    1|\n",
            "|[2019-04-23 04:26...|              j|    1|\n",
            "|[2019-04-23 04:26...|              m|    2|\n",
            "|[2019-04-23 04:27...|              b|    1|\n",
            "|[2019-04-23 04:26...|              j|    1|\n",
            "|[2019-04-23 04:26...|              d|    2|\n",
            "+--------------------+---------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGAkzQaJqsGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}