{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Reinforcement Learning의 사본",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKVeLItnS5uV",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "```\n",
        "Copyright (C) 2019 Software Platform Lab, Seoul National University\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); \n",
        "\n",
        "you may not use this file except in compliance with the License. \n",
        "\n",
        "You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 \n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software \n",
        "\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS, \n",
        "\n",
        "\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n",
        "\n",
        "\n",
        "See the License for the specific language governing permissions and\n",
        "\n",
        "\n",
        "limitations under the License.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-hA_QZNMFxS",
        "colab_type": "text"
      },
      "source": [
        "# Deep Reinforcement Learning (DRL)\n",
        "\n",
        "In this practice session, we will learn how to build a Deep Reinforcement Learning algorithm using TensorFlow. Most of the content is re-constructed from a great blog article [Deep Reinforcement Learning: Playing CartPole through Asynchronous Advantage Actor Critic (A3C) with tf.keras and eager execution](https://medium.com/tensorflow/deep-reinforcement-learning-playing-cartpole-through-asynchronous-advantage-actor-critic-a3c-7eab2eea5296) written by Raymond Yuan, a former Software Engineering Intern at Google. As mentioend earlier, we will use high-level APIs (Keras and Eager execution) that we learned last time, which allows us to build RL models much easier.\n",
        "\n",
        "** \\*\\*NOTE\\*\\* ** As an important reminder, since we will use Eager execution, let's not forget to enable eager execution at the beginning. Otherwise, you will encounter an error when we run the algorithms then you should restart the runtime and re-run all cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r9cT7wHMEAx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "deda233a-4fe1-4ae7-d765-fa356eca1a55"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "print(tf.executing_eagerly()) # Should be True"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ltiicerlg9KF",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## 1. Introduction to DRL\n",
        "\n",
        "Reinforcement learning (RL) is a task to learn how to decide the sequence of *actions* in an uncertain *environment* to achieve some goals (e.g., play a video game and achieve the maximum score). RL has been widely used in many tasks (e.g., video game, robotics, self-driving car) that require sequential decision-making.\n",
        "Similar to many other fields, Deep learning also plays an important role to improve the performance of RL; DRL, a combination of deep learning and reinforcement learning, has been successful in solving complex tasks with lower prior knowledge thanks to its capability to learn different levels of abstractions from data.\n",
        "\n",
        "## 2. CartPole Game\n",
        "\n",
        "### 2-1. What is CartPole?\n",
        "Among many DRL applications, we will implement CartPole in this session. Cartpole is a game in which a pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The starting state (cart position, cart velocity, pole angle, and pole velocity at tip) is randomly initialized between +/-0.05. The system is controlled by applying a force of +1 or -1 to the cart (moving left or right). The pendulum starts upright, and the goal is to prevent it from falling over. A *reward* of +1 is provided for every timestep that the pole remains upright. The *episode* ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
        "\n",
        "The blog post [Introduction to Reinforcement Learning](https://medium.com/@curiousily/getting-your-feet-rewarded-deep-reinforcement-learning-for-hackers-part-0-900ca5bb83e5) (by Venelin Valkov) provides a nice representation and summary of the concepts in CartPole game, as shown below:\n",
        "\n",
        "![cart pole game](https://cdn-images-1.medium.com/max/800/1*2OydMu71FKg3MCVF39T4Xg.png)  \n",
        "\n",
        "* Goal :  Balance the pole on top of a moving cart\n",
        "* States:  angle, angular speed, position, horizontal velocity\n",
        "* Actions:  horizontal force to the cart\n",
        "* Reward : 1 at each time step if the pole is upright\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78hn_ahQ-TED",
        "colab_type": "text"
      },
      "source": [
        "### 2-2. How does the system look like?\n",
        "For a better understanding, let us show a video of an implementation of CartPole in a real-world; the settings are slightly different from what we will build in a sense that the pole swings up instead of staying upright. The system attempts a few *trial*s and at the end of the video, we can see that the pole stays upright for a pretty long time!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T4VTLaEwxVQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "outputId": "795ec092-c4a5-49a4-f56a-53ab0417c614"
      },
      "source": [
        "from IPython.display import HTML # to embed a video in a jupyter notebook\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/XiigTGKZfks\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/XiigTGKZfks\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jPdixEk-cbY",
        "colab_type": "text"
      },
      "source": [
        "### 2-3. OpenAI Gym: library to simulate the environment\n",
        "[OpenAI Gym](https://gym.openai.com) provides environments to develop and compare RL algorithms, which aims to be a standardized benchmark. They also has implemented CartPole, and let's see how it looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51u9SdbJuiQk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "outputId": "68334b16-a843-47d9-b679-701ccf77226e"
      },
      "source": [
        "from IPython.display import HTML\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://gym.openai.com/videos/2019-04-06--My9IiAbqha/CartPole-v1/original.mp4\", frameborder=\"0\" allow=\"accelerometer; repeat; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://gym.openai.com/videos/2019-04-06--My9IiAbqha/CartPole-v1/original.mp4\", frameborder=\"0\" allow=\"accelerometer; repeat; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8woS5NNnPG21",
        "colab_type": "text"
      },
      "source": [
        "Isn't it pretty similar to the real-world system that we saw in the first video? By using the provided environment, it becomes much easier to build and evaluate the DRL algorithms.\n",
        "\n",
        "We will use this environment to demonstrate how our trained agent works. Rendering Gym at the Colab environment is not supported natively but there are some frontiers that worked for it (e.g., [GymRendering.ipynb](https://colab.research.google.com/drive/1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t) and [How to render openai gym in google colab (SO)](https://stackoverflow.com/questions/50107530/how-to-render-openai-gym-in-google-colab)), which we will use below.\n",
        "\n",
        "Let's start with installing some dependencies and importing necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1vexLQQ7RKl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install dependencies\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ts4tlxT7_Jm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "44b0467b-4233-4fdc-a814-0e87a7fd42f3"
      },
      "source": [
        "# Import OpenAI Gym\n",
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "from gym import logger as gymlogger\n",
        "gymlogger.set_level(40) # print error only\n",
        "\n",
        "# Import python libraries\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Setup display\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ9M9eORASlv",
        "colab_type": "text"
      },
      "source": [
        "We can create an environment by calling `gym.make()` function with passing the application name to it. For example, we will run `CartPole-v0` and there are many applications such as `Pendelum-v0` and `Acrobot-v1`. You can see the list of the environment at [OpenAI](http://gym.openai.com/envs/#classic_control) website.\n",
        "\n",
        "`env.action_space` shows the possible actions. In `CartPole-v0`, there are two discrete actions: push the cart in the direction of +1/-1. Let's see how the return value of `env.action_space` looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGzWdq2kATd_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "1b76cd69-ac68-4d1b-81af-f726196ff921"
      },
      "source": [
        "env = gym.make(\"CartPole-v0\") # try other envs (e.g., Acrobot-v1, Pendulum-v0)\n",
        "print(env.action_space)\n",
        "#Discrete 환경이고 두가지 action 이 정의 되어 있다(+1,-1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discrete(2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvgLiJIw-KaT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "d6ce2ed6-eb92-42fb-b321-dc40991d0996"
      },
      "source": [
        "env = gym.make(\"Acrobot-v1\") # try other envs (e.g., Acrobot-v1, Pendulum-v0)\n",
        "print(env.action_space)\n",
        "#3가지의 다른 Action 이 가능하다,."
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discrete(3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPXpGesO8LtY",
        "colab_type": "text"
      },
      "source": [
        "`show_video()` and `wrap_env()` below are the utility functions  to enable video recording of gym environment and displaying it\n",
        "To enable video, we need to use the wrapped environment by calling `env = wrap_env(env)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAPqEki48DRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8_GmPVD_kx5",
        "colab_type": "text"
      },
      "source": [
        "### Quiz1: Play with a dumb agent\n",
        "In CartPole game, you can take two actions and the values are encoded in binary numbers (0|1). The dumbest policy we can implement would be as follows:\n",
        "* Choose 0 only\n",
        "* Choose 1 only\n",
        "* Choose 0 and 1 randomly (hint: `env.action_space.sample()`)\n",
        "\n",
        "Let's try implementing the above policies by changing the logic to decide `action` in the following code and see how the results look different."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91DHYCCG9lHv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "outputId": "9452e6bd-6051-4616-b44d-a0467f95d931"
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "env = wrap_env(env)\n",
        "observation = env.reset()\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "    env.render()\n",
        "#     action = 0\n",
        "#     action = 1\n",
        "# 환경에 정의된 임의의 action 을 골라서 선택하기\n",
        "    action = env.action_space.sample()\n",
        "    ######### Define policy here ########\n",
        "    # action = \n",
        "    ###############################\n",
        "    \n",
        "    observation, reward, done, info = env.step(action) \n",
        "print(reward)\n",
        "env.close()\n",
        "show_video()\n",
        " "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1wNDEAAAAIZnJlZQAADXVtZGF0AAACrgYF//+q3EXpvebZSLeWLNgg2SPu73gyNjQgLSBjb3JlIDE1MiByMjg1NCBlOWE1OTAzIC0gSC4yNjQvTVBFRy00IEFWQyBjb2RlYyAtIENvcHlsZWZ0IDIwMDMtMjAxNyAtIGh0dHA6Ly93d3cudmlkZW9sYW4ub3JnL3gyNjQuaHRtbCAtIG9wdGlvbnM6IGNhYmFjPTEgcmVmPTMgZGVibG9jaz0xOjA6MCBhbmFseXNlPTB4MzoweDExMyBtZT1oZXggc3VibWU9NyBwc3k9MSBwc3lfcmQ9MS4wMDowLjAwIG1peGVkX3JlZj0xIG1lX3JhbmdlPTE2IGNocm9tYV9tZT0xIHRyZWxsaXM9MSA4eDhkY3Q9MSBjcW09MCBkZWFkem9uZT0yMSwxMSBmYXN0X3Bza2lwPTEgY2hyb21hX3FwX29mZnNldD0tMiB0aHJlYWRzPTMgbG9va2FoZWFkX3RocmVhZHM9MSBzbGljZWRfdGhyZWFkcz0wIG5yPTAgZGVjaW1hdGU9MSBpbnRlcmxhY2VkPTAgYmx1cmF5X2NvbXBhdD0wIGNvbnN0cmFpbmVkX2ludHJhPTAgYmZyYW1lcz0zIGJfcHlyYW1pZD0yIGJfYWRhcHQ9MSBiX2JpYXM9MCBkaXJlY3Q9MSB3ZWlnaHRiPTEgb3Blbl9nb3A9MCB3ZWlnaHRwPTIga2V5aW50PTI1MCBrZXlpbnRfbWluPTI1IHNjZW5lY3V0PTQwIGludHJhX3JlZnJlc2g9MCByY19sb29rYWhlYWQ9NDAgcmM9Y3JmIG1idHJlZT0xIGNyZj0yMy4wIHFjb21wPTAuNjAgcXBtaW49MCBxcG1heD02OSBxcHN0ZXA9NCBpcF9yYXRpbz0xLjQwIGFxPTE6MS4wMACAAAABzmWIhAAz//727L4FNf2f0JcRLMXaSnA+KqSAgHc0wAAAAwAAAwAAFgn0I7DkqgN3QAAAHGAFBCwCPCVC2EhH2Olc/N/wTjPQQcCVxMpU43N2X5cmz+/Wj8OFmxvlU6rcaIvriIjeDi1zgVIwE/IxhegYLAvv3X9QAPXHvr6yTxfSpUyYvysfGP8v4Qc1q+8Amb5Uib4Kgdbl9cPMwkZtXUDmQgsVfz9GFmgr+efVnOu9lBxiEfNo/u8UHrvZFcCbsVRysrS7DUt7XABnJ6pmySoK+Pp7BxOY151KJUWghDa8BkC/7wALOpjLUjqjfFQb1xhgPg8KQaiIUOVBhlvPLKO1YWmuH85bLtCls9+WwyQIuBOT8KnZlee5bA7nWPz/dwLl2CdcdAY35DD8BX0VJiqqD6XUWIfSRQkmAUrnNKwva/mCGnXRvU8bGTCsEjJYDM4TDaOKakC9jPSktYIcT/+7DYQvGEvz+lKM7MCuvPw8re0ggoIVryUXxIegXnLn45zm2OFhN3h4L7O5jAAmJuV4uhUSb9mVc4jjWr+AiBCqKwfZxvO9DNaVKUzASdpHxgYgIOEwiSyiM6QADUAAA0SwAAADAAADAAADAAAEfQAAAKdBmiRsQv/+jLAAAEYYhRXhjuO84/sKwSeAAaG6I1Nd02i67MbbM9B35hlz0cqu/xiSjGnxSCMB1hX4iBabxoLq0ECvkLCeKuusjmVG13LJ/0qFdK6PhF15yR74oce5aUAQR9D18rX3fUeQQDa6DUZ1zRUewQRiipznrCFUqn+NGEKTFNJDHAI5IEf9Wjza+G5smxAjCW6z0mzfJoc5MB+DqmYCQgAu4AAAAFRBnkJ4hH8AAAhkoGh9YKxsdLYcBA6h2EoEM0kavyJnxt316AD8TMrc2kU70ybPgA+14dvkhGi/IGAAAAlUXU02mBnmCD07mU6ExBo/euhtUywAK2EAAAA9AZ5hdEf/AAANf0CBKSQChwTKEU8/lqxIkxoQlvbdHQEdAAADAAA++2g73lfClgc767UlZgpqcLNoywAKmAAAADIBnmNqR/8AAA13RaakglbVHhumbZS01TMlksJueIWusnpU3sAAAAMAAAMBheZPPCAl4QAAANtBmmhJqEFomUwIX//+jLAAAEYQf+S1gCPLglnN5rvgXDAukFNZ09QsuTg1QYWF8kw2vtGHjrvdqXkVD+HGTdUx+Hilprx9GQD7fVYu8KfRIdMLw7cp09qFCb5V8LMM+r+aZ23QJ+cQJgDaSvGrx/Nxu0qvPCq4qw781nJbwob3O0q0VYnx5qLrOCI/X18z7aZ8CUXcudwjpth7q4igKvmpQhh8vDfVTvMLSYs0oQ9dc/R4AsqX77KAkxnocyyR+gEVTZ71qqsXITuJGi7XurQwz43lntrtFKVQGVEAAABIQZ6GRREsI/8AABa+ehKUoARidzqoix2ITFwbtoC1dFRypfzuv2is3q8BuF9yXTFxO8yxSJO+3hr8eAAAAwAABUm1SdRk8IEnAAAAQAGepXRH/wAAI8C+2QAjIlYUkbC+n8XgeEkqfYB9CGR2gpYLIQd7M+d36JY83znSeOT6of2Dc1dpzD7scnhAk4EAAAA9AZ6nakf/AAAjvZWtu4pta5RJCXT4QE0gicH7jjapMlBQaH7NB2XF/4EympmskqmNQ87+1sL7kOe+RlgjYAAAAOdBmqxJqEFsmUwIX//+jLAAAEYA1ykiAC/iUtAhovYQ5ldoUWIssPNfuEoP2JbiN1W8ho/EEnVhVB/CIAEE/5ISk9wgStsv2loS8r/TmsPbVXfY/2QOdYaiUncrzNctkiUWOw1TwH4Gp5KW0YByesvx7Of8DErj/+ZMb/xqY2IMBTMWJ5iEe9+66pMjns1d9bRmm+sI64xxMr7DydjjxTVICAEe86QjLPJRV5dOEFpYO/cRG78PPW8xKZn8ZQm9GAhr6wDhwK0PZy/l0eu+YyFtaTgPTxlov+QoVrF0p66lfXhqja50d7QAAACFQZ7KRRUsI/8AABa+YKYylhOsprbblspR8YASLdAdPwqpl1Br+TsFMYW5Bn31p5G1FjRfTF3A0w8mh3F1vsPhsEy/QM7m+wXb8lo3uVpnK8Fv8W0Ca7HlzIap/aDB/Y+yzUYf/Jk3GkTArd8LnXiE0iDOFKlogTjh1XJCAlu02nmnj4QGfQAAAFkBnul0R/8AACOjcfyUWi8MNpUWY8/HNY9RWggBsrOD82l69jab8DhscRhaAA/CHmZYj0mIcDREUuY/R587ANSNsDFRc7shTVXiiTynad5MjStkJ1PqlUCLgAAAAFYBnutqR/8AACO9lWSIIJ9SyeK0k9m3FfrHnh78ELWz385XqZFYruj9OKUeGNVR/XaQVWKIATYLhs57RDEzmWw6gPWQMNBsZcHcQie3YzEkgY8MlUBSQAAAANNBmvBJqEFsmUwIV//+OEAAAQ74Hh+xhizZwAFsZ71+ECsyuXAM9M8VHT+51UcvWHGrdQ8pO+cWn3qJt1f5qiiFITt6dsiMt9XQP2m41JcxQBp9kzVzB3acvTb5YIr0JkiUhkI85QJJI0DRan7FlLgs2NZyohKYn5pFvTUcRxXw8+UO1TI4XTJYInM1AgrlGab6ajJjD4SM6ohgjpRonqJZfi/fnTkVSy3GrqoCE53mHUpnPEcOBw5Vz1kxCCrE1wUB7rz9dt3+021NRBJcsW8HxCHhAAAAc0GfDkUVLCP/AAAWtQ+PMdfEOn3BigZ1wqgcyp6iZsq0zGpUBNLkzBUzXQz0WM2cv/xRHxbNgAFydE45Y90u4LXg2VZYQ61V7RdCBeGU6LZYRzRMXY1TwCcaxzM5mffYS+UXQ5fo09hdbSoSFOa37WFUBZ0AAABYAZ8tdEf/AAAjq+EhHbuWQjyPMXb/vD+J+0cqcWVHuf1vxlHQlFxwfkKwh6xEqZC6l7ZR48cfVJpkPJRRoMAJiiLUL/QgCmiZWYczbM8RG4v/V/91GuEDWwAAAFABny9qR/8AACLHDbYnD2JCmVXory1dgznbNv78gPgSna8r8cxmj0GzUyDi40rUACaSsO5ebC9udRq0MKZPVovWLamMmfwu8tE7hNwAKoB6QAAAAIBBmzRJqEFsmUwI//yEAAAPhZtdd23uEgDOY9A2SxXMaHyVMQhDtzTN7cEoOlieNbfGhqvMBd2zgCA+nBIeK3rjJGgANqJbwDtaiExCRSYZO8ofb6w5XuvuXzFrKoCU0KAbyzXQFr2ZlTZTXdFDX9coaYRVe73L4Vz/uCpXhABSQAAAAHxBn1JFFSwj/wAAFhe+JeQAI9iB/BJJPnmarBdKZnAfXAluCCIlCvSB2Wq6r8gOKK9dxuIWIbUinUblNzPy+rBPTfkVcsAYHyj/noEn/m3UV1wFCh1gBnX5tX3jVxpnMe0RvDL5ebsP3uNXiO1EI5OwTbhdBk057vCvCBQRAAAARQGfcXRH/wAAIsC5zA+Dy4m4cN75Lp/4yqEbfoLhYaeEqgTfrqVUZNR9jVf+cenpTELRPmco0utfbYy9quCtRgO+J4QGpAAAAEUBn3NqR/8AACKZLYqaYJamMxp9/cV35GyxUWYe4gfIafWO72iz8YJjBLxTA6GrHckviTq1Cog8JbqyYpJm+MSqbSqAZUAAAAQPbW9vdgAAAGxtdmhkAAAAAAAAAAAAAAAAAAAD6AAAAaQAAQAAAQAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgAAAzl0cmFrAAAAXHRraGQAAAADAAAAAAAAAAAAAAABAAAAAAAAAaQAAAAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAABAAAAAAlgAAAGQAAAAAAAkZWR0cwAAABxlbHN0AAAAAAAAAAEAAAGkAAACAAABAAAAAAKxbWRpYQAAACBtZGhkAAAAAAAAAAAAAAAAAAAyAAAAFQBVxAAAAAAALWhkbHIAAAAAAAAAAHZpZGUAAAAAAAAAAAAAAABWaWRlb0hhbmRsZXIAAAACXG1pbmYAAAAUdm1oZAAAAAEAAAAAAAAAAAAAACRkaW5mAAAAHGRyZWYAAAAAAAAAAQAAAAx1cmwgAAAAAQAAAhxzdGJsAAAAmHN0c2QAAAAAAAAAAQAAAIhhdmMxAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAlgBkABIAAAASAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGP//AAAAMmF2Y0MBZAAf/+EAGWdkAB+s2UCYM+XhAAADAAEAAAMAZA8YMZYBAAZo6+PLIsAAAAAYc3R0cwAAAAAAAAABAAAAFQAAAQAAAAAUc3RzcwAAAAAAAAABAAAAAQAAALhjdHRzAAAAAAAAABUAAAABAAACAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAAcc3RzYwAAAAAAAAABAAAAAQAAABUAAAABAAAAaHN0c3oAAAAAAAAAAAAAABUAAASEAAAAqwAAAFgAAABBAAAANgAAAN8AAABMAAAARAAAAEEAAADrAAAAiQAAAF0AAABaAAAA1wAAAHcAAABcAAAAVAAAAIQAAACAAAAASQAAAEkAAAAUc3RjbwAAAAAAAAABAAAAMAAAAGJ1ZHRhAAAAWm1ldGEAAAAAAAAAIWhkbHIAAAAAAAAAAG1kaXJhcHBsAAAAAAAAAAAAAAAALWlsc3QAAAAlqXRvbwAAAB1kYXRhAAAAAQAAAABMYXZmNTcuODMuMTAw\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3t9Hh_2ruTtC",
        "colab_type": "text"
      },
      "source": [
        "## 3. A3C algorithm\n",
        "### 3-1. Overview\n",
        "\n",
        "We'll walk through another great tutorial post about deep reinforcement learning. This post explains the learning process of A3C using more intuitive visualization. Below I borrow many of the description and figures but I would recommend to read the entire contents, which is really well-written to follow.\n",
        "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UBcgmdCVLZa",
        "colab_type": "text"
      },
      "source": [
        "  ![alt text](https://cdn-images-1.medium.com/max/1600/1*YtnGhtSAMnnHSL8PvS7t_w.png =500x)\n",
        "- Asynchronous: A3C utilizes multiple incarnations of the above in order to learn more efficiently. In A3C there is a *global network*, and multiple worker agents which each have their own set of *local network* parameters. Each of these agents interacts with it’s own copy of the *environment* at the same time as the other agents are interacting with their environments. The reason this works better than having a single agent (beyond the speedup of getting more work done), is that the experience of each agent is independent of the experience of the others. In this way the overall experience available for training becomes more diverse.\n",
        "- Actor-Critic: Other RL methods are categorized in two branches: value-iteration methods such as Q-learning, or policy-iteration methods such as Policy Gradient. Actor-Critic combines the benefits of both approaches. In the case of A3C, our network will estimate both a value function V(s) (*how good a certain state is to be in*) and a policy π(s) (*a set of action probability outputs*). These will be separate fully-connected layers sitting at the top of the network. Critically, the agent uses the value estimate (the critic) to update the policy (the actor) more intelligently than traditional policy gradient methods.\n",
        "- Advantage: As in Policy Gradient, the update rule used the discounted returns from a set of experiences in order to tell the agent which of its actions were “good” and which were “bad.” The network was then updated in order to encourage and discourage actions appropriately.\n",
        "\n",
        "### 3-2. Workflow\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1600/1*Hzql_1t0-wwDxiz0C97AcQ.png =400x) \n",
        " ![alt text](https://cdn-images-1.medium.com/max/1600/1*YtnGhtSAMnnHSL8PvS7t_w.png =400x)\n",
        " \n",
        " The figures above (the right one appears again) represent how the Master and Workers interact with each other to update Networks. Master holds the global network, and Workers take the global network, copying to its local network, followed by policy and value update by executing steps in their local environment *in parallel*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZseyunzmA7W",
        "colab_type": "text"
      },
      "source": [
        "## 4. Implementation\n",
        "\n",
        "### 4-1. Model (Network)\n",
        "\n",
        "Let’s first define what kind of model we’ll be using. The master agent will have the *global* network and each worker agent will have a *local* copy of this network in their own process. We’ll instantiate the model using Model Subclassing. Model subclassing gives us maximum flexibility at the cost of higher verbosity.\n",
        "\n",
        "The figure below visualizes the structure of our model network: our model takes inputs and returns policy probability logits and values.\n",
        "![Model (Network)](https://imgur.com/iSvszib.png =400x)\n",
        "\n",
        "\n",
        "### Quiz 2\n",
        "Please fill out the code snippet to build model using tf.keras API. As we learned about model subclassing in the last session, `__init__` is the place where we define layers and variables, and `call()` implements the logics computed in forward pass.\n",
        "As shown in the figure, we need four **Dense** layers:\n",
        "* `dense1`: 100 hidden units with a `relu` activation and an output named `x`.\n",
        "* `policy_logits`: 2 units (=`action_size`) with a default activation and an output named `logits`.\n",
        "* `dense2`: 100 hidden units with a `relu` activation and an output named `v1`.\n",
        "* `values`: 1 unit with a default activation and an output named `values`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLeEkZ2jds6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.keras as keras\n",
        "import tensorflow.keras.layers as layers\n",
        "\n",
        "class ActorCriticModel(keras.Model):\n",
        "  def __init__(self, state_size, action_size):\n",
        "    # Layers and variables\n",
        "    super(ActorCriticModel, self).__init__()\n",
        "    # sizes of state and action are determined by the environment (See 4-2)\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    \n",
        "    ############ Define layers ############    \n",
        "    self.dense1 = layers.Dense(100, activation='relu')\n",
        "    self.policy_logits = layers.Dense(action_size)\n",
        "    self.dense2 = layers.Dense(100,activation='relu')\n",
        "    self.values = layers.Dense(1)\n",
        "    #######################################\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # Forward pass\n",
        "    x = self.dense1(inputs)\n",
        "    ############ Compute logits and values ############\n",
        "    logits = self.policy_logits(x)\n",
        "    v1 = self.dense2(inputs)\n",
        "    values = self.values(v1)\n",
        "    ###################################################\n",
        "    return logits, values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5-qF-Hsmpcq",
        "colab_type": "text"
      },
      "source": [
        "### 4-2. Memory Class — Holding our experience\n",
        "To make keeping track of things easier, we’ll implement a `Memory` class. This class will simply give us the functionality to keep track of our actions, rewards, states that occur per step. `record()` is a utility function that prints the information (reward, loss, steps, etc)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwMgg2jSs9KT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 학습하면서 reward,state,action 의 이력을 저장하는 클래스\n",
        "class Memory:\n",
        "  def __init__(self):\n",
        "    self.states = []\n",
        "    self.actions = []\n",
        "    self.rewards = []\n",
        "    \n",
        "  def store(self, state, action, reward):\n",
        "    self.states.append(state)\n",
        "    self.actions.append(action)\n",
        "    self.rewards.append(reward)\n",
        "    \n",
        "  def clear(self):\n",
        "    self.states = []\n",
        "    self.actions = []\n",
        "    self.rewards = []\n",
        "    \n",
        "    \n",
        "def record(episode,\n",
        "           episode_reward,\n",
        "           worker_idx,\n",
        "           global_ep_reward,\n",
        "           result_queue,\n",
        "           total_loss,\n",
        "           num_steps):\n",
        "  \"\"\"Helper function to store score and print statistics.\n",
        "\n",
        "  Arguments:\n",
        "    episode: Current episode\n",
        "    episode_reward: Reward accumulated over the current episode\n",
        "    worker_idx: Which thread (worker)\n",
        "    global_ep_reward: The moving average of the global reward\n",
        "    result_queue: Queue storing the moving average of the scores\n",
        "    total_loss: The total loss accumualted over the current episode\n",
        "    num_steps: The number of steps the episode took to complete\n",
        "  \"\"\"\n",
        "  if global_ep_reward == 0:\n",
        "    global_ep_reward = episode_reward\n",
        "  else:\n",
        "    global_ep_reward = global_ep_reward * 0.99 + episode_reward * 0.01\n",
        "  print(\n",
        "      \"Episode: {} | \" \\\n",
        "      \"Moving Average Reward: {} | \" \\\n",
        "      \"Episode Reward: {} | \" \\\n",
        "      \"Loss: {} | \" \\\n",
        "      \"Steps: {} | \" \\\n",
        "      \"Worker: {}\".format(episode, int(global_ep_reward), int(episode_reward), int(total_loss / float(num_steps) * 1000) / 1000, num_steps, worker_idx)\n",
        "  )\n",
        "  # multi thread 로 수행되기 때문에, que 구조로 이용하여 worker 의 결과를 완료되는대로 저장하기 위함.\n",
        "  result_queue.put(global_ep_reward)\n",
        "  return global_ep_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WMtA8YgQgce",
        "colab_type": "text"
      },
      "source": [
        "### 4-3. Worker Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJNB1xgcm88s",
        "colab_type": "text"
      },
      "source": [
        "Now, we get to the crux of the algorithm: the worker agent. The worker agent inherits from the threading class, and we override the `run` method from Thread. This will allow us to achieve the first A in A3C, asynchronous. \n",
        "\n",
        "In this class, we will implement three functions:\n",
        "1. `__init__`: We’ll begin by instantiating a local model and setting up the specific training parameters.\n",
        "2. `compute_loss`: The worker agent calculates the losses to obtain gradients with respect to all of its own network parameters. This is where the last A of A3C come into play, the *advantage*. These are then applied to the global network. The losses are calculated as:\n",
        "  - Value Loss: L=∑(R — V(s))²\n",
        "  - Policy Loss: L = -log(𝝅(s)) \\* A(s)  \n",
        "Where `R` is the discounted rewards, `V` our value function (of an input state), `𝛑` our policy function (of an input state as well), and `A` our *advantage* function. We use the discounted rewards to estimate our `Q` value since we don’t directly determine the Q value with A3C.\n",
        "3. `run`: We’ll run all the threads for a given global maximum number of episodes. This is where the third `A`, *actor*, of A3C comes into play. Our agent will “act” according to our policy function, becoming the actor while the action is judged by the “critic,” which is our value function. While this section of the code may seem dense, it really isn’t doing much. Within each episode, the code simply does:\n",
        "  1. Get our policy (action probability distribution) based on the current frame\n",
        "  2. Step with action chosen according to policy\n",
        "  3. If the agent has taken a set number of steps (`update_freq`) or the agent has reached a terminal state (has died),  \n",
        "     then update the global model with gradients computed from local model\n",
        "  4. Repeat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MedG9Xr7tA9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import threading\n",
        "class Worker(threading.Thread): \n",
        "  # Set up global variables across different threads\n",
        "  global_episode = 0\n",
        "  # Moving average reward\n",
        "  global_moving_average_reward = 0\n",
        "  best_score = 0\n",
        "  save_lock = threading.Lock() \n",
        "\n",
        "\n",
        "  def __init__(self,\n",
        "               state_size,\n",
        "               action_size,\n",
        "               global_model,\n",
        "               opt,\n",
        "               result_queue,\n",
        "               idx,\n",
        "               game_name='CartPole-v0',\n",
        "               save_dir='/tmp'):\n",
        "    super(Worker, self).__init__()\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.result_queue = result_queue\n",
        "    self.global_model = global_model\n",
        "    self.opt = opt\n",
        "    self.local_model = ActorCriticModel(self.state_size, self.action_size)\n",
        "    self.worker_idx = idx\n",
        "    self.game_name = game_name\n",
        "    self.env = gym.make(self.game_name).unwrapped\n",
        "    self.save_dir = save_dir\n",
        "    self.ep_loss = 0.0\n",
        "    self.update_freq = 20 # How often to update the global model.\n",
        "    self.max_eps = 1000 # Global maximum number of episodes to run.\n",
        "    self.gamma = 0.99 # Discount factor of rewards.\n",
        "    \n",
        "  #1 stage \n",
        "  def compute_loss(self,\n",
        "                   done,\n",
        "                   new_state,\n",
        "                   memory):\n",
        "    if done:\n",
        "      reward_sum = 0.  # terminal\n",
        "    else:\n",
        "      reward_sum = self.local_model(\n",
        "          tf.convert_to_tensor(new_state[None, :],\n",
        "                               dtype=tf.float32))[-1].numpy()[0]\n",
        "\n",
        "    # Get discounted rewards\n",
        "    discounted_rewards = []\n",
        "    for reward in memory.rewards[::-1]:  # reverse buffer r\n",
        "      reward_sum = reward + self.gamma * reward_sum\n",
        "      discounted_rewards.append(reward_sum)\n",
        "    discounted_rewards.reverse()\n",
        "\n",
        "    logits, values = self.local_model(\n",
        "        tf.convert_to_tensor(np.vstack(memory.states),\n",
        "                             dtype=tf.float32))\n",
        "    # Get our advantages\n",
        "    advantage = tf.convert_to_tensor(np.array(discounted_rewards)[:, None],\n",
        "                            dtype=tf.float32) - values\n",
        "    # Value loss\n",
        "    value_loss = advantage ** 2\n",
        "\n",
        "    # Calculate our policy loss\n",
        "    actions_one_hot = tf.one_hot(memory.actions, self.action_size, dtype=tf.float32)\n",
        "\n",
        "    policy = tf.nn.softmax(logits)\n",
        "    entropy = tf.reduce_sum(policy * tf.log(policy + 1e-20), axis=1)\n",
        "\n",
        "    policy_loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=actions_one_hot,\n",
        "                                                             logits=logits)\n",
        "    policy_loss *= tf.stop_gradient(advantage)\n",
        "    policy_loss -= 0.01 * entropy\n",
        "    total_loss = tf.reduce_mean((0.5 * value_loss + policy_loss))\n",
        "    return total_loss  \n",
        "    \n",
        "    \n",
        "  def run(self):\n",
        "      total_step = 1\n",
        "      mem = Memory()\n",
        "      while Worker.global_episode < self.max_eps:\n",
        "        current_state = self.env.reset()\n",
        "        mem.clear()\n",
        "        ep_reward = 0.\n",
        "        ep_steps = 0\n",
        "        self.ep_loss = 0\n",
        "\n",
        "        time_count = 0\n",
        "        done = False\n",
        "        while not done:\n",
        "          logits, _ = self.local_model(\n",
        "              tf.convert_to_tensor(current_state[None, :],\n",
        "                                   dtype=tf.float32))\n",
        "          probs = tf.nn.softmax(logits)\n",
        "\n",
        "          action = np.random.choice(self.action_size, p=probs.numpy()[0])\n",
        "          new_state, reward, done, _ = self.env.step(action)\n",
        "          if done:\n",
        "            reward = -1\n",
        "          ep_reward += reward\n",
        "          mem.store(current_state, action, reward)\n",
        "\n",
        "          if time_count == self.update_freq or done:\n",
        "            # Calculate gradient wrt to local model. We do so by tracking the\n",
        "            # variables involved in computing the loss by using tf.GradientTape\n",
        "            with tf.GradientTape() as tape:\n",
        "              total_loss = self.compute_loss(done,\n",
        "                                             new_state,\n",
        "                                             mem)\n",
        "            self.ep_loss += total_loss\n",
        "            # Calculate local gradients\n",
        "            grads = tape.gradient(total_loss, self.local_model.trainable_weights)\n",
        "            # Push local gradients to global model\n",
        "            self.opt.apply_gradients(zip(grads,\n",
        "                                         self.global_model.trainable_weights))\n",
        "            # Update local model with new weights\n",
        "            self.local_model.set_weights(self.global_model.get_weights())\n",
        "\n",
        "            mem.clear()\n",
        "            time_count = 0\n",
        "\n",
        "            if done:  # done and print information\n",
        "              Worker.global_moving_average_reward = \\\n",
        "                record(Worker.global_episode, ep_reward, self.worker_idx,\n",
        "                       Worker.global_moving_average_reward, self.result_queue,\n",
        "                       self.ep_loss, ep_steps)\n",
        "              # We must use a lock to save our model and to print to prevent data races.\n",
        "              if ep_reward > Worker.best_score:\n",
        "                with Worker.save_lock:\n",
        "                  print(\"Saving best model to {}, \"\n",
        "                        \"episode score: {}\".format(self.save_dir, ep_reward))\n",
        "                  self.global_model.save_weights(\n",
        "                      os.path.join(self.save_dir,\n",
        "                                   'model_{}.h5'.format(self.game_name))\n",
        "                  )\n",
        "                  Worker.best_score = ep_reward\n",
        "              Worker.global_episode += 1\n",
        "          ep_steps += 1\n",
        "\n",
        "          time_count += 1\n",
        "          current_state = new_state\n",
        "          total_step += 1\n",
        "      self.result_queue.put(None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDKlBp13mZBO",
        "colab_type": "text"
      },
      "source": [
        "# **굵은 텍스트**### 4-3. Master Agent — Main thread\n",
        "Let’s get to the brain of the operation. The master agent holds a shared optimizer that updates its global network. This agent instantiates the global network that each worker agent will update as well as the optimizer that we will use to update it. A3C is shown to be quite resilient to a spread of learning rates, but we’ll use the AdamOptimizer with a learning rate of 5e-4 for the game CartPole.\n",
        "\n",
        "In this class, we will implement 3 functions:\n",
        "* `__init__`: Create a `MasterAgent` instance; here we create objects for environments\n",
        "* `train`: The *master agent* will instantiate and start each of the agents. The master agent handles the coordinating and supervision of each *worker agent*. Each of these agents will run asynchronously.  \n",
        "**Note:** This is technically not true asynchrony because in Python, because of GIL (Global Interpreter Lock) a single python process cannot run threads in parallel (utilize multiple cores). It can however run them concurrently (context switch during I/O bound operations). We implement with threads for the sake of simplicity and clarity of example.\n",
        "* `play`:  After training, we can evaluate the performance of our learned model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7RoTkbiS7aj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import multiprocessing\n",
        "from queue import Queue\n",
        "\n",
        "class MasterAgent():\n",
        "  def __init__(self):\n",
        "    # We will save the model in /tmp/CartPole-v0'\n",
        "    self.game_name = 'CartPole-v0'\n",
        "    save_dir = '/tmp'\n",
        "    self.save_dir = save_dir\n",
        "    if not os.path.exists(save_dir):\n",
        "      os.makedirs(save_dir)\n",
        "\n",
        "    # Create an environment from Gym and get information\n",
        "    env = gym.make(self.game_name)\n",
        "    self.state_size = env.observation_space.shape[0]\n",
        "    self.action_size = env.action_space.n\n",
        "    print(self.state_size, self.action_size)\n",
        "    \n",
        "    # Define an optimizer\n",
        "    learning_rate = 5e-4\n",
        "    self.opt = tf.train.AdamOptimizer(learning_rate, use_locking=True)\n",
        "\n",
        "    # Global network\n",
        "    self.global_model = ActorCriticModel(self.state_size, self.action_size)  \n",
        "    self.global_model(tf.convert_to_tensor(np.random.random((1, self.state_size)), dtype=tf.float32))\n",
        "    \n",
        "    \n",
        "  def train(self):\n",
        "      # A queue to accumulate results computed by worker agents\n",
        "      res_queue = Queue()\n",
        "\n",
        "      # Start worker agents\n",
        "      workers = [Worker(self.state_size,\n",
        "                        self.action_size,\n",
        "                        self.global_model,\n",
        "                        self.opt, res_queue,\n",
        "                        i, game_name=self.game_name,\n",
        "                        save_dir=self.save_dir) for i in range(multiprocessing.cpu_count())]\n",
        "      for i, worker in enumerate(workers):\n",
        "        print(\"Starting worker {}\".format(i))\n",
        "        worker.start()\n",
        "\n",
        "      # Record episode reward to plot\n",
        "      moving_average_rewards = []\n",
        "      while True:\n",
        "        reward = res_queue.get()\n",
        "        if reward is not None:\n",
        "          moving_average_rewards.append(reward)\n",
        "        else:\n",
        "          break\n",
        "      [w.join() for w in workers]\n",
        "\n",
        "      # Plot the reward in /tmp/CartPole-v0\n",
        "      plt.plot(moving_average_rewards)\n",
        "      plt.ylabel('Moving average ep reward')\n",
        "      plt.xlabel('Step')\n",
        "      plt.savefig(os.path.join(self.save_dir,\n",
        "                               '{}-moving-average.png'.format(self.game_name)))\n",
        "      plt.show()\n",
        "    \n",
        "    \n",
        "  def play(self):\n",
        "      env = gym.make(self.game_name)\n",
        "      env = wrap_env(env)\n",
        "    \n",
        "      state = env.reset()\n",
        "      model = self.global_model\n",
        "      model_path = os.path.join(self.save_dir, 'model_{}.h5'.format(self.game_name))\n",
        "      print('Loading model from: {}'.format(model_path))\n",
        "      model.load_weights(model_path)\n",
        "      step_counter = 0\n",
        "      reward_sum = 0\n",
        "\n",
        "      try:\n",
        "        done = False        \n",
        "        while not done:\n",
        "          env.render()\n",
        "          policy, value = model(tf.convert_to_tensor(state[None, :], dtype=tf.float32))\n",
        "\n",
        "          policy = tf.nn.softmax(policy)          \n",
        "          action = np.argmax(policy)\n",
        "          state, reward, done, _ = env.step(action)\n",
        "          reward_sum += reward\n",
        "          print(\"{}. Reward: {}, action: {}\".format(step_counter, reward_sum, action))\n",
        "          step_counter += 1\n",
        "      except KeyboardInterrupt:\n",
        "        print(\"Received Keyboard Interrupt. Shutting down.\")\n",
        "      finally:\n",
        "        env.close()\n",
        "        show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBWRVzJLmucP",
        "colab_type": "text"
      },
      "source": [
        "### 4-4. Putting all together\n",
        "\n",
        "We can train our model with the A3C algorithm we implemented.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0MjTDaLLJuE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 19420
        },
        "outputId": "e408e167-230b-4ecc-a103-95c0e09591ae"
      },
      "source": [
        "master = MasterAgent()\n",
        "master.train()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4 2\n",
            "Starting worker 0\n",
            "Starting worker 1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:80: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Episode: 0 | Moving Average Reward: 13 | Episode Reward: 13 | Loss: 2.038 | Steps: 14 | Worker: 1\n",
            "Saving best model to /tmp, episode score: 13.0\n",
            "Episode: 1 | Moving Average Reward: 12 | Episode Reward: 9 | Loss: 1.554 | Steps: 10 | Worker: 0\n",
            "Episode: 2 | Moving Average Reward: 12 | Episode Reward: 14 | Loss: 2.23 | Steps: 15 | Worker: 1\n",
            "Saving best model to /tmp, episode score: 14.0\n",
            "Episode: 3 | Moving Average Reward: 12 | Episode Reward: 11 | Loss: 1.687 | Steps: 12 | Worker: 1\n",
            "Episode: 4 | Moving Average Reward: 13 | Episode Reward: 29 | Loss: 2.898 | Steps: 30 | Worker: 0\n",
            "Saving best model to /tmp, episode score: 29.0\n",
            "Episode: 5 | Moving Average Reward: 13 | Episode Reward: 11 | Loss: 1.712 | Steps: 12 | Worker: 1\n",
            "Episode: 6 | Moving Average Reward: 13 | Episode Reward: 14 | Loss: 2.216 | Steps: 15 | Worker: 0\n",
            "Episode: 7 | Moving Average Reward: 13 | Episode Reward: 18 | Loss: 2.68 | Steps: 19 | Worker: 0\n",
            "Episode: 8 | Moving Average Reward: 13 | Episode Reward: 30 | Loss: 2.876 | Steps: 31 | Worker: 1\n",
            "Saving best model to /tmp, episode score: 30.0\n",
            "Episode: 9 | Moving Average Reward: 13 | Episode Reward: 25 | Loss: 3.05 | Steps: 26 | Worker: 0\n",
            "Episode: 10 | Moving Average Reward: 13 | Episode Reward: 17 | Loss: 2.613 | Steps: 18 | Worker: 1\n",
            "Episode: 11 | Moving Average Reward: 13 | Episode Reward: 33 | Loss: 2.905 | Steps: 34 | Worker: 0\n",
            "Saving best model to /tmp, episode score: 33.0\n",
            "Episode: 11 | Moving Average Reward: 13 | Episode Reward: 24 | Loss: 3.048 | Steps: 25 | Worker: 1\n",
            "Episode: 13 | Moving Average Reward: 13 | Episode Reward: 11 | Loss: 1.639 | Steps: 12 | Worker: 0\n",
            "Episode: 14 | Moving Average Reward: 13 | Episode Reward: 27 | Loss: 2.918 | Steps: 28 | Worker: 0\n",
            "Episode: 15 | Moving Average Reward: 14 | Episode Reward: 52 | Loss: 3.117 | Steps: 53 | Worker: 1\n",
            "Saving best model to /tmp, episode score: 52.0\n",
            "Episode: 16 | Moving Average Reward: 14 | Episode Reward: 19 | Loss: 2.841 | Steps: 20 | Worker: 0\n",
            "Episode: 17 | Moving Average Reward: 14 | Episode Reward: 6 | Loss: 1.017 | Steps: 7 | Worker: 1\n",
            "Episode: 18 | Moving Average Reward: 14 | Episode Reward: 10 | Loss: 1.607 | Steps: 11 | Worker: 0\n",
            "Episode: 19 | Moving Average Reward: 14 | Episode Reward: 9 | Loss: 1.398 | Steps: 10 | Worker: 1\n",
            "Episode: 20 | Moving Average Reward: 14 | Episode Reward: 13 | Loss: 1.919 | Steps: 14 | Worker: 1\n",
            "Episode: 20 | Moving Average Reward: 14 | Episode Reward: 14 | Loss: 2.142 | Steps: 15 | Worker: 0\n",
            "Episode: 22 | Moving Average Reward: 14 | Episode Reward: 13 | Loss: 1.947 | Steps: 14 | Worker: 0\n",
            "Episode: 23 | Moving Average Reward: 14 | Episode Reward: 8 | Loss: 1.211 | Steps: 9 | Worker: 1\n",
            "Episode: 24 | Moving Average Reward: 14 | Episode Reward: 18 | Loss: 2.622 | Steps: 19 | Worker: 1\n",
            "Episode: 25 | Moving Average Reward: 14 | Episode Reward: 9 | Loss: 1.434 | Steps: 10 | Worker: 1\n",
            "Episode: 26 | Moving Average Reward: 13 | Episode Reward: 10 | Loss: 1.449 | Steps: 11 | Worker: 1\n",
            "Episode: 27 | Moving Average Reward: 14 | Episode Reward: 45 | Loss: 3.204 | Steps: 46 | Worker: 0\n",
            "Episode: 28 | Moving Average Reward: 14 | Episode Reward: 12 | Loss: 1.861 | Steps: 13 | Worker: 1\n",
            "Episode: 29 | Moving Average Reward: 14 | Episode Reward: 11 | Loss: 1.58 | Steps: 12 | Worker: 1\n",
            "Episode: 30 | Moving Average Reward: 14 | Episode Reward: 21 | Loss: 3.382 | Steps: 22 | Worker: 0\n",
            "Episode: 31 | Moving Average Reward: 14 | Episode Reward: 18 | Loss: 2.677 | Steps: 19 | Worker: 0\n",
            "Episode: 32 | Moving Average Reward: 14 | Episode Reward: 32 | Loss: 2.856 | Steps: 33 | Worker: 1\n",
            "Episode: 33 | Moving Average Reward: 14 | Episode Reward: 18 | Loss: 2.616 | Steps: 19 | Worker: 1\n",
            "Episode: 34 | Moving Average Reward: 14 | Episode Reward: 30 | Loss: 2.875 | Steps: 31 | Worker: 0\n",
            "Episode: 35 | Moving Average Reward: 14 | Episode Reward: 13 | Loss: 1.913 | Steps: 14 | Worker: 0\n",
            "Episode: 36 | Moving Average Reward: 14 | Episode Reward: 35 | Loss: 3.033 | Steps: 36 | Worker: 1\n",
            "Episode: 36 | Moving Average Reward: 14 | Episode Reward: 20 | Loss: 3.919 | Steps: 21 | Worker: 0\n",
            "Episode: 38 | Moving Average Reward: 14 | Episode Reward: 22 | Loss: 3.23 | Steps: 23 | Worker: 0\n",
            "Episode: 39 | Moving Average Reward: 14 | Episode Reward: 23 | Loss: 3.157 | Steps: 24 | Worker: 1\n",
            "Episode: 40 | Moving Average Reward: 14 | Episode Reward: 13 | Loss: 1.957 | Steps: 14 | Worker: 0\n",
            "Episode: 41 | Moving Average Reward: 14 | Episode Reward: 15 | Loss: 2.184 | Steps: 16 | Worker: 1\n",
            "Episode: 42 | Moving Average Reward: 14 | Episode Reward: 18 | Loss: 2.479 | Steps: 19 | Worker: 0\n",
            "Episode: 43 | Moving Average Reward: 14 | Episode Reward: 17 | Loss: 2.488 | Steps: 18 | Worker: 0\n",
            "Episode: 44 | Moving Average Reward: 15 | Episode Reward: 26 | Loss: 2.908 | Steps: 27 | Worker: 1\n",
            "Episode: 45 | Moving Average Reward: 15 | Episode Reward: 15 | Loss: 2.191 | Steps: 16 | Worker: 1\n",
            "Episode: 46 | Moving Average Reward: 15 | Episode Reward: 28 | Loss: 2.856 | Steps: 29 | Worker: 0Episode: 46 | Moving Average Reward: 15 | Episode Reward: 12 | Loss: 1.8 | Steps: 13 | Worker: 1\n",
            "\n",
            "Episode: 48 | Moving Average Reward: 15 | Episode Reward: 14 | Loss: 2.081 | Steps: 15 | Worker: 1\n",
            "Episode: 49 | Moving Average Reward: 15 | Episode Reward: 38 | Loss: 3.022 | Steps: 39 | Worker: 0\n",
            "Episode: 50 | Moving Average Reward: 15 | Episode Reward: 20 | Loss: 3.714 | Steps: 21 | Worker: 1\n",
            "Episode: 51 | Moving Average Reward: 15 | Episode Reward: 14 | Loss: 2.071 | Steps: 15 | Worker: 0\n",
            "Episode: 52 | Moving Average Reward: 15 | Episode Reward: 13 | Loss: 1.854 | Steps: 14 | Worker: 0\n",
            "Episode: 53 | Moving Average Reward: 15 | Episode Reward: 22 | Loss: 3.253 | Steps: 23 | Worker: 1\n",
            "Episode: 54 | Moving Average Reward: 15 | Episode Reward: 15 | Loss: 2.148 | Steps: 16 | Worker: 0\n",
            "Episode: 55 | Moving Average Reward: 15 | Episode Reward: 32 | Loss: 2.82 | Steps: 33 | Worker: 1\n",
            "Episode: 56 | Moving Average Reward: 15 | Episode Reward: 38 | Loss: 3.033 | Steps: 39 | Worker: 0\n",
            "Episode: 57 | Moving Average Reward: 15 | Episode Reward: 23 | Loss: 3.171 | Steps: 24 | Worker: 1\n",
            "Episode: 58 | Moving Average Reward: 15 | Episode Reward: 24 | Loss: 3.064 | Steps: 25 | Worker: 0\n",
            "Episode: 59 | Moving Average Reward: 15 | Episode Reward: 15 | Loss: 2.159 | Steps: 16 | Worker: 1\n",
            "Episode: 60 | Moving Average Reward: 16 | Episode Reward: 36 | Loss: 2.926 | Steps: 37 | Worker: 0\n",
            "Episode: 61 | Moving Average Reward: 16 | Episode Reward: 19 | Loss: 2.587 | Steps: 20 | Worker: 0\n",
            "Episode: 62 | Moving Average Reward: 16 | Episode Reward: 58 | Loss: 3.093 | Steps: 59 | Worker: 1\n",
            "Saving best model to /tmp, episode score: 58.0\n",
            "Episode: 63 | Moving Average Reward: 16 | Episode Reward: 13 | Loss: 1.819 | Steps: 14 | Worker: 0\n",
            "Episode: 64 | Moving Average Reward: 16 | Episode Reward: 50 | Loss: 3.035 | Steps: 51 | Worker: 1\n",
            "Episode: 65 | Moving Average Reward: 17 | Episode Reward: 49 | Loss: 3.037 | Steps: 50 | Worker: 0Episode: 65 | Moving Average Reward: 16 | Episode Reward: 17 | Loss: 2.374 | Steps: 18 | Worker: 1\n",
            "\n",
            "Episode: 67 | Moving Average Reward: 17 | Episode Reward: 12 | Loss: 1.656 | Steps: 13 | Worker: 0\n",
            "Episode: 68 | Moving Average Reward: 17 | Episode Reward: 11 | Loss: 1.544 | Steps: 12 | Worker: 1\n",
            "Episode: 69 | Moving Average Reward: 16 | Episode Reward: 11 | Loss: 1.582 | Steps: 12 | Worker: 1Episode: 69 | Moving Average Reward: 17 | Episode Reward: 42 | Loss: 3.346 | Steps: 43 | Worker: 0\n",
            "\n",
            "Episode: 71 | Moving Average Reward: 16 | Episode Reward: 13 | Loss: 1.8 | Steps: 14 | Worker: 1\n",
            "Episode: 72 | Moving Average Reward: 17 | Episode Reward: 51 | Loss: 3.064 | Steps: 52 | Worker: 0\n",
            "Episode: 73 | Moving Average Reward: 17 | Episode Reward: 16 | Loss: 2.149 | Steps: 17 | Worker: 1\n",
            "Episode: 74 | Moving Average Reward: 17 | Episode Reward: 17 | Loss: 2.347 | Steps: 18 | Worker: 1\n",
            "Episode: 75 | Moving Average Reward: 17 | Episode Reward: 27 | Loss: 2.887 | Steps: 28 | Worker: 0\n",
            "Episode: 76 | Moving Average Reward: 17 | Episode Reward: 20 | Loss: 4.179 | Steps: 21 | Worker: 1\n",
            "Episode: 77 | Moving Average Reward: 17 | Episode Reward: 15 | Loss: 1.942 | Steps: 16 | Worker: 1\n",
            "Episode: 78 | Moving Average Reward: 17 | Episode Reward: 40 | Loss: 3.652 | Steps: 41 | Worker: 0\n",
            "Episode: 79 | Moving Average Reward: 17 | Episode Reward: 21 | Loss: 3.489 | Steps: 22 | Worker: 1\n",
            "Episode: 80 | Moving Average Reward: 17 | Episode Reward: 46 | Loss: 3.116 | Steps: 47 | Worker: 0\n",
            "Episode: 81 | Moving Average Reward: 18 | Episode Reward: 31 | Loss: 2.796 | Steps: 32 | Worker: 1\n",
            "Episode: 82 | Moving Average Reward: 18 | Episode Reward: 17 | Loss: 2.274 | Steps: 18 | Worker: 0\n",
            "Episode: 83 | Moving Average Reward: 18 | Episode Reward: 20 | Loss: 4.041 | Steps: 21 | Worker: 1\n",
            "Episode: 84 | Moving Average Reward: 18 | Episode Reward: 20 | Loss: 3.984 | Steps: 21 | Worker: 1\n",
            "Episode: 85 | Moving Average Reward: 18 | Episode Reward: 42 | Loss: 3.242 | Steps: 43 | Worker: 0\n",
            "Episode: 86 | Moving Average Reward: 18 | Episode Reward: 18 | Loss: 2.371 | Steps: 19 | Worker: 1\n",
            "Episode: 87 | Moving Average Reward: 18 | Episode Reward: 34 | Loss: 2.811 | Steps: 35 | Worker: 1\n",
            "Episode: 88 | Moving Average Reward: 18 | Episode Reward: 26 | Loss: 3.203 | Steps: 27 | Worker: 1\n",
            "Episode: 89 | Moving Average Reward: 19 | Episode Reward: 71 | Loss: 3.147 | Steps: 72 | Worker: 0\n",
            "Saving best model to /tmp, episode score: 71.0\n",
            "Episode: 90 | Moving Average Reward: 19 | Episode Reward: 16 | Loss: 2.012 | Steps: 17 | Worker: 1\n",
            "Episode: 91 | Moving Average Reward: 19 | Episode Reward: 19 | Loss: 2.441 | Steps: 20 | Worker: 1\n",
            "Episode: 92 | Moving Average Reward: 19 | Episode Reward: 42 | Loss: 3.375 | Steps: 43 | Worker: 0\n",
            "Episode: 93 | Moving Average Reward: 19 | Episode Reward: 30 | Loss: 2.887 | Steps: 31 | Worker: 1\n",
            "Episode: 94 | Moving Average Reward: 19 | Episode Reward: 14 | Loss: 1.814 | Steps: 15 | Worker: 1\n",
            "Episode: 95 | Moving Average Reward: 19 | Episode Reward: 26 | Loss: 2.911 | Steps: 27 | Worker: 0\n",
            "Episode: 96 | Moving Average Reward: 19 | Episode Reward: 7 | Loss: 0.798 | Steps: 8 | Worker: 0\n",
            "Episode: 97 | Moving Average Reward: 19 | Episode Reward: 28 | Loss: 2.769 | Steps: 29 | Worker: 1\n",
            "Episode: 98 | Moving Average Reward: 19 | Episode Reward: 12 | Loss: 1.526 | Steps: 13 | Worker: 0\n",
            "Episode: 99 | Moving Average Reward: 19 | Episode Reward: 14 | Loss: 1.825 | Steps: 15 | Worker: 1\n",
            "Episode: 100 | Moving Average Reward: 19 | Episode Reward: 17 | Loss: 2.099 | Steps: 18 | Worker: 0\n",
            "Episode: 101 | Moving Average Reward: 19 | Episode Reward: 37 | Loss: 2.812 | Steps: 38 | Worker: 1Episode: 101 | Moving Average Reward: 19 | Episode Reward: 35 | Loss: 2.78 | Steps: 36 | Worker: 0\n",
            "\n",
            "Episode: 103 | Moving Average Reward: 19 | Episode Reward: 20 | Loss: 3.583 | Steps: 21 | Worker: 0\n",
            "Episode: 104 | Moving Average Reward: 19 | Episode Reward: 45 | Loss: 3.147 | Steps: 46 | Worker: 1\n",
            "Episode: 105 | Moving Average Reward: 19 | Episode Reward: 24 | Loss: 3.257 | Steps: 25 | Worker: 1\n",
            "Episode: 106 | Moving Average Reward: 19 | Episode Reward: 53 | Loss: 2.998 | Steps: 54 | Worker: 0\n",
            "Episode: 107 | Moving Average Reward: 19 | Episode Reward: 11 | Loss: 1.248 | Steps: 12 | Worker: 0\n",
            "Episode: 108 | Moving Average Reward: 19 | Episode Reward: 25 | Loss: 2.864 | Steps: 26 | Worker: 1\n",
            "Episode: 109 | Moving Average Reward: 19 | Episode Reward: 19 | Loss: 2.317 | Steps: 20 | Worker: 0\n",
            "Episode: 110 | Moving Average Reward: 19 | Episode Reward: 12 | Loss: 1.511 | Steps: 13 | Worker: 1\n",
            "Episode: 111 | Moving Average Reward: 19 | Episode Reward: 23 | Loss: 3.096 | Steps: 24 | Worker: 0\n",
            "Episode: 112 | Moving Average Reward: 19 | Episode Reward: 11 | Loss: 1.269 | Steps: 12 | Worker: 0\n",
            "Episode: 113 | Moving Average Reward: 19 | Episode Reward: 38 | Loss: 2.876 | Steps: 39 | Worker: 1\n",
            "Episode: 114 | Moving Average Reward: 19 | Episode Reward: 11 | Loss: 1.368 | Steps: 12 | Worker: 0\n",
            "Episode: 115 | Moving Average Reward: 19 | Episode Reward: 15 | Loss: 1.835 | Steps: 16 | Worker: 0\n",
            "Episode: 116 | Moving Average Reward: 19 | Episode Reward: 29 | Loss: 2.856 | Steps: 30 | Worker: 1\n",
            "Episode: 117 | Moving Average Reward: 19 | Episode Reward: 14 | Loss: 1.633 | Steps: 15 | Worker: 0\n",
            "Episode: 118 | Moving Average Reward: 19 | Episode Reward: 25 | Loss: 2.848 | Steps: 26 | Worker: 1\n",
            "Episode: 119 | Moving Average Reward: 19 | Episode Reward: 21 | Loss: 3.698 | Steps: 22 | Worker: 0\n",
            "Episode: 120 | Moving Average Reward: 19 | Episode Reward: 18 | Loss: 2.26 | Steps: 19 | Worker: 0\n",
            "Episode: 121 | Moving Average Reward: 19 | Episode Reward: 12 | Loss: 1.385 | Steps: 13 | Worker: 0\n",
            "Episode: 122 | Moving Average Reward: 19 | Episode Reward: 33 | Loss: 2.761 | Steps: 34 | Worker: 1\n",
            "Episode: 123 | Moving Average Reward: 19 | Episode Reward: 15 | Loss: 1.751 | Steps: 16 | Worker: 1\n",
            "Episode: 124 | Moving Average Reward: 20 | Episode Reward: 31 | Loss: 2.617 | Steps: 32 | Worker: 0\n",
            "Episode: 125 | Moving Average Reward: 19 | Episode Reward: 15 | Loss: 1.713 | Steps: 16 | Worker: 0\n",
            "Episode: 126 | Moving Average Reward: 20 | Episode Reward: 25 | Loss: 2.994 | Steps: 26 | Worker: 1\n",
            "Episode: 127 | Moving Average Reward: 20 | Episode Reward: 21 | Loss: 3.415 | Steps: 22 | Worker: 1\n",
            "Episode: 128 | Moving Average Reward: 20 | Episode Reward: 72 | Loss: 3.005 | Steps: 73 | Worker: 0\n",
            "Saving best model to /tmp, episode score: 72.0\n",
            "Episode: 129 | Moving Average Reward: 20 | Episode Reward: 21 | Loss: 3.331 | Steps: 22 | Worker: 1\n",
            "Episode: 130 | Moving Average Reward: 20 | Episode Reward: 19 | Loss: 2.257 | Steps: 20 | Worker: 0\n",
            "Episode: 131 | Moving Average Reward: 20 | Episode Reward: 21 | Loss: 3.475 | Steps: 22 | Worker: 1\n",
            "Episode: 132 | Moving Average Reward: 20 | Episode Reward: 25 | Loss: 2.985 | Steps: 26 | Worker: 1\n",
            "Episode: 133 | Moving Average Reward: 21 | Episode Reward: 74 | Loss: 3.069 | Steps: 75 | Worker: 0\n",
            "Saving best model to /tmp, episode score: 74.0\n",
            "Episode: 134 | Moving Average Reward: 21 | Episode Reward: 43 | Loss: 3.179 | Steps: 44 | Worker: 1\n",
            "Episode: 135 | Moving Average Reward: 21 | Episode Reward: 26 | Loss: 2.821 | Steps: 27 | Worker: 0\n",
            "Episode: 136 | Moving Average Reward: 21 | Episode Reward: 22 | Loss: 3.356 | Steps: 23 | Worker: 1\n",
            "Episode: 137 | Moving Average Reward: 21 | Episode Reward: 22 | Loss: 3.325 | Steps: 23 | Worker: 0\n",
            "Episode: 138 | Moving Average Reward: 21 | Episode Reward: 26 | Loss: 2.787 | Steps: 27 | Worker: 0\n",
            "Episode: 139 | Moving Average Reward: 21 | Episode Reward: 57 | Loss: 2.926 | Steps: 58 | Worker: 1\n",
            "Episode: 140 | Moving Average Reward: 21 | Episode Reward: 23 | Loss: 3.221 | Steps: 24 | Worker: 0\n",
            "Episode: 141 | Moving Average Reward: 21 | Episode Reward: 19 | Loss: 2.203 | Steps: 20 | Worker: 1\n",
            "Episode: 142 | Moving Average Reward: 21 | Episode Reward: 13 | Loss: 1.202 | Steps: 14 | Worker: 1\n",
            "Episode: 143 | Moving Average Reward: 22 | Episode Reward: 73 | Loss: 2.958 | Steps: 74 | Worker: 0\n",
            "Episode: 144 | Moving Average Reward: 22 | Episode Reward: 53 | Loss: 2.825 | Steps: 54 | Worker: 1\n",
            "Episode: 145 | Moving Average Reward: 22 | Episode Reward: 17 | Loss: 1.888 | Steps: 18 | Worker: 1\n",
            "Episode: 146 | Moving Average Reward: 22 | Episode Reward: 44 | Loss: 3.092 | Steps: 45 | Worker: 0\n",
            "Episode: 147 | Moving Average Reward: 22 | Episode Reward: 40 | Loss: 3.557 | Steps: 41 | Worker: 1\n",
            "Episode: 148 | Moving Average Reward: 22 | Episode Reward: 9 | Loss: 0.882 | Steps: 10 | Worker: 1\n",
            "Episode: 149 | Moving Average Reward: 22 | Episode Reward: 46 | Loss: 2.993 | Steps: 47 | Worker: 0\n",
            "Episode: 150 | Moving Average Reward: 22 | Episode Reward: 15 | Loss: 1.455 | Steps: 16 | Worker: 1\n",
            "Episode: 151 | Moving Average Reward: 23 | Episode Reward: 45 | Loss: 3.024 | Steps: 46 | Worker: 0\n",
            "Episode: 152 | Moving Average Reward: 23 | Episode Reward: 43 | Loss: 3.195 | Steps: 44 | Worker: 1\n",
            "Episode: 153 | Moving Average Reward: 23 | Episode Reward: 21 | Loss: 3.948 | Steps: 22 | Worker: 1\n",
            "Episode: 154 | Moving Average Reward: 23 | Episode Reward: 32 | Loss: 2.52 | Steps: 33 | Worker: 0\n",
            "Episode: 155 | Moving Average Reward: 23 | Episode Reward: 42 | Loss: 3.229 | Steps: 43 | Worker: 1\n",
            "Episode: 156 | Moving Average Reward: 23 | Episode Reward: 49 | Loss: 2.82 | Steps: 50 | Worker: 0\n",
            "Episode: 157 | Moving Average Reward: 23 | Episode Reward: 19 | Loss: 2.011 | Steps: 20 | Worker: 1\n",
            "Episode: 158 | Moving Average Reward: 23 | Episode Reward: 27 | Loss: 2.656 | Steps: 28 | Worker: 0\n",
            "Episode: 159 | Moving Average Reward: 23 | Episode Reward: 16 | Loss: 1.572 | Steps: 17 | Worker: 0\n",
            "Episode: 160 | Moving Average Reward: 23 | Episode Reward: 20 | Loss: 3.995 | Steps: 21 | Worker: 0\n",
            "Episode: 161 | Moving Average Reward: 24 | Episode Reward: 74 | Loss: 2.893 | Steps: 75 | Worker: 1\n",
            "Episode: 162 | Moving Average Reward: 24 | Episode Reward: 14 | Loss: 1.315 | Steps: 15 | Worker: 1\n",
            "Episode: 163 | Moving Average Reward: 24 | Episode Reward: 26 | Loss: 2.81 | Steps: 27 | Worker: 0\n",
            "Episode: 164 | Moving Average Reward: 23 | Episode Reward: 10 | Loss: 0.756 | Steps: 11 | Worker: 1\n",
            "Episode: 165 | Moving Average Reward: 23 | Episode Reward: 18 | Loss: 1.82 | Steps: 19 | Worker: 0\n",
            "Episode: 166 | Moving Average Reward: 24 | Episode Reward: 40 | Loss: 3.816 | Steps: 41 | Worker: 1\n",
            "Episode: 167 | Moving Average Reward: 24 | Episode Reward: 45 | Loss: 3.021 | Steps: 46 | Worker: 0\n",
            "Episode: 168 | Moving Average Reward: 24 | Episode Reward: 23 | Loss: 2.862 | Steps: 24 | Worker: 1\n",
            "Episode: 169 | Moving Average Reward: 24 | Episode Reward: 15 | Loss: 1.455 | Steps: 16 | Worker: 1\n",
            "Episode: 170 | Moving Average Reward: 24 | Episode Reward: 42 | Loss: 3.179 | Steps: 43 | Worker: 0\n",
            "Episode: 171 | Moving Average Reward: 24 | Episode Reward: 30 | Loss: 2.521 | Steps: 31 | Worker: 0\n",
            "Episode: 172 | Moving Average Reward: 24 | Episode Reward: 48 | Loss: 2.818 | Steps: 49 | Worker: 1\n",
            "Episode: 173 | Moving Average Reward: 24 | Episode Reward: 10 | Loss: 0.808 | Steps: 11 | Worker: 0\n",
            "Episode: 174 | Moving Average Reward: 24 | Episode Reward: 19 | Loss: 1.927 | Steps: 20 | Worker: 1\n",
            "Episode: 175 | Moving Average Reward: 24 | Episode Reward: 34 | Loss: 2.514 | Steps: 35 | Worker: 1\n",
            "Episode: 176 | Moving Average Reward: 24 | Episode Reward: 43 | Loss: 3.099 | Steps: 44 | Worker: 0\n",
            "Episode: 177 | Moving Average Reward: 24 | Episode Reward: 17 | Loss: 1.598 | Steps: 18 | Worker: 1\n",
            "Episode: 178 | Moving Average Reward: 24 | Episode Reward: 21 | Loss: 3.135 | Steps: 22 | Worker: 0\n",
            "Episode: 179 | Moving Average Reward: 24 | Episode Reward: 37 | Loss: 2.48 | Steps: 38 | Worker: 1\n",
            "Episode: 180 | Moving Average Reward: 24 | Episode Reward: 34 | Loss: 2.511 | Steps: 35 | Worker: 0\n",
            "Episode: 181 | Moving Average Reward: 24 | Episode Reward: 42 | Loss: 3.271 | Steps: 43 | Worker: 1\n",
            "Episode: 182 | Moving Average Reward: 25 | Episode Reward: 47 | Loss: 2.828 | Steps: 48 | Worker: 0\n",
            "Episode: 183 | Moving Average Reward: 25 | Episode Reward: 27 | Loss: 2.721 | Steps: 28 | Worker: 1\n",
            "Episode: 183 | Moving Average Reward: 25 | Episode Reward: 13 | Loss: 1.001 | Steps: 14 | Worker: 0\n",
            "Episode: 185 | Moving Average Reward: 24 | Episode Reward: 14 | Loss: 1.253 | Steps: 15 | Worker: 1\n",
            "Episode: 186 | Moving Average Reward: 25 | Episode Reward: 60 | Loss: 3.483 | Steps: 61 | Worker: 1\n",
            "Episode: 187 | Moving Average Reward: 26 | Episode Reward: 107 | Loss: 3.035 | Steps: 108 | Worker: 0\n",
            "Saving best model to /tmp, episode score: 107.0\n",
            "Episode: 188 | Moving Average Reward: 26 | Episode Reward: 19 | Loss: 1.683 | Steps: 20 | Worker: 1\n",
            "Episode: 189 | Moving Average Reward: 25 | Episode Reward: 15 | Loss: 1.333 | Steps: 16 | Worker: 0\n",
            "Episode: 190 | Moving Average Reward: 25 | Episode Reward: 20 | Loss: 4.901 | Steps: 21 | Worker: 1\n",
            "Episode: 191 | Moving Average Reward: 25 | Episode Reward: 10 | Loss: 0.772 | Steps: 11 | Worker: 1\n",
            "Episode: 192 | Moving Average Reward: 25 | Episode Reward: 28 | Loss: 2.457 | Steps: 29 | Worker: 1\n",
            "Episode: 193 | Moving Average Reward: 26 | Episode Reward: 86 | Loss: 3.113 | Steps: 87 | Worker: 0\n",
            "Episode: 194 | Moving Average Reward: 26 | Episode Reward: 59 | Loss: 2.847 | Steps: 60 | Worker: 1\n",
            "Episode: 195 | Moving Average Reward: 26 | Episode Reward: 24 | Loss: 2.843 | Steps: 25 | Worker: 0\n",
            "Episode: 196 | Moving Average Reward: 26 | Episode Reward: 51 | Loss: 2.725 | Steps: 52 | Worker: 1\n",
            "Episode: 197 | Moving Average Reward: 26 | Episode Reward: 32 | Loss: 2.459 | Steps: 33 | Worker: 0\n",
            "Episode: 198 | Moving Average Reward: 26 | Episode Reward: 15 | Loss: 1.214 | Steps: 16 | Worker: 0\n",
            "Episode: 199 | Moving Average Reward: 26 | Episode Reward: 37 | Loss: 2.35 | Steps: 38 | Worker: 0\n",
            "Episode: 200 | Moving Average Reward: 26 | Episode Reward: 12 | Loss: 0.856 | Steps: 13 | Worker: 0\n",
            "Episode: 201 | Moving Average Reward: 27 | Episode Reward: 68 | Loss: 2.972 | Steps: 69 | Worker: 1\n",
            "Episode: 202 | Moving Average Reward: 27 | Episode Reward: 38 | Loss: 2.423 | Steps: 39 | Worker: 0\n",
            "Episode: 203 | Moving Average Reward: 27 | Episode Reward: 42 | Loss: 3.091 | Steps: 43 | Worker: 1\n",
            "Episode: 204 | Moving Average Reward: 27 | Episode Reward: 37 | Loss: 2.379 | Steps: 38 | Worker: 0\n",
            "Episode: 205 | Moving Average Reward: 27 | Episode Reward: 25 | Loss: 2.798 | Steps: 26 | Worker: 1\n",
            "Episode: 206 | Moving Average Reward: 27 | Episode Reward: 23 | Loss: 3.008 | Steps: 24 | Worker: 0\n",
            "Episode: 207 | Moving Average Reward: 27 | Episode Reward: 20 | Loss: 4.106 | Steps: 21 | Worker: 1\n",
            "Episode: 208 | Moving Average Reward: 27 | Episode Reward: 17 | Loss: 1.325 | Steps: 18 | Worker: 0\n",
            "Episode: 209 | Moving Average Reward: 27 | Episode Reward: 15 | Loss: 1.193 | Steps: 16 | Worker: 1\n",
            "Episode: 210 | Moving Average Reward: 27 | Episode Reward: 18 | Loss: 1.453 | Steps: 19 | Worker: 0\n",
            "Episode: 211 | Moving Average Reward: 27 | Episode Reward: 25 | Loss: 2.712 | Steps: 26 | Worker: 1\n",
            "Episode: 212 | Moving Average Reward: 27 | Episode Reward: 57 | Loss: 2.685 | Steps: 58 | Worker: 1Episode: 212 | Moving Average Reward: 27 | Episode Reward: 65 | Loss: 3.075 | Steps: 66 | Worker: 0\n",
            "\n",
            "Episode: 214 | Moving Average Reward: 27 | Episode Reward: 16 | Loss: 1.189 | Steps: 17 | Worker: 0\n",
            "Episode: 215 | Moving Average Reward: 27 | Episode Reward: 29 | Loss: 2.402 | Steps: 30 | Worker: 1\n",
            "Episode: 216 | Moving Average Reward: 27 | Episode Reward: 22 | Loss: 3.37 | Steps: 23 | Worker: 0\n",
            "Episode: 217 | Moving Average Reward: 27 | Episode Reward: 51 | Loss: 2.614 | Steps: 52 | Worker: 0\n",
            "Episode: 218 | Moving Average Reward: 27 | Episode Reward: 65 | Loss: 3.061 | Steps: 66 | Worker: 1\n",
            "Episode: 219 | Moving Average Reward: 27 | Episode Reward: 18 | Loss: 1.394 | Steps: 19 | Worker: 1\n",
            "Episode: 220 | Moving Average Reward: 27 | Episode Reward: 27 | Loss: 2.579 | Steps: 28 | Worker: 0\n",
            "Episode: 221 | Moving Average Reward: 27 | Episode Reward: 14 | Loss: 0.995 | Steps: 15 | Worker: 0\n",
            "Episode: 222 | Moving Average Reward: 27 | Episode Reward: 27 | Loss: 2.6 | Steps: 28 | Worker: 1\n",
            "Episode: 223 | Moving Average Reward: 27 | Episode Reward: 32 | Loss: 2.631 | Steps: 33 | Worker: 1\n",
            "Episode: 224 | Moving Average Reward: 27 | Episode Reward: 21 | Loss: 3.472 | Steps: 22 | Worker: 1\n",
            "Episode: 225 | Moving Average Reward: 27 | Episode Reward: 18 | Loss: 1.311 | Steps: 19 | Worker: 1\n",
            "Episode: 226 | Moving Average Reward: 27 | Episode Reward: 30 | Loss: 2.434 | Steps: 31 | Worker: 1\n",
            "Episode: 227 | Moving Average Reward: 27 | Episode Reward: 73 | Loss: 2.715 | Steps: 74 | Worker: 1\n",
            "Episode: 228 | Moving Average Reward: 29 | Episode Reward: 145 | Loss: 3.184 | Steps: 146 | Worker: 0\n",
            "Saving best model to /tmp, episode score: 145.0\n",
            "Episode: 229 | Moving Average Reward: 28 | Episode Reward: 10 | Loss: 0.658 | Steps: 11 | Worker: 0\n",
            "Episode: 230 | Moving Average Reward: 28 | Episode Reward: 16 | Loss: 1.087 | Steps: 17 | Worker: 1\n",
            "Episode: 231 | Moving Average Reward: 28 | Episode Reward: 26 | Loss: 2.757 | Steps: 27 | Worker: 1\n",
            "Episode: 232 | Moving Average Reward: 29 | Episode Reward: 53 | Loss: 2.702 | Steps: 54 | Worker: 0\n",
            "Episode: 233 | Moving Average Reward: 28 | Episode Reward: 27 | Loss: 2.495 | Steps: 28 | Worker: 1\n",
            "Episode: 234 | Moving Average Reward: 29 | Episode Reward: 40 | Loss: 3.45 | Steps: 41 | Worker: 0\n",
            "Episode: 235 | Moving Average Reward: 29 | Episode Reward: 51 | Loss: 2.579 | Steps: 52 | Worker: 1\n",
            "Episode: 236 | Moving Average Reward: 29 | Episode Reward: 17 | Loss: 0.981 | Steps: 18 | Worker: 1\n",
            "Episode: 237 | Moving Average Reward: 29 | Episode Reward: 29 | Loss: 2.349 | Steps: 30 | Worker: 0\n",
            "Episode: 238 | Moving Average Reward: 29 | Episode Reward: 28 | Loss: 2.372 | Steps: 29 | Worker: 0\n",
            "Episode: 239 | Moving Average Reward: 29 | Episode Reward: 20 | Loss: 3.705 | Steps: 21 | Worker: 1\n",
            "Episode: 240 | Moving Average Reward: 29 | Episode Reward: 22 | Loss: 3.334 | Steps: 23 | Worker: 0\n",
            "Episode: 241 | Moving Average Reward: 29 | Episode Reward: 31 | Loss: 2.319 | Steps: 32 | Worker: 1\n",
            "Episode: 242 | Moving Average Reward: 28 | Episode Reward: 18 | Loss: 1.187 | Steps: 19 | Worker: 1\n",
            "Episode: 243 | Moving Average Reward: 29 | Episode Reward: 52 | Loss: 2.6 | Steps: 53 | Worker: 0\n",
            "Episode: 244 | Moving Average Reward: 29 | Episode Reward: 27 | Loss: 2.643 | Steps: 28 | Worker: 1\n",
            "Episode: 245 | Moving Average Reward: 29 | Episode Reward: 35 | Loss: 2.207 | Steps: 36 | Worker: 1\n",
            "Episode: 246 | Moving Average Reward: 29 | Episode Reward: 40 | Loss: 3.708 | Steps: 41 | Worker: 0\n",
            "Episode: 247 | Moving Average Reward: 29 | Episode Reward: 8 | Loss: 0.678 | Steps: 9 | Worker: 0\n",
            "Episode: 248 | Moving Average Reward: 29 | Episode Reward: 23 | Loss: 3.872 | Steps: 24 | Worker: 1\n",
            "Episode: 249 | Moving Average Reward: 29 | Episode Reward: 57 | Loss: 2.455 | Steps: 58 | Worker: 0\n",
            "Episode: 250 | Moving Average Reward: 29 | Episode Reward: 16 | Loss: 0.899 | Steps: 17 | Worker: 0\n",
            "Episode: 251 | Moving Average Reward: 29 | Episode Reward: 64 | Loss: 3.054 | Steps: 65 | Worker: 1\n",
            "Episode: 252 | Moving Average Reward: 30 | Episode Reward: 81 | Loss: 3.108 | Steps: 82 | Worker: 0\n",
            "Episode: 253 | Moving Average Reward: 30 | Episode Reward: 90 | Loss: 2.779 | Steps: 91 | Worker: 1\n",
            "Episode: 254 | Moving Average Reward: 30 | Episode Reward: 32 | Loss: 2.117 | Steps: 33 | Worker: 1\n",
            "Episode: 255 | Moving Average Reward: 31 | Episode Reward: 66 | Loss: 2.778 | Steps: 67 | Worker: 0\n",
            "Episode: 256 | Moving Average Reward: 30 | Episode Reward: 17 | Loss: 0.961 | Steps: 18 | Worker: 0\n",
            "Episode: 257 | Moving Average Reward: 31 | Episode Reward: 67 | Loss: 3.083 | Steps: 68 | Worker: 1\n",
            "Episode: 258 | Moving Average Reward: 31 | Episode Reward: 46 | Loss: 2.842 | Steps: 47 | Worker: 0\n",
            "Episode: 259 | Moving Average Reward: 31 | Episode Reward: 33 | Loss: 2.392 | Steps: 34 | Worker: 1\n",
            "Episode: 260 | Moving Average Reward: 31 | Episode Reward: 16 | Loss: 0.757 | Steps: 17 | Worker: 0\n",
            "Episode: 261 | Moving Average Reward: 31 | Episode Reward: 46 | Loss: 2.934 | Steps: 47 | Worker: 1\n",
            "Episode: 262 | Moving Average Reward: 31 | Episode Reward: 67 | Loss: 3.129 | Steps: 68 | Worker: 0\n",
            "Episode: 263 | Moving Average Reward: 31 | Episode Reward: 29 | Loss: 2.351 | Steps: 30 | Worker: 1\n",
            "Episode: 264 | Moving Average Reward: 31 | Episode Reward: 26 | Loss: 2.799 | Steps: 27 | Worker: 1\n",
            "Episode: 265 | Moving Average Reward: 31 | Episode Reward: 58 | Loss: 2.546 | Steps: 59 | Worker: 1\n",
            "Episode: 266 | Moving Average Reward: 32 | Episode Reward: 108 | Loss: 2.817 | Steps: 109 | Worker: 0\n",
            "Episode: 267 | Moving Average Reward: 32 | Episode Reward: 9 | Loss: 1.13 | Steps: 10 | Worker: 0\n",
            "Episode: 268 | Moving Average Reward: 32 | Episode Reward: 39 | Loss: 2.092 | Steps: 40 | Worker: 1\n",
            "Episode: 269 | Moving Average Reward: 32 | Episode Reward: 21 | Loss: 4.085 | Steps: 22 | Worker: 1\n",
            "Episode: 270 | Moving Average Reward: 32 | Episode Reward: 29 | Loss: 2.354 | Steps: 30 | Worker: 0\n",
            "Episode: 271 | Moving Average Reward: 32 | Episode Reward: 23 | Loss: 3.401 | Steps: 24 | Worker: 0\n",
            "Episode: 272 | Moving Average Reward: 32 | Episode Reward: 40 | Loss: 3.578 | Steps: 41 | Worker: 1\n",
            "Episode: 273 | Moving Average Reward: 32 | Episode Reward: 38 | Loss: 2.015 | Steps: 39 | Worker: 0\n",
            "Episode: 274 | Moving Average Reward: 32 | Episode Reward: 17 | Loss: 0.858 | Steps: 18 | Worker: 0\n",
            "Episode: 275 | Moving Average Reward: 32 | Episode Reward: 47 | Loss: 2.652 | Steps: 48 | Worker: 1\n",
            "Episode: 276 | Moving Average Reward: 32 | Episode Reward: 16 | Loss: 0.688 | Steps: 17 | Worker: 1\n",
            "Episode: 277 | Moving Average Reward: 32 | Episode Reward: 52 | Loss: 2.532 | Steps: 53 | Worker: 0\n",
            "Episode: 278 | Moving Average Reward: 32 | Episode Reward: 23 | Loss: 3.08 | Steps: 24 | Worker: 1\n",
            "Episode: 279 | Moving Average Reward: 32 | Episode Reward: 30 | Loss: 2.0 | Steps: 31 | Worker: 0\n",
            "Episode: 280 | Moving Average Reward: 32 | Episode Reward: 40 | Loss: 3.775 | Steps: 41 | Worker: 1\n",
            "Episode: 281 | Moving Average Reward: 32 | Episode Reward: 52 | Loss: 2.486 | Steps: 53 | Worker: 0\n",
            "Episode: 282 | Moving Average Reward: 32 | Episode Reward: 51 | Loss: 2.557 | Steps: 52 | Worker: 1\n",
            "Episode: 283 | Moving Average Reward: 32 | Episode Reward: 32 | Loss: 2.01 | Steps: 33 | Worker: 0\n",
            "Episode: 284 | Moving Average Reward: 32 | Episode Reward: 30 | Loss: 2.245 | Steps: 31 | Worker: 1\n",
            "Episode: 285 | Moving Average Reward: 32 | Episode Reward: 33 | Loss: 2.193 | Steps: 34 | Worker: 0\n",
            "Episode: 286 | Moving Average Reward: 32 | Episode Reward: 27 | Loss: 2.455 | Steps: 28 | Worker: 1\n",
            "Episode: 287 | Moving Average Reward: 32 | Episode Reward: 61 | Loss: 3.669 | Steps: 62 | Worker: 0\n",
            "Episode: 288 | Moving Average Reward: 33 | Episode Reward: 48 | Loss: 2.808 | Steps: 49 | Worker: 1\n",
            "Episode: 289 | Moving Average Reward: 32 | Episode Reward: 16 | Loss: 0.771 | Steps: 17 | Worker: 1\n",
            "Episode: 289 | Moving Average Reward: 32 | Episode Reward: 21 | Loss: 4.147 | Steps: 22 | Worker: 0\n",
            "Episode: 291 | Moving Average Reward: 32 | Episode Reward: 33 | Loss: 1.857 | Steps: 34 | Worker: 0\n",
            "Episode: 292 | Moving Average Reward: 33 | Episode Reward: 53 | Loss: 2.451 | Steps: 54 | Worker: 1\n",
            "Episode: 293 | Moving Average Reward: 33 | Episode Reward: 20 | Loss: 4.668 | Steps: 21 | Worker: 0\n",
            "Episode: 294 | Moving Average Reward: 32 | Episode Reward: 13 | Loss: 0.575 | Steps: 14 | Worker: 0\n",
            "Episode: 295 | Moving Average Reward: 33 | Episode Reward: 67 | Loss: 2.692 | Steps: 68 | Worker: 1\n",
            "Episode: 296 | Moving Average Reward: 33 | Episode Reward: 40 | Loss: 3.531 | Steps: 41 | Worker: 0\n",
            "Episode: 297 | Moving Average Reward: 33 | Episode Reward: 19 | Loss: 0.913 | Steps: 20 | Worker: 1\n",
            "Episode: 298 | Moving Average Reward: 33 | Episode Reward: 38 | Loss: 2.061 | Steps: 39 | Worker: 1\n",
            "Episode: 299 | Moving Average Reward: 33 | Episode Reward: 74 | Loss: 2.563 | Steps: 75 | Worker: 0\n",
            "Episode: 300 | Moving Average Reward: 33 | Episode Reward: 63 | Loss: 2.981 | Steps: 64 | Worker: 0\n",
            "Episode: 301 | Moving Average Reward: 34 | Episode Reward: 91 | Loss: 2.813 | Steps: 92 | Worker: 1\n",
            "Episode: 302 | Moving Average Reward: 34 | Episode Reward: 31 | Loss: 2.175 | Steps: 32 | Worker: 0\n",
            "Episode: 303 | Moving Average Reward: 34 | Episode Reward: 10 | Loss: 0.599 | Steps: 11 | Worker: 0\n",
            "Episode: 304 | Moving Average Reward: 34 | Episode Reward: 67 | Loss: 2.949 | Steps: 68 | Worker: 1\n",
            "Episode: 305 | Moving Average Reward: 34 | Episode Reward: 49 | Loss: 2.712 | Steps: 50 | Worker: 0\n",
            "Episode: 306 | Moving Average Reward: 34 | Episode Reward: 21 | Loss: 4.456 | Steps: 22 | Worker: 1\n",
            "Episode: 307 | Moving Average Reward: 34 | Episode Reward: 15 | Loss: 0.643 | Steps: 16 | Worker: 0\n",
            "Episode: 308 | Moving Average Reward: 34 | Episode Reward: 15 | Loss: 0.616 | Steps: 16 | Worker: 0\n",
            "Episode: 309 | Moving Average Reward: 34 | Episode Reward: 28 | Loss: 2.42 | Steps: 29 | Worker: 1\n",
            "Episode: 310 | Moving Average Reward: 34 | Episode Reward: 57 | Loss: 2.415 | Steps: 58 | Worker: 1\n",
            "Episode: 311 | Moving Average Reward: 34 | Episode Reward: 66 | Loss: 2.807 | Steps: 67 | Worker: 0\n",
            "Episode: 312 | Moving Average Reward: 34 | Episode Reward: 58 | Loss: 2.506 | Steps: 59 | Worker: 1\n",
            "Episode: 313 | Moving Average Reward: 34 | Episode Reward: 13 | Loss: 0.581 | Steps: 14 | Worker: 1\n",
            "Episode: 314 | Moving Average Reward: 35 | Episode Reward: 83 | Loss: 3.037 | Steps: 84 | Worker: 0\n",
            "Episode: 315 | Moving Average Reward: 35 | Episode Reward: 45 | Loss: 2.961 | Steps: 46 | Worker: 1\n",
            "Episode: 316 | Moving Average Reward: 35 | Episode Reward: 28 | Loss: 2.693 | Steps: 29 | Worker: 0\n",
            "Episode: 317 | Moving Average Reward: 34 | Episode Reward: 17 | Loss: 0.764 | Steps: 18 | Worker: 0\n",
            "Episode: 318 | Moving Average Reward: 34 | Episode Reward: 7 | Loss: 3.198 | Steps: 8 | Worker: 0\n",
            "Episode: 319 | Moving Average Reward: 34 | Episode Reward: 63 | Loss: 3.084 | Steps: 64 | Worker: 1\n",
            "Episode: 320 | Moving Average Reward: 35 | Episode Reward: 54 | Loss: 2.133 | Steps: 55 | Worker: 0\n",
            "Episode: 321 | Moving Average Reward: 35 | Episode Reward: 52 | Loss: 2.706 | Steps: 53 | Worker: 1\n",
            "Episode: 322 | Moving Average Reward: 35 | Episode Reward: 36 | Loss: 2.066 | Steps: 37 | Worker: 1Episode: 322 | Moving Average Reward: 35 | Episode Reward: 77 | Loss: 2.43 | Steps: 78 | Worker: 0\n",
            "\n",
            "Episode: 324 | Moving Average Reward: 35 | Episode Reward: 25 | Loss: 3.526 | Steps: 26 | Worker: 0\n",
            "Episode: 325 | Moving Average Reward: 35 | Episode Reward: 20 | Loss: 8.822 | Steps: 21 | Worker: 0\n",
            "Episode: 326 | Moving Average Reward: 35 | Episode Reward: 75 | Loss: 2.261 | Steps: 76 | Worker: 1\n",
            "Episode: 327 | Moving Average Reward: 35 | Episode Reward: 46 | Loss: 3.152 | Steps: 47 | Worker: 0\n",
            "Episode: 328 | Moving Average Reward: 35 | Episode Reward: 16 | Loss: 0.912 | Steps: 17 | Worker: 0\n",
            "Episode: 329 | Moving Average Reward: 36 | Episode Reward: 90 | Loss: 2.596 | Steps: 91 | Worker: 1\n",
            "Episode: 330 | Moving Average Reward: 36 | Episode Reward: 45 | Loss: 3.306 | Steps: 46 | Worker: 0\n",
            "Episode: 331 | Moving Average Reward: 36 | Episode Reward: 26 | Loss: 3.414 | Steps: 27 | Worker: 1\n",
            "Episode: 332 | Moving Average Reward: 36 | Episode Reward: 55 | Loss: 2.108 | Steps: 56 | Worker: 0\n",
            "Episode: 333 | Moving Average Reward: 36 | Episode Reward: 76 | Loss: 2.422 | Steps: 77 | Worker: 1\n",
            "Episode: 334 | Moving Average Reward: 37 | Episode Reward: 69 | Loss: 2.683 | Steps: 70 | Worker: 0\n",
            "Episode: 335 | Moving Average Reward: 37 | Episode Reward: 26 | Loss: 3.42 | Steps: 27 | Worker: 0\n",
            "Episode: 336 | Moving Average Reward: 37 | Episode Reward: 114 | Loss: 2.596 | Steps: 115 | Worker: 1\n",
            "Episode: 337 | Moving Average Reward: 38 | Episode Reward: 75 | Loss: 2.409 | Steps: 76 | Worker: 0\n",
            "Episode: 338 | Moving Average Reward: 38 | Episode Reward: 40 | Loss: 3.616 | Steps: 41 | Worker: 1\n",
            "Episode: 339 | Moving Average Reward: 38 | Episode Reward: 18 | Loss: 0.685 | Steps: 19 | Worker: 1\n",
            "Episode: 340 | Moving Average Reward: 38 | Episode Reward: 41 | Loss: 3.723 | Steps: 42 | Worker: 0\n",
            "Episode: 341 | Moving Average Reward: 37 | Episode Reward: 26 | Loss: 3.656 | Steps: 27 | Worker: 1\n",
            "Episode: 342 | Moving Average Reward: 37 | Episode Reward: 18 | Loss: 0.829 | Steps: 19 | Worker: 0\n",
            "Episode: 343 | Moving Average Reward: 37 | Episode Reward: 20 | Loss: 5.048 | Steps: 21 | Worker: 1\n",
            "Episode: 344 | Moving Average Reward: 37 | Episode Reward: 45 | Loss: 2.931 | Steps: 46 | Worker: 1\n",
            "Episode: 345 | Moving Average Reward: 38 | Episode Reward: 116 | Loss: 2.604 | Steps: 117 | Worker: 0\n",
            "Episode: 346 | Moving Average Reward: 38 | Episode Reward: 51 | Loss: 2.622 | Steps: 52 | Worker: 1\n",
            "Episode: 347 | Moving Average Reward: 38 | Episode Reward: 39 | Loss: 1.933 | Steps: 40 | Worker: 0\n",
            "Episode: 348 | Moving Average Reward: 38 | Episode Reward: 23 | Loss: 3.906 | Steps: 24 | Worker: 0\n",
            "Episode: 349 | Moving Average Reward: 38 | Episode Reward: 75 | Loss: 2.585 | Steps: 76 | Worker: 1\n",
            "Episode: 350 | Moving Average Reward: 38 | Episode Reward: 23 | Loss: 4.418 | Steps: 24 | Worker: 0\n",
            "Episode: 351 | Moving Average Reward: 38 | Episode Reward: 18 | Loss: 0.679 | Steps: 19 | Worker: 1\n",
            "Episode: 352 | Moving Average Reward: 38 | Episode Reward: 18 | Loss: 0.726 | Steps: 19 | Worker: 0\n",
            "Episode: 353 | Moving Average Reward: 38 | Episode Reward: 37 | Loss: 2.267 | Steps: 38 | Worker: 0\n",
            "Episode: 354 | Moving Average Reward: 38 | Episode Reward: 82 | Loss: 3.139 | Steps: 83 | Worker: 1\n",
            "Episode: 355 | Moving Average Reward: 38 | Episode Reward: 53 | Loss: 2.554 | Steps: 54 | Worker: 1\n",
            "Episode: 356 | Moving Average Reward: 38 | Episode Reward: 42 | Loss: 4.015 | Steps: 43 | Worker: 1\n",
            "Episode: 357 | Moving Average Reward: 39 | Episode Reward: 142 | Loss: 2.883 | Steps: 143 | Worker: 0\n",
            "Episode: 358 | Moving Average Reward: 39 | Episode Reward: 19 | Loss: 0.818 | Steps: 20 | Worker: 1\n",
            "Episode: 359 | Moving Average Reward: 39 | Episode Reward: 25 | Loss: 3.975 | Steps: 26 | Worker: 1\n",
            "Episode: 360 | Moving Average Reward: 39 | Episode Reward: 49 | Loss: 2.988 | Steps: 50 | Worker: 1\n",
            "Episode: 361 | Moving Average Reward: 40 | Episode Reward: 109 | Loss: 3.105 | Steps: 110 | Worker: 0\n",
            "Episode: 362 | Moving Average Reward: 40 | Episode Reward: 30 | Loss: 2.886 | Steps: 31 | Worker: 1\n",
            "Episode: 363 | Moving Average Reward: 40 | Episode Reward: 47 | Loss: 3.082 | Steps: 48 | Worker: 1\n",
            "Episode: 364 | Moving Average Reward: 40 | Episode Reward: 38 | Loss: 1.998 | Steps: 39 | Worker: 1\n",
            "Episode: 365 | Moving Average Reward: 40 | Episode Reward: 20 | Loss: 7.517 | Steps: 21 | Worker: 1\n",
            "Episode: 366 | Moving Average Reward: 41 | Episode Reward: 198 | Loss: 2.674 | Steps: 199 | Worker: 0\n",
            "Saving best model to /tmp, episode score: 198.0\n",
            "Episode: 367 | Moving Average Reward: 41 | Episode Reward: 49 | Loss: 2.514 | Steps: 50 | Worker: 1\n",
            "Episode: 368 | Moving Average Reward: 41 | Episode Reward: 30 | Loss: 2.562 | Steps: 31 | Worker: 0\n",
            "Episode: 369 | Moving Average Reward: 41 | Episode Reward: 53 | Loss: 2.452 | Steps: 54 | Worker: 1\n",
            "Episode: 370 | Moving Average Reward: 41 | Episode Reward: 26 | Loss: 4.899 | Steps: 27 | Worker: 1\n",
            "Episode: 371 | Moving Average Reward: 41 | Episode Reward: 50 | Loss: 2.731 | Steps: 51 | Worker: 1\n",
            "Episode: 371 | Moving Average Reward: 42 | Episode Reward: 111 | Loss: 2.559 | Steps: 112 | Worker: 0\n",
            "Episode: 373 | Moving Average Reward: 41 | Episode Reward: 39 | Loss: 1.953 | Steps: 40 | Worker: 1\n",
            "Episode: 374 | Moving Average Reward: 41 | Episode Reward: 66 | Loss: 3.151 | Steps: 67 | Worker: 0\n",
            "Episode: 375 | Moving Average Reward: 41 | Episode Reward: 48 | Loss: 2.83 | Steps: 49 | Worker: 1\n",
            "Episode: 376 | Moving Average Reward: 42 | Episode Reward: 60 | Loss: 4.406 | Steps: 61 | Worker: 0\n",
            "Episode: 377 | Moving Average Reward: 42 | Episode Reward: 90 | Loss: 2.518 | Steps: 91 | Worker: 1\n",
            "Episode: 378 | Moving Average Reward: 42 | Episode Reward: 89 | Loss: 2.649 | Steps: 90 | Worker: 0\n",
            "Episode: 379 | Moving Average Reward: 43 | Episode Reward: 64 | Loss: 3.375 | Steps: 65 | Worker: 1\n",
            "Episode: 380 | Moving Average Reward: 43 | Episode Reward: 42 | Loss: 4.087 | Steps: 43 | Worker: 1\n",
            "Episode: 381 | Moving Average Reward: 43 | Episode Reward: 63 | Loss: 3.676 | Steps: 64 | Worker: 0\n",
            "Episode: 382 | Moving Average Reward: 43 | Episode Reward: 31 | Loss: 2.677 | Steps: 32 | Worker: 0\n",
            "Episode: 383 | Moving Average Reward: 43 | Episode Reward: 35 | Loss: 2.193 | Steps: 36 | Worker: 0\n",
            "Episode: 384 | Moving Average Reward: 43 | Episode Reward: 90 | Loss: 2.541 | Steps: 91 | Worker: 1\n",
            "Episode: 385 | Moving Average Reward: 43 | Episode Reward: 38 | Loss: 1.795 | Steps: 39 | Worker: 1\n",
            "Episode: 386 | Moving Average Reward: 43 | Episode Reward: 68 | Loss: 2.65 | Steps: 69 | Worker: 0\n",
            "Episode: 387 | Moving Average Reward: 43 | Episode Reward: 15 | Loss: 1.539 | Steps: 16 | Worker: 1\n",
            "Episode: 388 | Moving Average Reward: 43 | Episode Reward: 15 | Loss: 1.507 | Steps: 16 | Worker: 1\n",
            "Episode: 389 | Moving Average Reward: 43 | Episode Reward: 34 | Loss: 2.615 | Steps: 35 | Worker: 0\n",
            "Episode: 390 | Moving Average Reward: 43 | Episode Reward: 93 | Loss: 2.468 | Steps: 94 | Worker: 1\n",
            "Episode: 391 | Moving Average Reward: 43 | Episode Reward: 51 | Loss: 2.794 | Steps: 52 | Worker: 1\n",
            "Episode: 392 | Moving Average Reward: 44 | Episode Reward: 140 | Loss: 3.859 | Steps: 141 | Worker: 0\n",
            "Episode: 393 | Moving Average Reward: 44 | Episode Reward: 52 | Loss: 2.461 | Steps: 53 | Worker: 1\n",
            "Episode: 394 | Moving Average Reward: 44 | Episode Reward: 51 | Loss: 2.654 | Steps: 52 | Worker: 1\n",
            "Episode: 395 | Moving Average Reward: 45 | Episode Reward: 150 | Loss: 3.45 | Steps: 151 | Worker: 0\n",
            "Episode: 396 | Moving Average Reward: 46 | Episode Reward: 68 | Loss: 2.964 | Steps: 69 | Worker: 1\n",
            "Episode: 397 | Moving Average Reward: 45 | Episode Reward: 21 | Loss: 5.961 | Steps: 22 | Worker: 0\n",
            "Episode: 398 | Moving Average Reward: 45 | Episode Reward: 15 | Loss: 2.014 | Steps: 16 | Worker: 0\n",
            "Episode: 399 | Moving Average Reward: 45 | Episode Reward: 50 | Loss: 2.63 | Steps: 51 | Worker: 1\n",
            "Episode: 400 | Moving Average Reward: 45 | Episode Reward: 70 | Loss: 3.012 | Steps: 71 | Worker: 0\n",
            "Episode: 401 | Moving Average Reward: 45 | Episode Reward: 29 | Loss: 3.25 | Steps: 30 | Worker: 0\n",
            "Episode: 402 | Moving Average Reward: 46 | Episode Reward: 90 | Loss: 2.719 | Steps: 91 | Worker: 1\n",
            "Episode: 403 | Moving Average Reward: 46 | Episode Reward: 52 | Loss: 2.547 | Steps: 53 | Worker: 0\n",
            "Episode: 404 | Moving Average Reward: 46 | Episode Reward: 58 | Loss: 2.103 | Steps: 59 | Worker: 1\n",
            "Episode: 405 | Moving Average Reward: 46 | Episode Reward: 30 | Loss: 3.279 | Steps: 31 | Worker: 0\n",
            "Episode: 406 | Moving Average Reward: 46 | Episode Reward: 33 | Loss: 3.128 | Steps: 34 | Worker: 0\n",
            "Episode: 407 | Moving Average Reward: 46 | Episode Reward: 87 | Loss: 3.788 | Steps: 88 | Worker: 1\n",
            "Episode: 408 | Moving Average Reward: 46 | Episode Reward: 37 | Loss: 2.47 | Steps: 38 | Worker: 1\n",
            "Episode: 409 | Moving Average Reward: 47 | Episode Reward: 159 | Loss: 2.728 | Steps: 160 | Worker: 0\n",
            "Episode: 410 | Moving Average Reward: 47 | Episode Reward: 53 | Loss: 2.884 | Steps: 54 | Worker: 0\n",
            "Episode: 411 | Moving Average Reward: 47 | Episode Reward: 23 | Loss: 6.167 | Steps: 24 | Worker: 0\n",
            "Episode: 412 | Moving Average Reward: 47 | Episode Reward: 57 | Loss: 2.255 | Steps: 58 | Worker: 0\n",
            "Episode: 413 | Moving Average Reward: 49 | Episode Reward: 230 | Loss: 2.682 | Steps: 231 | Worker: 1\n",
            "Saving best model to /tmp, episode score: 230.0\n",
            "Episode: 414 | Moving Average Reward: 49 | Episode Reward: 130 | Loss: 2.613 | Steps: 131 | Worker: 1\n",
            "Episode: 415 | Moving Average Reward: 51 | Episode Reward: 164 | Loss: 3.161 | Steps: 165 | Worker: 0\n",
            "Episode: 416 | Moving Average Reward: 51 | Episode Reward: 126 | Loss: 3.341 | Steps: 127 | Worker: 0\n",
            "Episode: 417 | Moving Average Reward: 52 | Episode Reward: 163 | Loss: 2.971 | Steps: 164 | Worker: 1\n",
            "Episode: 418 | Moving Average Reward: 52 | Episode Reward: 38 | Loss: 2.509 | Steps: 39 | Worker: 0\n",
            "Episode: 419 | Moving Average Reward: 52 | Episode Reward: 52 | Loss: 2.818 | Steps: 53 | Worker: 1\n",
            "Episode: 420 | Moving Average Reward: 53 | Episode Reward: 72 | Loss: 2.776 | Steps: 73 | Worker: 0\n",
            "Episode: 421 | Moving Average Reward: 53 | Episode Reward: 113 | Loss: 2.745 | Steps: 114 | Worker: 1\n",
            "Episode: 422 | Moving Average Reward: 53 | Episode Reward: 79 | Loss: 2.083 | Steps: 80 | Worker: 1\n",
            "Episode: 423 | Moving Average Reward: 55 | Episode Reward: 180 | Loss: 3.166 | Steps: 181 | Worker: 0\n",
            "Episode: 424 | Moving Average Reward: 54 | Episode Reward: 35 | Loss: 2.727 | Steps: 36 | Worker: 0\n",
            "Episode: 425 | Moving Average Reward: 54 | Episode Reward: 53 | Loss: 2.881 | Steps: 54 | Worker: 0\n",
            "Episode: 426 | Moving Average Reward: 57 | Episode Reward: 276 | Loss: 2.538 | Steps: 277 | Worker: 1\n",
            "Saving best model to /tmp, episode score: 276.0\n",
            "Episode: 427 | Moving Average Reward: 57 | Episode Reward: 84 | Loss: 3.012 | Steps: 85 | Worker: 1\n",
            "Episode: 428 | Moving Average Reward: 57 | Episode Reward: 117 | Loss: 2.822 | Steps: 118 | Worker: 1\n",
            "Episode: 429 | Moving Average Reward: 61 | Episode Reward: 418 | Loss: 2.379 | Steps: 419 | Worker: 0\n",
            "Saving best model to /tmp, episode score: 418.0\n",
            "Episode: 430 | Moving Average Reward: 62 | Episode Reward: 138 | Loss: 3.269 | Steps: 139 | Worker: 1\n",
            "Episode: 431 | Moving Average Reward: 63 | Episode Reward: 159 | Loss: 2.225 | Steps: 160 | Worker: 0\n",
            "Episode: 432 | Moving Average Reward: 63 | Episode Reward: 88 | Loss: 4.075 | Steps: 89 | Worker: 0\n",
            "Episode: 433 | Moving Average Reward: 64 | Episode Reward: 205 | Loss: 3.044 | Steps: 206 | Worker: 1\n",
            "Episode: 434 | Moving Average Reward: 65 | Episode Reward: 90 | Loss: 2.747 | Steps: 91 | Worker: 0\n",
            "Episode: 435 | Moving Average Reward: 65 | Episode Reward: 132 | Loss: 3.519 | Steps: 133 | Worker: 0\n",
            "Episode: 436 | Moving Average Reward: 66 | Episode Reward: 160 | Loss: 3.172 | Steps: 161 | Worker: 1\n",
            "Episode: 437 | Moving Average Reward: 66 | Episode Reward: 43 | Loss: 5.243 | Steps: 44 | Worker: 1\n",
            "Episode: 438 | Moving Average Reward: 66 | Episode Reward: 38 | Loss: 3.297 | Steps: 39 | Worker: 1\n",
            "Episode: 439 | Moving Average Reward: 66 | Episode Reward: 94 | Loss: 3.825 | Steps: 95 | Worker: 1\n",
            "Episode: 440 | Moving Average Reward: 68 | Episode Reward: 212 | Loss: 2.62 | Steps: 213 | Worker: 0\n",
            "Episode: 441 | Moving Average Reward: 67 | Episode Reward: 15 | Loss: 5.209 | Steps: 16 | Worker: 1\n",
            "Episode: 442 | Moving Average Reward: 67 | Episode Reward: 44 | Loss: 5.914 | Steps: 45 | Worker: 1\n",
            "Episode: 443 | Moving Average Reward: 67 | Episode Reward: 92 | Loss: 4.005 | Steps: 93 | Worker: 1\n",
            "Episode: 444 | Moving Average Reward: 69 | Episode Reward: 228 | Loss: 2.816 | Steps: 229 | Worker: 0\n",
            "Episode: 445 | Moving Average Reward: 70 | Episode Reward: 177 | Loss: 2.7 | Steps: 178 | Worker: 1\n",
            "Episode: 446 | Moving Average Reward: 71 | Episode Reward: 171 | Loss: 2.569 | Steps: 172 | Worker: 0\n",
            "Episode: 447 | Moving Average Reward: 72 | Episode Reward: 152 | Loss: 2.935 | Steps: 153 | Worker: 1\n",
            "Episode: 448 | Moving Average Reward: 71 | Episode Reward: 17 | Loss: 5.028 | Steps: 18 | Worker: 1\n",
            "Episode: 449 | Moving Average Reward: 72 | Episode Reward: 132 | Loss: 3.703 | Steps: 133 | Worker: 0\n",
            "Episode: 450 | Moving Average Reward: 73 | Episode Reward: 179 | Loss: 3.097 | Steps: 180 | Worker: 0\n",
            "Episode: 451 | Moving Average Reward: 74 | Episode Reward: 216 | Loss: 3.049 | Steps: 217 | Worker: 1\n",
            "Episode: 452 | Moving Average Reward: 75 | Episode Reward: 155 | Loss: 3.006 | Steps: 156 | Worker: 0\n",
            "Episode: 453 | Moving Average Reward: 76 | Episode Reward: 204 | Loss: 3.427 | Steps: 205 | Worker: 1\n",
            "Episode: 454 | Moving Average Reward: 77 | Episode Reward: 199 | Loss: 2.874 | Steps: 200 | Worker: 0\n",
            "Episode: 455 | Moving Average Reward: 77 | Episode Reward: 22 | Loss: 13.496 | Steps: 23 | Worker: 0\n",
            "Episode: 456 | Moving Average Reward: 78 | Episode Reward: 151 | Loss: 4.238 | Steps: 152 | Worker: 1\n",
            "Episode: 457 | Moving Average Reward: 78 | Episode Reward: 93 | Loss: 3.837 | Steps: 94 | Worker: 0\n",
            "Episode: 458 | Moving Average Reward: 78 | Episode Reward: 59 | Loss: 4.145 | Steps: 60 | Worker: 0\n",
            "Episode: 459 | Moving Average Reward: 78 | Episode Reward: 132 | Loss: 3.664 | Steps: 133 | Worker: 1\n",
            "Episode: 460 | Moving Average Reward: 78 | Episode Reward: 104 | Loss: 5.664 | Steps: 105 | Worker: 0\n",
            "Episode: 461 | Moving Average Reward: 79 | Episode Reward: 139 | Loss: 3.432 | Steps: 140 | Worker: 1\n",
            "Episode: 462 | Moving Average Reward: 79 | Episode Reward: 134 | Loss: 3.665 | Steps: 135 | Worker: 1\n",
            "Episode: 463 | Moving Average Reward: 82 | Episode Reward: 335 | Loss: 2.619 | Steps: 336 | Worker: 0\n",
            "Episode: 464 | Moving Average Reward: 82 | Episode Reward: 97 | Loss: 3.019 | Steps: 98 | Worker: 0\n",
            "Episode: 465 | Moving Average Reward: 82 | Episode Reward: 82 | Loss: 5.236 | Steps: 83 | Worker: 0\n",
            "Episode: 466 | Moving Average Reward: 85 | Episode Reward: 358 | Loss: 2.593 | Steps: 359 | Worker: 1\n",
            "Episode: 467 | Moving Average Reward: 84 | Episode Reward: 30 | Loss: 7.558 | Steps: 31 | Worker: 1\n",
            "Episode: 468 | Moving Average Reward: 84 | Episode Reward: 54 | Loss: 4.278 | Steps: 55 | Worker: 1\n",
            "Episode: 469 | Moving Average Reward: 85 | Episode Reward: 222 | Loss: 4.506 | Steps: 223 | Worker: 0\n",
            "Episode: 470 | Moving Average Reward: 87 | Episode Reward: 207 | Loss: 2.977 | Steps: 208 | Worker: 1\n",
            "Episode: 471 | Moving Average Reward: 87 | Episode Reward: 117 | Loss: 3.071 | Steps: 118 | Worker: 0\n",
            "Episode: 472 | Moving Average Reward: 89 | Episode Reward: 251 | Loss: 2.679 | Steps: 252 | Worker: 1\n",
            "Episode: 473 | Moving Average Reward: 91 | Episode Reward: 380 | Loss: 2.944 | Steps: 381 | Worker: 0\n",
            "Episode: 474 | Moving Average Reward: 93 | Episode Reward: 278 | Loss: 2.862 | Steps: 279 | Worker: 0\n",
            "Episode: 475 | Moving Average Reward: 97 | Episode Reward: 417 | Loss: 2.43 | Steps: 418 | Worker: 1\n",
            "Episode: 476 | Moving Average Reward: 96 | Episode Reward: 54 | Loss: 4.673 | Steps: 55 | Worker: 1\n",
            "Episode: 477 | Moving Average Reward: 97 | Episode Reward: 181 | Loss: 4.18 | Steps: 182 | Worker: 0\n",
            "Episode: 478 | Moving Average Reward: 97 | Episode Reward: 117 | Loss: 3.703 | Steps: 118 | Worker: 0\n",
            "Episode: 479 | Moving Average Reward: 99 | Episode Reward: 304 | Loss: 3.78 | Steps: 305 | Worker: 1\n",
            "Episode: 480 | Moving Average Reward: 101 | Episode Reward: 230 | Loss: 3.341 | Steps: 231 | Worker: 0\n",
            "Episode: 481 | Moving Average Reward: 101 | Episode Reward: 189 | Loss: 4.585 | Steps: 190 | Worker: 1\n",
            "Episode: 482 | Moving Average Reward: 102 | Episode Reward: 180 | Loss: 6.17 | Steps: 181 | Worker: 0\n",
            "Episode: 483 | Moving Average Reward: 103 | Episode Reward: 153 | Loss: 4.499 | Steps: 154 | Worker: 1\n",
            "Episode: 484 | Moving Average Reward: 104 | Episode Reward: 233 | Loss: 3.399 | Steps: 234 | Worker: 1\n",
            "Episode: 485 | Moving Average Reward: 107 | Episode Reward: 370 | Loss: 3.131 | Steps: 371 | Worker: 0\n",
            "Episode: 486 | Moving Average Reward: 108 | Episode Reward: 227 | Loss: 2.907 | Steps: 228 | Worker: 1\n",
            "Episode: 487 | Moving Average Reward: 109 | Episode Reward: 271 | Loss: 3.265 | Steps: 272 | Worker: 0\n",
            "Episode: 488 | Moving Average Reward: 112 | Episode Reward: 318 | Loss: 3.082 | Steps: 319 | Worker: 1\n",
            "Episode: 489 | Moving Average Reward: 113 | Episode Reward: 257 | Loss: 2.873 | Steps: 258 | Worker: 0\n",
            "Episode: 490 | Moving Average Reward: 114 | Episode Reward: 218 | Loss: 3.218 | Steps: 219 | Worker: 1\n",
            "Episode: 491 | Moving Average Reward: 116 | Episode Reward: 312 | Loss: 3.299 | Steps: 313 | Worker: 0\n",
            "Episode: 492 | Moving Average Reward: 116 | Episode Reward: 142 | Loss: 4.429 | Steps: 143 | Worker: 0\n",
            "Episode: 493 | Moving Average Reward: 118 | Episode Reward: 287 | Loss: 3.783 | Steps: 288 | Worker: 1\n",
            "Episode: 494 | Moving Average Reward: 117 | Episode Reward: 32 | Loss: 10.08 | Steps: 33 | Worker: 0\n",
            "Episode: 495 | Moving Average Reward: 120 | Episode Reward: 389 | Loss: 3.204 | Steps: 390 | Worker: 1\n",
            "Episode: 496 | Moving Average Reward: 123 | Episode Reward: 459 | Loss: 2.235 | Steps: 460 | Worker: 0\n",
            "Saving best model to /tmp, episode score: 459.0\n",
            "Episode: 497 | Moving Average Reward: 124 | Episode Reward: 183 | Loss: 6.194 | Steps: 184 | Worker: 1\n",
            "Episode: 498 | Moving Average Reward: 125 | Episode Reward: 254 | Loss: 3.779 | Steps: 255 | Worker: 0\n",
            "Episode: 499 | Moving Average Reward: 126 | Episode Reward: 170 | Loss: 5.345 | Steps: 171 | Worker: 1\n",
            "Episode: 500 | Moving Average Reward: 125 | Episode Reward: 99 | Loss: 3.599 | Steps: 100 | Worker: 0\n",
            "Episode: 501 | Moving Average Reward: 126 | Episode Reward: 176 | Loss: 2.754 | Steps: 177 | Worker: 1\n",
            "Episode: 502 | Moving Average Reward: 126 | Episode Reward: 196 | Loss: 3.613 | Steps: 197 | Worker: 1\n",
            "Episode: 503 | Moving Average Reward: 130 | Episode Reward: 441 | Loss: 3.629 | Steps: 442 | Worker: 0\n",
            "Episode: 504 | Moving Average Reward: 131 | Episode Reward: 250 | Loss: 4.312 | Steps: 251 | Worker: 1\n",
            "Episode: 505 | Moving Average Reward: 132 | Episode Reward: 282 | Loss: 4.464 | Steps: 283 | Worker: 0\n",
            "Episode: 506 | Moving Average Reward: 133 | Episode Reward: 157 | Loss: 3.647 | Steps: 158 | Worker: 0\n",
            "Episode: 507 | Moving Average Reward: 135 | Episode Reward: 404 | Loss: 3.285 | Steps: 405 | Worker: 1\n",
            "Episode: 508 | Moving Average Reward: 136 | Episode Reward: 252 | Loss: 2.542 | Steps: 253 | Worker: 0\n",
            "Episode: 509 | Moving Average Reward: 137 | Episode Reward: 167 | Loss: 4.372 | Steps: 168 | Worker: 0\n",
            "Episode: 510 | Moving Average Reward: 140 | Episode Reward: 493 | Loss: 2.104 | Steps: 494 | Worker: 1\n",
            "Saving best model to /tmp, episode score: 493.0\n",
            "Episode: 511 | Moving Average Reward: 142 | Episode Reward: 317 | Loss: 2.283 | Steps: 318 | Worker: 0\n",
            "Episode: 512 | Moving Average Reward: 144 | Episode Reward: 305 | Loss: 2.639 | Steps: 306 | Worker: 1\n",
            "Episode: 513 | Moving Average Reward: 144 | Episode Reward: 208 | Loss: 2.966 | Steps: 209 | Worker: 0\n",
            "Episode: 514 | Moving Average Reward: 145 | Episode Reward: 198 | Loss: 3.324 | Steps: 199 | Worker: 0Episode: 514 | Moving Average Reward: 145 | Episode Reward: 242 | Loss: 3.639 | Steps: 243 | Worker: 1\n",
            "\n",
            "Episode: 516 | Moving Average Reward: 146 | Episode Reward: 243 | Loss: 3.134 | Steps: 244 | Worker: 0\n",
            "Episode: 517 | Moving Average Reward: 147 | Episode Reward: 249 | Loss: 2.85 | Steps: 250 | Worker: 1\n",
            "Episode: 518 | Moving Average Reward: 148 | Episode Reward: 233 | Loss: 2.74 | Steps: 234 | Worker: 1\n",
            "Episode: 519 | Moving Average Reward: 150 | Episode Reward: 339 | Loss: 2.06 | Steps: 340 | Worker: 0\n",
            "Episode: 520 | Moving Average Reward: 152 | Episode Reward: 312 | Loss: 2.396 | Steps: 313 | Worker: 1\n",
            "Episode: 521 | Moving Average Reward: 152 | Episode Reward: 194 | Loss: 2.797 | Steps: 195 | Worker: 0\n",
            "Episode: 522 | Moving Average Reward: 153 | Episode Reward: 227 | Loss: 3.063 | Steps: 228 | Worker: 1\n",
            "Episode: 523 | Moving Average Reward: 154 | Episode Reward: 236 | Loss: 2.514 | Steps: 237 | Worker: 1\n",
            "Episode: 524 | Moving Average Reward: 154 | Episode Reward: 162 | Loss: 3.735 | Steps: 163 | Worker: 1\n",
            "Episode: 525 | Moving Average Reward: 159 | Episode Reward: 666 | Loss: 2.389 | Steps: 667 | Worker: 0\n",
            "Saving best model to /tmp, episode score: 666.0\n",
            "Episode: 526 | Moving Average Reward: 159 | Episode Reward: 160 | Loss: 3.944 | Steps: 161 | Worker: 0\n",
            "Episode: 527 | Moving Average Reward: 160 | Episode Reward: 224 | Loss: 3.542 | Steps: 225 | Worker: 1\n",
            "Episode: 528 | Moving Average Reward: 160 | Episode Reward: 204 | Loss: 3.557 | Steps: 205 | Worker: 1\n",
            "Episode: 529 | Moving Average Reward: 161 | Episode Reward: 263 | Loss: 3.107 | Steps: 264 | Worker: 0\n",
            "Episode: 530 | Moving Average Reward: 161 | Episode Reward: 203 | Loss: 5.456 | Steps: 204 | Worker: 1\n",
            "Episode: 531 | Moving Average Reward: 163 | Episode Reward: 329 | Loss: 4.443 | Steps: 330 | Worker: 0\n",
            "Episode: 532 | Moving Average Reward: 163 | Episode Reward: 180 | Loss: 7.069 | Steps: 181 | Worker: 1\n",
            "Episode: 533 | Moving Average Reward: 163 | Episode Reward: 106 | Loss: 10.31 | Steps: 107 | Worker: 0\n",
            "Episode: 534 | Moving Average Reward: 164 | Episode Reward: 291 | Loss: 4.825 | Steps: 292 | Worker: 1\n",
            "Episode: 535 | Moving Average Reward: 166 | Episode Reward: 373 | Loss: 3.778 | Steps: 374 | Worker: 0\n",
            "Episode: 536 | Moving Average Reward: 167 | Episode Reward: 227 | Loss: 6.383 | Steps: 228 | Worker: 1\n",
            "Episode: 537 | Moving Average Reward: 167 | Episode Reward: 205 | Loss: 6.662 | Steps: 206 | Worker: 0\n",
            "Episode: 538 | Moving Average Reward: 167 | Episode Reward: 173 | Loss: 7.006 | Steps: 174 | Worker: 1\n",
            "Episode: 539 | Moving Average Reward: 167 | Episode Reward: 204 | Loss: 6.634 | Steps: 205 | Worker: 0\n",
            "Episode: 540 | Moving Average Reward: 168 | Episode Reward: 229 | Loss: 5.767 | Steps: 230 | Worker: 1\n",
            "Episode: 541 | Moving Average Reward: 168 | Episode Reward: 207 | Loss: 5.727 | Steps: 208 | Worker: 0\n",
            "Episode: 542 | Moving Average Reward: 169 | Episode Reward: 233 | Loss: 5.265 | Steps: 234 | Worker: 1\n",
            "Episode: 543 | Moving Average Reward: 169 | Episode Reward: 184 | Loss: 7.777 | Steps: 185 | Worker: 0\n",
            "Episode: 544 | Moving Average Reward: 169 | Episode Reward: 179 | Loss: 5.607 | Steps: 180 | Worker: 1\n",
            "Episode: 545 | Moving Average Reward: 169 | Episode Reward: 144 | Loss: 8.381 | Steps: 145 | Worker: 0\n",
            "Episode: 546 | Moving Average Reward: 169 | Episode Reward: 140 | Loss: 9.417 | Steps: 141 | Worker: 1\n",
            "Episode: 547 | Moving Average Reward: 168 | Episode Reward: 130 | Loss: 8.03 | Steps: 131 | Worker: 0\n",
            "Episode: 548 | Moving Average Reward: 168 | Episode Reward: 135 | Loss: 7.042 | Steps: 136 | Worker: 0\n",
            "Episode: 549 | Moving Average Reward: 169 | Episode Reward: 242 | Loss: 5.269 | Steps: 243 | Worker: 1\n",
            "Episode: 550 | Moving Average Reward: 170 | Episode Reward: 253 | Loss: 4.148 | Steps: 254 | Worker: 0\n",
            "Episode: 551 | Moving Average Reward: 170 | Episode Reward: 166 | Loss: 6.438 | Steps: 167 | Worker: 0\n",
            "Episode: 552 | Moving Average Reward: 172 | Episode Reward: 430 | Loss: 2.723 | Steps: 431 | Worker: 1\n",
            "Episode: 553 | Moving Average Reward: 172 | Episode Reward: 132 | Loss: 6.561 | Steps: 133 | Worker: 1\n",
            "Episode: 554 | Moving Average Reward: 172 | Episode Reward: 210 | Loss: 5.015 | Steps: 211 | Worker: 0\n",
            "Episode: 555 | Moving Average Reward: 172 | Episode Reward: 199 | Loss: 4.232 | Steps: 200 | Worker: 1\n",
            "Episode: 556 | Moving Average Reward: 173 | Episode Reward: 244 | Loss: 4.868 | Steps: 245 | Worker: 0\n",
            "Episode: 557 | Moving Average Reward: 173 | Episode Reward: 162 | Loss: 7.131 | Steps: 163 | Worker: 1\n",
            "Episode: 558 | Moving Average Reward: 173 | Episode Reward: 224 | Loss: 5.111 | Steps: 225 | Worker: 1\n",
            "Episode: 559 | Moving Average Reward: 177 | Episode Reward: 485 | Loss: 3.001 | Steps: 486 | Worker: 0\n",
            "Episode: 560 | Moving Average Reward: 179 | Episode Reward: 385 | Loss: 3.438 | Steps: 386 | Worker: 1\n",
            "Episode: 561 | Moving Average Reward: 180 | Episode Reward: 275 | Loss: 3.618 | Steps: 276 | Worker: 0\n",
            "Episode: 562 | Moving Average Reward: 180 | Episode Reward: 247 | Loss: 4.526 | Steps: 248 | Worker: 0\n",
            "Episode: 563 | Moving Average Reward: 182 | Episode Reward: 347 | Loss: 3.478 | Steps: 348 | Worker: 1\n",
            "Episode: 564 | Moving Average Reward: 180 | Episode Reward: 24 | Loss: 41.141 | Steps: 25 | Worker: 1\n",
            "Episode: 565 | Moving Average Reward: 181 | Episode Reward: 290 | Loss: 3.805 | Steps: 291 | Worker: 0\n",
            "Episode: 566 | Moving Average Reward: 182 | Episode Reward: 238 | Loss: 3.754 | Steps: 239 | Worker: 1\n",
            "Episode: 567 | Moving Average Reward: 183 | Episode Reward: 305 | Loss: 3.94 | Steps: 306 | Worker: 0\n",
            "Episode: 568 | Moving Average Reward: 184 | Episode Reward: 241 | Loss: 5.133 | Steps: 242 | Worker: 1\n",
            "Episode: 569 | Moving Average Reward: 186 | Episode Reward: 409 | Loss: 3.027 | Steps: 410 | Worker: 0\n",
            "Episode: 570 | Moving Average Reward: 189 | Episode Reward: 434 | Loss: 2.744 | Steps: 435 | Worker: 1\n",
            "Episode: 571 | Moving Average Reward: 190 | Episode Reward: 302 | Loss: 4.133 | Steps: 303 | Worker: 0\n",
            "Episode: 572 | Moving Average Reward: 193 | Episode Reward: 528 | Loss: 2.629 | Steps: 529 | Worker: 1\n",
            "Episode: 573 | Moving Average Reward: 195 | Episode Reward: 345 | Loss: 3.711 | Steps: 346 | Worker: 0\n",
            "Episode: 574 | Moving Average Reward: 196 | Episode Reward: 339 | Loss: 3.009 | Steps: 340 | Worker: 1\n",
            "Episode: 575 | Moving Average Reward: 197 | Episode Reward: 263 | Loss: 4.718 | Steps: 264 | Worker: 0\n",
            "Episode: 576 | Moving Average Reward: 197 | Episode Reward: 225 | Loss: 4.978 | Steps: 226 | Worker: 1\n",
            "Episode: 577 | Moving Average Reward: 197 | Episode Reward: 234 | Loss: 4.037 | Steps: 235 | Worker: 0\n",
            "Episode: 578 | Moving Average Reward: 197 | Episode Reward: 191 | Loss: 5.173 | Steps: 192 | Worker: 0\n",
            "Episode: 579 | Moving Average Reward: 198 | Episode Reward: 256 | Loss: 3.721 | Steps: 257 | Worker: 1\n",
            "Episode: 580 | Moving Average Reward: 197 | Episode Reward: 126 | Loss: 7.812 | Steps: 127 | Worker: 1\n",
            "Episode: 581 | Moving Average Reward: 197 | Episode Reward: 185 | Loss: 6.751 | Steps: 186 | Worker: 0\n",
            "Episode: 582 | Moving Average Reward: 196 | Episode Reward: 111 | Loss: 7.585 | Steps: 112 | Worker: 1\n",
            "Episode: 583 | Moving Average Reward: 195 | Episode Reward: 131 | Loss: 6.543 | Steps: 132 | Worker: 1\n",
            "Episode: 584 | Moving Average Reward: 198 | Episode Reward: 434 | Loss: 2.528 | Steps: 435 | Worker: 0\n",
            "Episode: 585 | Moving Average Reward: 198 | Episode Reward: 221 | Loss: 5.082 | Steps: 222 | Worker: 1\n",
            "Episode: 586 | Moving Average Reward: 198 | Episode Reward: 214 | Loss: 3.966 | Steps: 215 | Worker: 0\n",
            "Episode: 587 | Moving Average Reward: 198 | Episode Reward: 167 | Loss: 5.571 | Steps: 168 | Worker: 1\n",
            "Episode: 588 | Moving Average Reward: 198 | Episode Reward: 181 | Loss: 5.564 | Steps: 182 | Worker: 1\n",
            "Episode: 589 | Moving Average Reward: 198 | Episode Reward: 208 | Loss: 4.35 | Steps: 209 | Worker: 0\n",
            "Episode: 590 | Moving Average Reward: 198 | Episode Reward: 227 | Loss: 4.113 | Steps: 228 | Worker: 1\n",
            "Episode: 591 | Moving Average Reward: 198 | Episode Reward: 159 | Loss: 4.297 | Steps: 160 | Worker: 0\n",
            "Episode: 592 | Moving Average Reward: 197 | Episode Reward: 150 | Loss: 5.109 | Steps: 151 | Worker: 1\n",
            "Episode: 593 | Moving Average Reward: 197 | Episode Reward: 137 | Loss: 4.98 | Steps: 138 | Worker: 0\n",
            "Episode: 594 | Moving Average Reward: 195 | Episode Reward: 83 | Loss: 10.688 | Steps: 84 | Worker: 1\n",
            "Episode: 595 | Moving Average Reward: 195 | Episode Reward: 100 | Loss: 7.723 | Steps: 101 | Worker: 1\n",
            "Episode: 596 | Moving Average Reward: 194 | Episode Reward: 180 | Loss: 5.135 | Steps: 181 | Worker: 0\n",
            "Episode: 597 | Moving Average Reward: 193 | Episode Reward: 88 | Loss: 9.899 | Steps: 89 | Worker: 0\n",
            "Episode: 598 | Moving Average Reward: 192 | Episode Reward: 102 | Loss: 8.381 | Steps: 103 | Worker: 1\n",
            "Episode: 599 | Moving Average Reward: 192 | Episode Reward: 112 | Loss: 7.029 | Steps: 113 | Worker: 1\n",
            "Episode: 600 | Moving Average Reward: 191 | Episode Reward: 132 | Loss: 5.142 | Steps: 133 | Worker: 0\n",
            "Episode: 601 | Moving Average Reward: 189 | Episode Reward: 42 | Loss: 24.867 | Steps: 43 | Worker: 1\n",
            "Episode: 602 | Moving Average Reward: 189 | Episode Reward: 125 | Loss: 5.151 | Steps: 126 | Worker: 1\n",
            "Episode: 603 | Moving Average Reward: 189 | Episode Reward: 181 | Loss: 5.25 | Steps: 182 | Worker: 0\n",
            "Episode: 604 | Moving Average Reward: 188 | Episode Reward: 140 | Loss: 4.905 | Steps: 141 | Worker: 0Episode: 604 | Moving Average Reward: 188 | Episode Reward: 131 | Loss: 4.447 | Steps: 132 | Worker: 1\n",
            "\n",
            "Episode: 606 | Moving Average Reward: 188 | Episode Reward: 127 | Loss: 4.341 | Steps: 128 | Worker: 1\n",
            "Episode: 607 | Moving Average Reward: 187 | Episode Reward: 145 | Loss: 4.058 | Steps: 146 | Worker: 0\n",
            "Episode: 608 | Moving Average Reward: 187 | Episode Reward: 134 | Loss: 3.363 | Steps: 135 | Worker: 0\n",
            "Episode: 609 | Moving Average Reward: 186 | Episode Reward: 152 | Loss: 3.187 | Steps: 153 | Worker: 1\n",
            "Episode: 610 | Moving Average Reward: 185 | Episode Reward: 75 | Loss: 10.575 | Steps: 76 | Worker: 1\n",
            "Episode: 611 | Moving Average Reward: 185 | Episode Reward: 137 | Loss: 3.033 | Steps: 138 | Worker: 0\n",
            "Episode: 612 | Moving Average Reward: 184 | Episode Reward: 141 | Loss: 5.185 | Steps: 142 | Worker: 1\n",
            "Episode: 613 | Moving Average Reward: 184 | Episode Reward: 137 | Loss: 2.983 | Steps: 138 | Worker: 0\n",
            "Episode: 614 | Moving Average Reward: 183 | Episode Reward: 111 | Loss: 7.077 | Steps: 112 | Worker: 1\n",
            "Episode: 615 | Moving Average Reward: 182 | Episode Reward: 125 | Loss: 4.191 | Steps: 126 | Worker: 0\n",
            "Episode: 616 | Moving Average Reward: 182 | Episode Reward: 138 | Loss: 2.67 | Steps: 139 | Worker: 0\n",
            "Episode: 617 | Moving Average Reward: 183 | Episode Reward: 240 | Loss: 2.749 | Steps: 241 | Worker: 1\n",
            "Episode: 618 | Moving Average Reward: 182 | Episode Reward: 117 | Loss: 2.823 | Steps: 118 | Worker: 0\n",
            "Episode: 619 | Moving Average Reward: 182 | Episode Reward: 155 | Loss: 3.534 | Steps: 156 | Worker: 1\n",
            "Episode: 620 | Moving Average Reward: 181 | Episode Reward: 114 | Loss: 3.649 | Steps: 115 | Worker: 0\n",
            "Episode: 621 | Moving Average Reward: 180 | Episode Reward: 127 | Loss: 3.244 | Steps: 128 | Worker: 1\n",
            "Episode: 622 | Moving Average Reward: 180 | Episode Reward: 99 | Loss: 5.755 | Steps: 100 | Worker: 0\n",
            "Episode: 623 | Moving Average Reward: 179 | Episode Reward: 144 | Loss: 3.543 | Steps: 145 | Worker: 1\n",
            "Episode: 624 | Moving Average Reward: 179 | Episode Reward: 176 | Loss: 2.413 | Steps: 177 | Worker: 0\n",
            "Episode: 625 | Moving Average Reward: 179 | Episode Reward: 127 | Loss: 3.068 | Steps: 128 | Worker: 1\n",
            "Episode: 626 | Moving Average Reward: 178 | Episode Reward: 133 | Loss: 2.604 | Steps: 134 | Worker: 0\n",
            "Episode: 627 | Moving Average Reward: 178 | Episode Reward: 112 | Loss: 2.965 | Steps: 113 | Worker: 1\n",
            "Episode: 628 | Moving Average Reward: 177 | Episode Reward: 131 | Loss: 3.125 | Steps: 132 | Worker: 0\n",
            "Episode: 629 | Moving Average Reward: 176 | Episode Reward: 88 | Loss: 8.659 | Steps: 89 | Worker: 1\n",
            "Episode: 630 | Moving Average Reward: 176 | Episode Reward: 171 | Loss: 2.242 | Steps: 172 | Worker: 0\n",
            "Episode: 631 | Moving Average Reward: 176 | Episode Reward: 157 | Loss: 2.081 | Steps: 158 | Worker: 1\n",
            "Episode: 632 | Moving Average Reward: 176 | Episode Reward: 158 | Loss: 1.913 | Steps: 159 | Worker: 0\n",
            "Episode: 633 | Moving Average Reward: 175 | Episode Reward: 141 | Loss: 4.203 | Steps: 142 | Worker: 1\n",
            "Episode: 634 | Moving Average Reward: 175 | Episode Reward: 173 | Loss: 1.988 | Steps: 174 | Worker: 0\n",
            "Episode: 635 | Moving Average Reward: 176 | Episode Reward: 192 | Loss: 1.857 | Steps: 193 | Worker: 1\n",
            "Episode: 636 | Moving Average Reward: 175 | Episode Reward: 164 | Loss: 2.564 | Steps: 165 | Worker: 0\n",
            "Episode: 637 | Moving Average Reward: 175 | Episode Reward: 159 | Loss: 1.821 | Steps: 160 | Worker: 1\n",
            "Episode: 638 | Moving Average Reward: 175 | Episode Reward: 178 | Loss: 1.794 | Steps: 179 | Worker: 0\n",
            "Episode: 639 | Moving Average Reward: 175 | Episode Reward: 189 | Loss: 2.012 | Steps: 190 | Worker: 1\n",
            "Episode: 640 | Moving Average Reward: 175 | Episode Reward: 177 | Loss: 1.994 | Steps: 178 | Worker: 0\n",
            "Episode: 641 | Moving Average Reward: 175 | Episode Reward: 134 | Loss: 2.292 | Steps: 135 | Worker: 1\n",
            "Episode: 642 | Moving Average Reward: 176 | Episode Reward: 227 | Loss: 1.908 | Steps: 228 | Worker: 1\n",
            "Episode: 643 | Moving Average Reward: 175 | Episode Reward: 151 | Loss: 1.95 | Steps: 152 | Worker: 1\n",
            "Episode: 644 | Moving Average Reward: 178 | Episode Reward: 419 | Loss: 4.318 | Steps: 420 | Worker: 0\n",
            "Episode: 645 | Moving Average Reward: 177 | Episode Reward: 115 | Loss: 2.242 | Steps: 116 | Worker: 0\n",
            "Episode: 646 | Moving Average Reward: 177 | Episode Reward: 157 | Loss: 2.034 | Steps: 158 | Worker: 1\n",
            "Episode: 647 | Moving Average Reward: 176 | Episode Reward: 126 | Loss: 2.571 | Steps: 127 | Worker: 0\n",
            "Episode: 648 | Moving Average Reward: 176 | Episode Reward: 129 | Loss: 2.54 | Steps: 130 | Worker: 1\n",
            "Episode: 649 | Moving Average Reward: 176 | Episode Reward: 181 | Loss: 11.985 | Steps: 182 | Worker: 0\n",
            "Episode: 650 | Moving Average Reward: 175 | Episode Reward: 115 | Loss: 2.02 | Steps: 116 | Worker: 0\n",
            "Episode: 651 | Moving Average Reward: 176 | Episode Reward: 239 | Loss: 1.492 | Steps: 240 | Worker: 0\n",
            "Episode: 652 | Moving Average Reward: 175 | Episode Reward: 121 | Loss: 2.714 | Steps: 122 | Worker: 0\n",
            "Episode: 653 | Moving Average Reward: 175 | Episode Reward: 144 | Loss: 2.388 | Steps: 145 | Worker: 0\n",
            "Episode: 654 | Moving Average Reward: 181 | Episode Reward: 805 | Loss: 1.256 | Steps: 806 | Worker: 1\n",
            "Saving best model to /tmp, episode score: 805.0\n",
            "Episode: 655 | Moving Average Reward: 181 | Episode Reward: 122 | Loss: 3.14 | Steps: 123 | Worker: 0\n",
            "Episode: 656 | Moving Average Reward: 180 | Episode Reward: 136 | Loss: 1.64 | Steps: 137 | Worker: 1\n",
            "Episode: 657 | Moving Average Reward: 180 | Episode Reward: 150 | Loss: 3.23 | Steps: 151 | Worker: 0\n",
            "Episode: 658 | Moving Average Reward: 179 | Episode Reward: 128 | Loss: 2.035 | Steps: 129 | Worker: 1\n",
            "Episode: 659 | Moving Average Reward: 179 | Episode Reward: 137 | Loss: 2.631 | Steps: 138 | Worker: 1\n",
            "Episode: 660 | Moving Average Reward: 180 | Episode Reward: 254 | Loss: 1.437 | Steps: 255 | Worker: 0\n",
            "Episode: 661 | Moving Average Reward: 179 | Episode Reward: 129 | Loss: 2.123 | Steps: 130 | Worker: 1\n",
            "Episode: 662 | Moving Average Reward: 179 | Episode Reward: 152 | Loss: 1.613 | Steps: 153 | Worker: 0\n",
            "Episode: 663 | Moving Average Reward: 179 | Episode Reward: 176 | Loss: 1.357 | Steps: 177 | Worker: 1\n",
            "Episode: 664 | Moving Average Reward: 179 | Episode Reward: 156 | Loss: 1.582 | Steps: 157 | Worker: 0\n",
            "Episode: 665 | Moving Average Reward: 178 | Episode Reward: 110 | Loss: 5.202 | Steps: 111 | Worker: 1\n",
            "Episode: 666 | Moving Average Reward: 178 | Episode Reward: 127 | Loss: 1.868 | Steps: 128 | Worker: 0Episode: 666 | Moving Average Reward: 178 | Episode Reward: 126 | Loss: 1.943 | Steps: 127 | Worker: 1\n",
            "\n",
            "Episode: 668 | Moving Average Reward: 177 | Episode Reward: 98 | Loss: 1.531 | Steps: 99 | Worker: 0\n",
            "Episode: 669 | Moving Average Reward: 176 | Episode Reward: 119 | Loss: 1.399 | Steps: 120 | Worker: 1\n",
            "Episode: 670 | Moving Average Reward: 176 | Episode Reward: 144 | Loss: 1.659 | Steps: 145 | Worker: 0\n",
            "Episode: 671 | Moving Average Reward: 175 | Episode Reward: 146 | Loss: 1.863 | Steps: 147 | Worker: 1\n",
            "Episode: 672 | Moving Average Reward: 175 | Episode Reward: 145 | Loss: 1.69 | Steps: 146 | Worker: 0\n",
            "Episode: 673 | Moving Average Reward: 175 | Episode Reward: 181 | Loss: 1.671 | Steps: 182 | Worker: 1\n",
            "Episode: 674 | Moving Average Reward: 175 | Episode Reward: 118 | Loss: 2.3 | Steps: 119 | Worker: 0\n",
            "Episode: 675 | Moving Average Reward: 175 | Episode Reward: 181 | Loss: 1.641 | Steps: 182 | Worker: 1\n",
            "Episode: 676 | Moving Average Reward: 175 | Episode Reward: 168 | Loss: 1.82 | Steps: 169 | Worker: 0\n",
            "Episode: 677 | Moving Average Reward: 174 | Episode Reward: 128 | Loss: 1.611 | Steps: 129 | Worker: 1\n",
            "Episode: 678 | Moving Average Reward: 174 | Episode Reward: 167 | Loss: 1.813 | Steps: 168 | Worker: 0\n",
            "Episode: 679 | Moving Average Reward: 174 | Episode Reward: 117 | Loss: 1.416 | Steps: 118 | Worker: 1\n",
            "Episode: 680 | Moving Average Reward: 173 | Episode Reward: 141 | Loss: 1.928 | Steps: 142 | Worker: 0\n",
            "Episode: 681 | Moving Average Reward: 173 | Episode Reward: 175 | Loss: 1.32 | Steps: 176 | Worker: 1\n",
            "Episode: 682 | Moving Average Reward: 173 | Episode Reward: 130 | Loss: 1.603 | Steps: 131 | Worker: 0\n",
            "Episode: 683 | Moving Average Reward: 172 | Episode Reward: 128 | Loss: 1.858 | Steps: 129 | Worker: 0\n",
            "Episode: 684 | Moving Average Reward: 173 | Episode Reward: 244 | Loss: 1.408 | Steps: 245 | Worker: 1\n",
            "Episode: 685 | Moving Average Reward: 173 | Episode Reward: 167 | Loss: 1.93 | Steps: 168 | Worker: 0\n",
            "Episode: 686 | Moving Average Reward: 173 | Episode Reward: 156 | Loss: 1.185 | Steps: 157 | Worker: 1\n",
            "Episode: 687 | Moving Average Reward: 173 | Episode Reward: 188 | Loss: 1.429 | Steps: 189 | Worker: 0\n",
            "Episode: 688 | Moving Average Reward: 173 | Episode Reward: 138 | Loss: 1.142 | Steps: 139 | Worker: 1\n",
            "Episode: 689 | Moving Average Reward: 173 | Episode Reward: 209 | Loss: 1.376 | Steps: 210 | Worker: 0\n",
            "Episode: 690 | Moving Average Reward: 173 | Episode Reward: 224 | Loss: 1.407 | Steps: 225 | Worker: 1\n",
            "Episode: 691 | Moving Average Reward: 173 | Episode Reward: 167 | Loss: 1.688 | Steps: 168 | Worker: 1\n",
            "Episode: 692 | Moving Average Reward: 174 | Episode Reward: 199 | Loss: 1.332 | Steps: 200 | Worker: 1\n",
            "Episode: 693 | Moving Average Reward: 177 | Episode Reward: 503 | Loss: 1.102 | Steps: 504 | Worker: 0\n",
            "Episode: 694 | Moving Average Reward: 177 | Episode Reward: 219 | Loss: 1.408 | Steps: 220 | Worker: 1\n",
            "Episode: 695 | Moving Average Reward: 177 | Episode Reward: 187 | Loss: 1.354 | Steps: 188 | Worker: 0\n",
            "Episode: 696 | Moving Average Reward: 177 | Episode Reward: 184 | Loss: 1.706 | Steps: 185 | Worker: 1\n",
            "Episode: 697 | Moving Average Reward: 177 | Episode Reward: 174 | Loss: 1.297 | Steps: 175 | Worker: 1\n",
            "Episode: 698 | Moving Average Reward: 178 | Episode Reward: 257 | Loss: 1.023 | Steps: 258 | Worker: 0\n",
            "Episode: 699 | Moving Average Reward: 178 | Episode Reward: 127 | Loss: 1.641 | Steps: 128 | Worker: 0\n",
            "Episode: 700 | Moving Average Reward: 177 | Episode Reward: 137 | Loss: 1.114 | Steps: 138 | Worker: 0\n",
            "Episode: 701 | Moving Average Reward: 179 | Episode Reward: 393 | Loss: 1.012 | Steps: 394 | Worker: 1\n",
            "Episode: 702 | Moving Average Reward: 180 | Episode Reward: 281 | Loss: 1.366 | Steps: 282 | Worker: 0\n",
            "Episode: 703 | Moving Average Reward: 181 | Episode Reward: 190 | Loss: 1.216 | Steps: 191 | Worker: 1\n",
            "Episode: 704 | Moving Average Reward: 181 | Episode Reward: 265 | Loss: 1.326 | Steps: 266 | Worker: 1\n",
            "Episode: 705 | Moving Average Reward: 183 | Episode Reward: 320 | Loss: 1.063 | Steps: 321 | Worker: 0\n",
            "Episode: 706 | Moving Average Reward: 182 | Episode Reward: 122 | Loss: 1.655 | Steps: 123 | Worker: 1\n",
            "Episode: 707 | Moving Average Reward: 182 | Episode Reward: 197 | Loss: 0.996 | Steps: 198 | Worker: 0\n",
            "Episode: 708 | Moving Average Reward: 182 | Episode Reward: 163 | Loss: 1.389 | Steps: 164 | Worker: 1\n",
            "Episode: 709 | Moving Average Reward: 181 | Episode Reward: 118 | Loss: 1.015 | Steps: 119 | Worker: 1\n",
            "Episode: 710 | Moving Average Reward: 182 | Episode Reward: 275 | Loss: 0.983 | Steps: 276 | Worker: 0\n",
            "Episode: 711 | Moving Average Reward: 182 | Episode Reward: 157 | Loss: 1.063 | Steps: 158 | Worker: 1\n",
            "Episode: 712 | Moving Average Reward: 181 | Episode Reward: 43 | Loss: 35.583 | Steps: 44 | Worker: 1\n",
            "Episode: 713 | Moving Average Reward: 181 | Episode Reward: 159 | Loss: 1.063 | Steps: 160 | Worker: 0\n",
            "Episode: 714 | Moving Average Reward: 180 | Episode Reward: 162 | Loss: 1.282 | Steps: 163 | Worker: 1\n",
            "Episode: 715 | Moving Average Reward: 180 | Episode Reward: 178 | Loss: 0.973 | Steps: 179 | Worker: 0\n",
            "Episode: 716 | Moving Average Reward: 182 | Episode Reward: 375 | Loss: 0.89 | Steps: 376 | Worker: 0\n",
            "Episode: 717 | Moving Average Reward: 184 | Episode Reward: 366 | Loss: 5.824 | Steps: 367 | Worker: 1\n",
            "Episode: 718 | Moving Average Reward: 185 | Episode Reward: 297 | Loss: 0.861 | Steps: 298 | Worker: 0\n",
            "Episode: 719 | Moving Average Reward: 187 | Episode Reward: 333 | Loss: 0.965 | Steps: 334 | Worker: 1\n",
            "Episode: 720 | Moving Average Reward: 186 | Episode Reward: 140 | Loss: 1.362 | Steps: 141 | Worker: 0\n",
            "Episode: 721 | Moving Average Reward: 186 | Episode Reward: 121 | Loss: 1.316 | Steps: 122 | Worker: 1\n",
            "Episode: 722 | Moving Average Reward: 185 | Episode Reward: 154 | Loss: 0.918 | Steps: 155 | Worker: 0\n",
            "Episode: 723 | Moving Average Reward: 185 | Episode Reward: 134 | Loss: 0.901 | Steps: 135 | Worker: 1\n",
            "Episode: 724 | Moving Average Reward: 184 | Episode Reward: 156 | Loss: 1.127 | Steps: 157 | Worker: 0\n",
            "Episode: 725 | Moving Average Reward: 184 | Episode Reward: 163 | Loss: 1.231 | Steps: 164 | Worker: 1\n",
            "Episode: 726 | Moving Average Reward: 184 | Episode Reward: 140 | Loss: 1.187 | Steps: 141 | Worker: 1\n",
            "Episode: 727 | Moving Average Reward: 184 | Episode Reward: 180 | Loss: 1.559 | Steps: 181 | Worker: 0\n",
            "Episode: 728 | Moving Average Reward: 183 | Episode Reward: 112 | Loss: 1.025 | Steps: 113 | Worker: 0\n",
            "Episode: 729 | Moving Average Reward: 183 | Episode Reward: 161 | Loss: 2.493 | Steps: 162 | Worker: 1\n",
            "Episode: 730 | Moving Average Reward: 182 | Episode Reward: 123 | Loss: 1.071 | Steps: 124 | Worker: 0\n",
            "Episode: 731 | Moving Average Reward: 182 | Episode Reward: 120 | Loss: 1.213 | Steps: 121 | Worker: 1\n",
            "Episode: 732 | Moving Average Reward: 182 | Episode Reward: 207 | Loss: 0.941 | Steps: 208 | Worker: 0\n",
            "Episode: 733 | Moving Average Reward: 181 | Episode Reward: 117 | Loss: 3.481 | Steps: 118 | Worker: 1\n",
            "Episode: 734 | Moving Average Reward: 180 | Episode Reward: 108 | Loss: 1.142 | Steps: 109 | Worker: 1\n",
            "Episode: 735 | Moving Average Reward: 180 | Episode Reward: 144 | Loss: 2.702 | Steps: 145 | Worker: 0\n",
            "Episode: 736 | Moving Average Reward: 179 | Episode Reward: 114 | Loss: 0.705 | Steps: 115 | Worker: 0\n",
            "Episode: 737 | Moving Average Reward: 179 | Episode Reward: 147 | Loss: 1.288 | Steps: 148 | Worker: 1\n",
            "Episode: 738 | Moving Average Reward: 178 | Episode Reward: 95 | Loss: 2.749 | Steps: 96 | Worker: 0\n",
            "Episode: 739 | Moving Average Reward: 178 | Episode Reward: 148 | Loss: 2.392 | Steps: 149 | Worker: 1\n",
            "Episode: 740 | Moving Average Reward: 178 | Episode Reward: 158 | Loss: 0.685 | Steps: 159 | Worker: 0\n",
            "Episode: 741 | Moving Average Reward: 177 | Episode Reward: 159 | Loss: 0.676 | Steps: 160 | Worker: 0\n",
            "Episode: 742 | Moving Average Reward: 178 | Episode Reward: 226 | Loss: 0.745 | Steps: 227 | Worker: 1\n",
            "Episode: 743 | Moving Average Reward: 177 | Episode Reward: 119 | Loss: 0.592 | Steps: 120 | Worker: 0\n",
            "Episode: 744 | Moving Average Reward: 177 | Episode Reward: 144 | Loss: 1.509 | Steps: 145 | Worker: 1\n",
            "Episode: 745 | Moving Average Reward: 177 | Episode Reward: 160 | Loss: 1.112 | Steps: 161 | Worker: 0\n",
            "Episode: 746 | Moving Average Reward: 179 | Episode Reward: 415 | Loss: 0.657 | Steps: 416 | Worker: 1\n",
            "Episode: 747 | Moving Average Reward: 180 | Episode Reward: 301 | Loss: 0.786 | Steps: 302 | Worker: 0\n",
            "Episode: 748 | Moving Average Reward: 180 | Episode Reward: 143 | Loss: 0.987 | Steps: 144 | Worker: 1\n",
            "Episode: 749 | Moving Average Reward: 180 | Episode Reward: 142 | Loss: 0.817 | Steps: 143 | Worker: 0\n",
            "Episode: 750 | Moving Average Reward: 179 | Episode Reward: 131 | Loss: 0.656 | Steps: 132 | Worker: 1\n",
            "Episode: 751 | Moving Average Reward: 179 | Episode Reward: 185 | Loss: 1.192 | Steps: 186 | Worker: 0\n",
            "Episode: 752 | Moving Average Reward: 180 | Episode Reward: 245 | Loss: 0.953 | Steps: 246 | Worker: 1\n",
            "Episode: 753 | Moving Average Reward: 181 | Episode Reward: 304 | Loss: 0.712 | Steps: 305 | Worker: 0\n",
            "Episode: 754 | Moving Average Reward: 181 | Episode Reward: 212 | Loss: 0.949 | Steps: 213 | Worker: 1\n",
            "Episode: 755 | Moving Average Reward: 184 | Episode Reward: 447 | Loss: 6.752 | Steps: 448 | Worker: 0\n",
            "Episode: 756 | Moving Average Reward: 185 | Episode Reward: 228 | Loss: 0.685 | Steps: 229 | Worker: 1\n",
            "Episode: 757 | Moving Average Reward: 184 | Episode Reward: 144 | Loss: 1.451 | Steps: 145 | Worker: 0\n",
            "Episode: 758 | Moving Average Reward: 184 | Episode Reward: 149 | Loss: 0.76 | Steps: 150 | Worker: 1\n",
            "Episode: 759 | Moving Average Reward: 185 | Episode Reward: 298 | Loss: 0.769 | Steps: 299 | Worker: 1\n",
            "Episode: 760 | Moving Average Reward: 186 | Episode Reward: 347 | Loss: 11.341 | Steps: 348 | Worker: 1\n",
            "Episode: 761 | Moving Average Reward: 193 | Episode Reward: 855 | Loss: 0.707 | Steps: 856 | Worker: 0\n",
            "Saving best model to /tmp, episode score: 855.0\n",
            "Episode: 762 | Moving Average Reward: 193 | Episode Reward: 202 | Loss: 0.91 | Steps: 203 | Worker: 1\n",
            "Episode: 763 | Moving Average Reward: 193 | Episode Reward: 189 | Loss: 1.46 | Steps: 190 | Worker: 0\n",
            "Episode: 764 | Moving Average Reward: 193 | Episode Reward: 180 | Loss: 1.179 | Steps: 181 | Worker: 1\n",
            "Episode: 765 | Moving Average Reward: 193 | Episode Reward: 213 | Loss: 0.752 | Steps: 214 | Worker: 0\n",
            "Episode: 766 | Moving Average Reward: 193 | Episode Reward: 161 | Loss: 1.112 | Steps: 162 | Worker: 0\n",
            "Episode: 767 | Moving Average Reward: 194 | Episode Reward: 303 | Loss: 1.154 | Steps: 304 | Worker: 1\n",
            "Episode: 768 | Moving Average Reward: 194 | Episode Reward: 192 | Loss: 0.876 | Steps: 193 | Worker: 0\n",
            "Episode: 769 | Moving Average Reward: 194 | Episode Reward: 174 | Loss: 0.947 | Steps: 175 | Worker: 0\n",
            "Episode: 770 | Moving Average Reward: 194 | Episode Reward: 210 | Loss: 0.97 | Steps: 211 | Worker: 1\n",
            "Episode: 771 | Moving Average Reward: 194 | Episode Reward: 172 | Loss: 0.904 | Steps: 173 | Worker: 0\n",
            "Episode: 772 | Moving Average Reward: 194 | Episode Reward: 197 | Loss: 1.326 | Steps: 198 | Worker: 1\n",
            "Episode: 773 | Moving Average Reward: 194 | Episode Reward: 181 | Loss: 1.014 | Steps: 182 | Worker: 0\n",
            "Episode: 774 | Moving Average Reward: 194 | Episode Reward: 243 | Loss: 0.967 | Steps: 244 | Worker: 1\n",
            "Episode: 775 | Moving Average Reward: 194 | Episode Reward: 156 | Loss: 0.759 | Steps: 157 | Worker: 0\n",
            "Episode: 776 | Moving Average Reward: 193 | Episode Reward: 167 | Loss: 0.73 | Steps: 168 | Worker: 1\n",
            "Episode: 777 | Moving Average Reward: 194 | Episode Reward: 232 | Loss: 0.683 | Steps: 233 | Worker: 0\n",
            "Episode: 778 | Moving Average Reward: 193 | Episode Reward: 148 | Loss: 0.727 | Steps: 149 | Worker: 0\n",
            "Episode: 779 | Moving Average Reward: 194 | Episode Reward: 296 | Loss: 13.565 | Steps: 297 | Worker: 1\n",
            "Episode: 780 | Moving Average Reward: 194 | Episode Reward: 183 | Loss: 0.841 | Steps: 184 | Worker: 0\n",
            "Episode: 781 | Moving Average Reward: 194 | Episode Reward: 161 | Loss: 2.275 | Steps: 162 | Worker: 1\n",
            "Episode: 782 | Moving Average Reward: 194 | Episode Reward: 166 | Loss: 0.753 | Steps: 167 | Worker: 0\n",
            "Episode: 783 | Moving Average Reward: 195 | Episode Reward: 315 | Loss: 0.593 | Steps: 316 | Worker: 1\n",
            "Episode: 784 | Moving Average Reward: 195 | Episode Reward: 195 | Loss: 0.777 | Steps: 196 | Worker: 0\n",
            "Episode: 785 | Moving Average Reward: 195 | Episode Reward: 170 | Loss: 0.807 | Steps: 171 | Worker: 0\n",
            "Episode: 786 | Moving Average Reward: 195 | Episode Reward: 203 | Loss: 1.747 | Steps: 204 | Worker: 1\n",
            "Episode: 787 | Moving Average Reward: 194 | Episode Reward: 169 | Loss: 1.11 | Steps: 170 | Worker: 0\n",
            "Episode: 788 | Moving Average Reward: 195 | Episode Reward: 250 | Loss: 0.732 | Steps: 251 | Worker: 1\n",
            "Episode: 789 | Moving Average Reward: 195 | Episode Reward: 231 | Loss: 0.87 | Steps: 232 | Worker: 0\n",
            "Episode: 790 | Moving Average Reward: 196 | Episode Reward: 284 | Loss: 0.915 | Steps: 285 | Worker: 0\n",
            "Episode: 791 | Moving Average Reward: 197 | Episode Reward: 261 | Loss: 0.829 | Steps: 262 | Worker: 0\n",
            "Episode: 792 | Moving Average Reward: 202 | Episode Reward: 739 | Loss: 0.514 | Steps: 740 | Worker: 1\n",
            "Episode: 793 | Moving Average Reward: 203 | Episode Reward: 249 | Loss: 0.786 | Steps: 250 | Worker: 0\n",
            "Episode: 794 | Moving Average Reward: 204 | Episode Reward: 300 | Loss: 1.09 | Steps: 301 | Worker: 1\n",
            "Episode: 795 | Moving Average Reward: 204 | Episode Reward: 239 | Loss: 0.602 | Steps: 240 | Worker: 0\n",
            "Episode: 796 | Moving Average Reward: 204 | Episode Reward: 168 | Loss: 0.78 | Steps: 169 | Worker: 1\n",
            "Episode: 797 | Moving Average Reward: 203 | Episode Reward: 144 | Loss: 0.704 | Steps: 145 | Worker: 0\n",
            "Episode: 798 | Moving Average Reward: 203 | Episode Reward: 146 | Loss: 0.649 | Steps: 147 | Worker: 0\n",
            "Episode: 799 | Moving Average Reward: 203 | Episode Reward: 270 | Loss: 0.588 | Steps: 271 | Worker: 1\n",
            "Episode: 800 | Moving Average Reward: 204 | Episode Reward: 240 | Loss: 0.667 | Steps: 241 | Worker: 1\n",
            "Episode: 801 | Moving Average Reward: 205 | Episode Reward: 373 | Loss: 0.505 | Steps: 374 | Worker: 0\n",
            "Episode: 802 | Moving Average Reward: 205 | Episode Reward: 211 | Loss: 1.422 | Steps: 212 | Worker: 1\n",
            "Episode: 803 | Moving Average Reward: 205 | Episode Reward: 153 | Loss: 0.704 | Steps: 154 | Worker: 0\n",
            "Episode: 804 | Moving Average Reward: 204 | Episode Reward: 164 | Loss: 0.66 | Steps: 165 | Worker: 1\n",
            "Episode: 805 | Moving Average Reward: 205 | Episode Reward: 244 | Loss: 0.589 | Steps: 245 | Worker: 1\n",
            "Episode: 806 | Moving Average Reward: 207 | Episode Reward: 402 | Loss: 0.628 | Steps: 403 | Worker: 0\n",
            "Episode: 807 | Moving Average Reward: 207 | Episode Reward: 205 | Loss: 1.308 | Steps: 206 | Worker: 1\n",
            "Episode: 808 | Moving Average Reward: 207 | Episode Reward: 246 | Loss: 0.659 | Steps: 247 | Worker: 0\n",
            "Episode: 809 | Moving Average Reward: 207 | Episode Reward: 177 | Loss: 0.732 | Steps: 178 | Worker: 1\n",
            "Episode: 810 | Moving Average Reward: 207 | Episode Reward: 194 | Loss: 0.619 | Steps: 195 | Worker: 0\n",
            "Episode: 811 | Moving Average Reward: 207 | Episode Reward: 196 | Loss: 0.663 | Steps: 197 | Worker: 1\n",
            "Episode: 812 | Moving Average Reward: 206 | Episode Reward: 148 | Loss: 0.929 | Steps: 149 | Worker: 0\n",
            "Episode: 813 | Moving Average Reward: 207 | Episode Reward: 298 | Loss: 0.687 | Steps: 299 | Worker: 0\n",
            "Episode: 814 | Moving Average Reward: 209 | Episode Reward: 380 | Loss: 0.709 | Steps: 381 | Worker: 1\n",
            "Episode: 815 | Moving Average Reward: 208 | Episode Reward: 184 | Loss: 0.825 | Steps: 185 | Worker: 1\n",
            "Episode: 816 | Moving Average Reward: 208 | Episode Reward: 212 | Loss: 14.654 | Steps: 213 | Worker: 0\n",
            "Episode: 817 | Moving Average Reward: 208 | Episode Reward: 162 | Loss: 0.663 | Steps: 163 | Worker: 0\n",
            "Episode: 818 | Moving Average Reward: 208 | Episode Reward: 238 | Loss: 0.681 | Steps: 239 | Worker: 1\n",
            "Episode: 819 | Moving Average Reward: 208 | Episode Reward: 162 | Loss: 0.645 | Steps: 163 | Worker: 0\n",
            "Episode: 820 | Moving Average Reward: 208 | Episode Reward: 249 | Loss: 0.611 | Steps: 250 | Worker: 1\n",
            "Episode: 821 | Moving Average Reward: 209 | Episode Reward: 291 | Loss: 0.633 | Steps: 292 | Worker: 1\n",
            "Episode: 822 | Moving Average Reward: 213 | Episode Reward: 580 | Loss: 0.529 | Steps: 581 | Worker: 0\n",
            "Episode: 823 | Moving Average Reward: 212 | Episode Reward: 163 | Loss: 1.298 | Steps: 164 | Worker: 1\n",
            "Episode: 824 | Moving Average Reward: 212 | Episode Reward: 157 | Loss: 0.478 | Steps: 158 | Worker: 1\n",
            "Episode: 825 | Moving Average Reward: 211 | Episode Reward: 180 | Loss: 0.653 | Steps: 181 | Worker: 0\n",
            "Episode: 826 | Moving Average Reward: 212 | Episode Reward: 270 | Loss: 0.508 | Steps: 271 | Worker: 1\n",
            "Episode: 827 | Moving Average Reward: 211 | Episode Reward: 81 | Loss: 17.204 | Steps: 82 | Worker: 1\n",
            "Episode: 828 | Moving Average Reward: 210 | Episode Reward: 158 | Loss: 22.36 | Steps: 159 | Worker: 1\n",
            "Episode: 829 | Moving Average Reward: 215 | Episode Reward: 665 | Loss: 8.142 | Steps: 666 | Worker: 0\n",
            "Episode: 830 | Moving Average Reward: 215 | Episode Reward: 275 | Loss: 0.464 | Steps: 276 | Worker: 1\n",
            "Episode: 831 | Moving Average Reward: 215 | Episode Reward: 157 | Loss: 0.704 | Steps: 158 | Worker: 0\n",
            "Episode: 832 | Moving Average Reward: 214 | Episode Reward: 167 | Loss: 0.647 | Steps: 168 | Worker: 1\n",
            "Episode: 833 | Moving Average Reward: 213 | Episode Reward: 146 | Loss: 1.331 | Steps: 147 | Worker: 0\n",
            "Episode: 834 | Moving Average Reward: 213 | Episode Reward: 182 | Loss: 0.751 | Steps: 183 | Worker: 0\n",
            "Episode: 835 | Moving Average Reward: 213 | Episode Reward: 169 | Loss: 0.676 | Steps: 170 | Worker: 1\n",
            "Episode: 836 | Moving Average Reward: 212 | Episode Reward: 132 | Loss: 0.81 | Steps: 133 | Worker: 1\n",
            "Episode: 837 | Moving Average Reward: 212 | Episode Reward: 233 | Loss: 0.629 | Steps: 234 | Worker: 0\n",
            "Episode: 838 | Moving Average Reward: 212 | Episode Reward: 255 | Loss: 0.604 | Steps: 256 | Worker: 1\n",
            "Episode: 839 | Moving Average Reward: 212 | Episode Reward: 197 | Loss: 0.47 | Steps: 198 | Worker: 0\n",
            "Episode: 840 | Moving Average Reward: 212 | Episode Reward: 191 | Loss: 0.909 | Steps: 192 | Worker: 1\n",
            "Episode: 841 | Moving Average Reward: 212 | Episode Reward: 243 | Loss: 0.573 | Steps: 244 | Worker: 0\n",
            "Episode: 842 | Moving Average Reward: 214 | Episode Reward: 384 | Loss: 0.692 | Steps: 385 | Worker: 1\n",
            "Episode: 843 | Moving Average Reward: 215 | Episode Reward: 303 | Loss: 12.97 | Steps: 304 | Worker: 0\n",
            "Episode: 844 | Moving Average Reward: 215 | Episode Reward: 173 | Loss: 0.543 | Steps: 174 | Worker: 0\n",
            "Episode: 845 | Moving Average Reward: 214 | Episode Reward: 152 | Loss: 0.576 | Steps: 153 | Worker: 1\n",
            "Episode: 846 | Moving Average Reward: 214 | Episode Reward: 203 | Loss: 0.646 | Steps: 204 | Worker: 1\n",
            "Episode: 847 | Moving Average Reward: 214 | Episode Reward: 221 | Loss: 0.768 | Steps: 222 | Worker: 0\n",
            "Episode: 848 | Moving Average Reward: 214 | Episode Reward: 228 | Loss: 0.604 | Steps: 229 | Worker: 1\n",
            "Episode: 849 | Moving Average Reward: 215 | Episode Reward: 271 | Loss: 0.506 | Steps: 272 | Worker: 0\n",
            "Episode: 850 | Moving Average Reward: 215 | Episode Reward: 217 | Loss: 0.423 | Steps: 218 | Worker: 0\n",
            "Episode: 851 | Moving Average Reward: 216 | Episode Reward: 329 | Loss: 0.524 | Steps: 330 | Worker: 1\n",
            "Episode: 852 | Moving Average Reward: 216 | Episode Reward: 207 | Loss: 0.613 | Steps: 208 | Worker: 0\n",
            "Episode: 853 | Moving Average Reward: 216 | Episode Reward: 238 | Loss: 0.44 | Steps: 239 | Worker: 1\n",
            "Episode: 854 | Moving Average Reward: 215 | Episode Reward: 160 | Loss: 0.621 | Steps: 161 | Worker: 0\n",
            "Episode: 855 | Moving Average Reward: 216 | Episode Reward: 312 | Loss: 0.404 | Steps: 313 | Worker: 1\n",
            "Episode: 856 | Moving Average Reward: 216 | Episode Reward: 238 | Loss: 0.718 | Steps: 239 | Worker: 0\n",
            "Episode: 857 | Moving Average Reward: 216 | Episode Reward: 185 | Loss: 0.617 | Steps: 186 | Worker: 1\n",
            "Episode: 858 | Moving Average Reward: 216 | Episode Reward: 246 | Loss: 0.621 | Steps: 247 | Worker: 0\n",
            "Episode: 859 | Moving Average Reward: 217 | Episode Reward: 274 | Loss: 0.531 | Steps: 275 | Worker: 1\n",
            "Episode: 860 | Moving Average Reward: 217 | Episode Reward: 212 | Loss: 0.614 | Steps: 213 | Worker: 0\n",
            "Episode: 861 | Moving Average Reward: 216 | Episode Reward: 160 | Loss: 0.904 | Steps: 161 | Worker: 1\n",
            "Episode: 862 | Moving Average Reward: 216 | Episode Reward: 209 | Loss: 0.693 | Steps: 210 | Worker: 0\n",
            "Episode: 863 | Moving Average Reward: 217 | Episode Reward: 238 | Loss: 0.399 | Steps: 239 | Worker: 1\n",
            "Episode: 864 | Moving Average Reward: 216 | Episode Reward: 173 | Loss: 0.694 | Steps: 174 | Worker: 0\n",
            "Episode: 865 | Moving Average Reward: 215 | Episode Reward: 130 | Loss: 0.811 | Steps: 131 | Worker: 0\n",
            "Episode: 866 | Moving Average Reward: 215 | Episode Reward: 212 | Loss: 0.648 | Steps: 213 | Worker: 1\n",
            "Episode: 867 | Moving Average Reward: 215 | Episode Reward: 211 | Loss: 0.605 | Steps: 212 | Worker: 0\n",
            "Episode: 868 | Moving Average Reward: 216 | Episode Reward: 322 | Loss: 0.524 | Steps: 323 | Worker: 1\n",
            "Episode: 869 | Moving Average Reward: 216 | Episode Reward: 178 | Loss: 0.777 | Steps: 179 | Worker: 0\n",
            "Episode: 870 | Moving Average Reward: 216 | Episode Reward: 219 | Loss: 0.569 | Steps: 220 | Worker: 0\n",
            "Episode: 871 | Moving Average Reward: 217 | Episode Reward: 346 | Loss: 0.561 | Steps: 347 | Worker: 0\n",
            "Episode: 872 | Moving Average Reward: 217 | Episode Reward: 229 | Loss: 0.563 | Steps: 230 | Worker: 0\n",
            "Episode: 873 | Moving Average Reward: 217 | Episode Reward: 201 | Loss: 0.529 | Steps: 202 | Worker: 0\n",
            "Episode: 874 | Moving Average Reward: 225 | Episode Reward: 1007 | Loss: 0.399 | Steps: 1008 | Worker: 1\n",
            "Saving best model to /tmp, episode score: 1007.0\n",
            "Episode: 875 | Moving Average Reward: 226 | Episode Reward: 309 | Loss: 0.481 | Steps: 310 | Worker: 1\n",
            "Episode: 876 | Moving Average Reward: 229 | Episode Reward: 527 | Loss: 1.014 | Steps: 528 | Worker: 0\n",
            "Episode: 877 | Moving Average Reward: 229 | Episode Reward: 294 | Loss: 0.582 | Steps: 295 | Worker: 1\n",
            "Episode: 878 | Moving Average Reward: 229 | Episode Reward: 188 | Loss: 1.02 | Steps: 189 | Worker: 0\n",
            "Episode: 879 | Moving Average Reward: 229 | Episode Reward: 209 | Loss: 0.976 | Steps: 210 | Worker: 1\n",
            "Episode: 880 | Moving Average Reward: 229 | Episode Reward: 199 | Loss: 0.625 | Steps: 200 | Worker: 0\n",
            "Episode: 881 | Moving Average Reward: 231 | Episode Reward: 455 | Loss: 0.491 | Steps: 456 | Worker: 0\n",
            "Episode: 882 | Moving Average Reward: 235 | Episode Reward: 686 | Loss: 6.135 | Steps: 687 | Worker: 1\n",
            "Episode: 883 | Moving Average Reward: 236 | Episode Reward: 258 | Loss: 0.442 | Steps: 259 | Worker: 0\n",
            "Episode: 884 | Moving Average Reward: 235 | Episode Reward: 193 | Loss: 0.461 | Steps: 194 | Worker: 0\n",
            "Episode: 885 | Moving Average Reward: 237 | Episode Reward: 397 | Loss: 5.912 | Steps: 398 | Worker: 1\n",
            "Episode: 886 | Moving Average Reward: 237 | Episode Reward: 247 | Loss: 0.662 | Steps: 248 | Worker: 0\n",
            "Episode: 887 | Moving Average Reward: 238 | Episode Reward: 311 | Loss: 20.23 | Steps: 312 | Worker: 0\n",
            "Episode: 888 | Moving Average Reward: 241 | Episode Reward: 573 | Loss: 9.235 | Steps: 574 | Worker: 1\n",
            "Episode: 889 | Moving Average Reward: 243 | Episode Reward: 444 | Loss: 0.729 | Steps: 445 | Worker: 1\n",
            "Episode: 890 | Moving Average Reward: 250 | Episode Reward: 992 | Loss: 0.423 | Steps: 993 | Worker: 0\n",
            "Episode: 891 | Moving Average Reward: 254 | Episode Reward: 582 | Loss: 0.48 | Steps: 583 | Worker: 1\n",
            "Episode: 892 | Moving Average Reward: 254 | Episode Reward: 231 | Loss: 13.599 | Steps: 232 | Worker: 1\n",
            "Episode: 893 | Moving Average Reward: 255 | Episode Reward: 398 | Loss: 0.528 | Steps: 399 | Worker: 0\n",
            "Episode: 894 | Moving Average Reward: 254 | Episode Reward: 189 | Loss: 0.484 | Steps: 190 | Worker: 0\n",
            "Episode: 895 | Moving Average Reward: 253 | Episode Reward: 142 | Loss: 16.013 | Steps: 143 | Worker: 0\n",
            "Episode: 896 | Moving Average Reward: 255 | Episode Reward: 416 | Loss: 8.043 | Steps: 417 | Worker: 0\n",
            "Episode: 897 | Moving Average Reward: 264 | Episode Reward: 1200 | Loss: 4.721 | Steps: 1201 | Worker: 1\n",
            "Saving best model to /tmp, episode score: 1200.0\n",
            "Episode: 898 | Moving Average Reward: 266 | Episode Reward: 469 | Loss: 10.969 | Steps: 470 | Worker: 0\n",
            "Episode: 899 | Moving Average Reward: 266 | Episode Reward: 225 | Loss: 23.428 | Steps: 226 | Worker: 1\n",
            "Episode: 900 | Moving Average Reward: 266 | Episode Reward: 248 | Loss: 18.349 | Steps: 249 | Worker: 0\n",
            "Episode: 901 | Moving Average Reward: 266 | Episode Reward: 274 | Loss: 17.164 | Steps: 275 | Worker: 1\n",
            "Episode: 902 | Moving Average Reward: 265 | Episode Reward: 229 | Loss: 20.894 | Steps: 230 | Worker: 0\n",
            "Episode: 903 | Moving Average Reward: 265 | Episode Reward: 201 | Loss: 22.539 | Steps: 202 | Worker: 1\n",
            "Episode: 904 | Moving Average Reward: 264 | Episode Reward: 156 | Loss: 23.904 | Steps: 157 | Worker: 0\n",
            "Episode: 905 | Moving Average Reward: 263 | Episode Reward: 181 | Loss: 27.03 | Steps: 182 | Worker: 1\n",
            "Episode: 906 | Moving Average Reward: 262 | Episode Reward: 181 | Loss: 25.216 | Steps: 182 | Worker: 0\n",
            "Episode: 907 | Moving Average Reward: 261 | Episode Reward: 122 | Loss: 39.747 | Steps: 123 | Worker: 1\n",
            "Episode: 908 | Moving Average Reward: 259 | Episode Reward: 137 | Loss: 26.319 | Steps: 138 | Worker: 0\n",
            "Episode: 909 | Moving Average Reward: 258 | Episode Reward: 134 | Loss: 26.847 | Steps: 135 | Worker: 1\n",
            "Episode: 910 | Moving Average Reward: 257 | Episode Reward: 150 | Loss: 26.536 | Steps: 151 | Worker: 0\n",
            "Episode: 911 | Moving Average Reward: 256 | Episode Reward: 183 | Loss: 19.965 | Steps: 184 | Worker: 1\n",
            "Episode: 912 | Moving Average Reward: 255 | Episode Reward: 158 | Loss: 19.622 | Steps: 159 | Worker: 0\n",
            "Episode: 913 | Moving Average Reward: 254 | Episode Reward: 169 | Loss: 19.072 | Steps: 170 | Worker: 1\n",
            "Episode: 914 | Moving Average Reward: 254 | Episode Reward: 184 | Loss: 19.307 | Steps: 185 | Worker: 0\n",
            "Episode: 915 | Moving Average Reward: 253 | Episode Reward: 213 | Loss: 15.947 | Steps: 214 | Worker: 1\n",
            "Episode: 916 | Moving Average Reward: 252 | Episode Reward: 156 | Loss: 20.534 | Steps: 157 | Worker: 0\n",
            "Episode: 917 | Moving Average Reward: 250 | Episode Reward: 63 | Loss: 52.021 | Steps: 64 | Worker: 1\n",
            "Episode: 918 | Moving Average Reward: 249 | Episode Reward: 148 | Loss: 19.852 | Steps: 149 | Worker: 0\n",
            "Episode: 919 | Moving Average Reward: 248 | Episode Reward: 151 | Loss: 16.959 | Steps: 152 | Worker: 1\n",
            "Episode: 920 | Moving Average Reward: 248 | Episode Reward: 211 | Loss: 13.377 | Steps: 212 | Worker: 0\n",
            "Episode: 921 | Moving Average Reward: 248 | Episode Reward: 249 | Loss: 10.342 | Steps: 250 | Worker: 1\n",
            "Episode: 922 | Moving Average Reward: 247 | Episode Reward: 183 | Loss: 15.716 | Steps: 184 | Worker: 0\n",
            "Episode: 923 | Moving Average Reward: 247 | Episode Reward: 196 | Loss: 12.011 | Steps: 197 | Worker: 1\n",
            "Episode: 924 | Moving Average Reward: 246 | Episode Reward: 192 | Loss: 12.432 | Steps: 193 | Worker: 0\n",
            "Episode: 925 | Moving Average Reward: 246 | Episode Reward: 205 | Loss: 12.25 | Steps: 206 | Worker: 1\n",
            "Episode: 926 | Moving Average Reward: 245 | Episode Reward: 161 | Loss: 17.029 | Steps: 162 | Worker: 1\n",
            "Episode: 927 | Moving Average Reward: 246 | Episode Reward: 316 | Loss: 7.054 | Steps: 317 | Worker: 0\n",
            "Episode: 928 | Moving Average Reward: 245 | Episode Reward: 137 | Loss: 15.648 | Steps: 138 | Worker: 1\n",
            "Episode: 929 | Moving Average Reward: 244 | Episode Reward: 147 | Loss: 16.77 | Steps: 148 | Worker: 0\n",
            "Episode: 930 | Moving Average Reward: 243 | Episode Reward: 169 | Loss: 13.28 | Steps: 170 | Worker: 1\n",
            "Episode: 931 | Moving Average Reward: 242 | Episode Reward: 177 | Loss: 11.071 | Steps: 178 | Worker: 0\n",
            "Episode: 932 | Moving Average Reward: 241 | Episode Reward: 131 | Loss: 15.621 | Steps: 132 | Worker: 1\n",
            "Episode: 933 | Moving Average Reward: 240 | Episode Reward: 148 | Loss: 15.436 | Steps: 149 | Worker: 0\n",
            "Episode: 934 | Moving Average Reward: 240 | Episode Reward: 187 | Loss: 12.445 | Steps: 188 | Worker: 1\n",
            "Episode: 935 | Moving Average Reward: 239 | Episode Reward: 141 | Loss: 16.9 | Steps: 142 | Worker: 0\n",
            "Episode: 936 | Moving Average Reward: 238 | Episode Reward: 142 | Loss: 16.38 | Steps: 143 | Worker: 0\n",
            "Episode: 937 | Moving Average Reward: 237 | Episode Reward: 201 | Loss: 10.11 | Steps: 202 | Worker: 1\n",
            "Episode: 938 | Moving Average Reward: 236 | Episode Reward: 137 | Loss: 12.469 | Steps: 138 | Worker: 0\n",
            "Episode: 939 | Moving Average Reward: 235 | Episode Reward: 145 | Loss: 12.469 | Steps: 146 | Worker: 1\n",
            "Episode: 940 | Moving Average Reward: 235 | Episode Reward: 183 | Loss: 10.624 | Steps: 184 | Worker: 0\n",
            "Episode: 941 | Moving Average Reward: 234 | Episode Reward: 161 | Loss: 15.476 | Steps: 162 | Worker: 1\n",
            "Episode: 942 | Moving Average Reward: 233 | Episode Reward: 110 | Loss: 17.34 | Steps: 111 | Worker: 0\n",
            "Episode: 943 | Moving Average Reward: 232 | Episode Reward: 112 | Loss: 15.532 | Steps: 113 | Worker: 1\n",
            "Episode: 944 | Moving Average Reward: 231 | Episode Reward: 131 | Loss: 12.679 | Steps: 132 | Worker: 0\n",
            "Episode: 945 | Moving Average Reward: 230 | Episode Reward: 157 | Loss: 8.941 | Steps: 158 | Worker: 1\n",
            "Episode: 946 | Moving Average Reward: 229 | Episode Reward: 105 | Loss: 18.114 | Steps: 106 | Worker: 0\n",
            "Episode: 947 | Moving Average Reward: 228 | Episode Reward: 131 | Loss: 13.108 | Steps: 132 | Worker: 1\n",
            "Episode: 948 | Moving Average Reward: 226 | Episode Reward: 77 | Loss: 19.623 | Steps: 78 | Worker: 0\n",
            "Episode: 949 | Moving Average Reward: 224 | Episode Reward: 59 | Loss: 27.092 | Steps: 60 | Worker: 1\n",
            "Episode: 950 | Moving Average Reward: 224 | Episode Reward: 173 | Loss: 9.14 | Steps: 174 | Worker: 0\n",
            "Episode: 951 | Moving Average Reward: 223 | Episode Reward: 179 | Loss: 7.736 | Steps: 180 | Worker: 1\n",
            "Episode: 952 | Moving Average Reward: 222 | Episode Reward: 123 | Loss: 15.376 | Steps: 124 | Worker: 0\n",
            "Episode: 953 | Moving Average Reward: 222 | Episode Reward: 134 | Loss: 9.432 | Steps: 135 | Worker: 1\n",
            "Episode: 954 | Moving Average Reward: 221 | Episode Reward: 176 | Loss: 7.634 | Steps: 177 | Worker: 0\n",
            "Episode: 955 | Moving Average Reward: 221 | Episode Reward: 163 | Loss: 11.331 | Steps: 164 | Worker: 1\n",
            "Episode: 956 | Moving Average Reward: 220 | Episode Reward: 157 | Loss: 7.493 | Steps: 158 | Worker: 0\n",
            "Episode: 957 | Moving Average Reward: 219 | Episode Reward: 134 | Loss: 10.23 | Steps: 135 | Worker: 1\n",
            "Episode: 958 | Moving Average Reward: 219 | Episode Reward: 171 | Loss: 7.931 | Steps: 172 | Worker: 1\n",
            "Episode: 959 | Moving Average Reward: 219 | Episode Reward: 274 | Loss: 4.375 | Steps: 275 | Worker: 0\n",
            "Episode: 960 | Moving Average Reward: 218 | Episode Reward: 156 | Loss: 6.906 | Steps: 157 | Worker: 1\n",
            "Episode: 961 | Moving Average Reward: 218 | Episode Reward: 164 | Loss: 9.409 | Steps: 165 | Worker: 0\n",
            "Episode: 962 | Moving Average Reward: 218 | Episode Reward: 188 | Loss: 5.983 | Steps: 189 | Worker: 0\n",
            "Episode: 963 | Moving Average Reward: 218 | Episode Reward: 267 | Loss: 6.041 | Steps: 268 | Worker: 1\n",
            "Episode: 964 | Moving Average Reward: 217 | Episode Reward: 147 | Loss: 7.522 | Steps: 148 | Worker: 1\n",
            "Episode: 965 | Moving Average Reward: 217 | Episode Reward: 185 | Loss: 6.892 | Steps: 186 | Worker: 0\n",
            "Episode: 966 | Moving Average Reward: 217 | Episode Reward: 254 | Loss: 5.208 | Steps: 255 | Worker: 1\n",
            "Episode: 967 | Moving Average Reward: 218 | Episode Reward: 226 | Loss: 4.435 | Steps: 227 | Worker: 0\n",
            "Episode: 968 | Moving Average Reward: 217 | Episode Reward: 141 | Loss: 8.657 | Steps: 142 | Worker: 1\n",
            "Episode: 969 | Moving Average Reward: 217 | Episode Reward: 245 | Loss: 5.473 | Steps: 246 | Worker: 0\n",
            "Episode: 970 | Moving Average Reward: 216 | Episode Reward: 160 | Loss: 7.645 | Steps: 161 | Worker: 1\n",
            "Episode: 971 | Moving Average Reward: 217 | Episode Reward: 249 | Loss: 4.921 | Steps: 250 | Worker: 1\n",
            "Episode: 972 | Moving Average Reward: 218 | Episode Reward: 318 | Loss: 3.27 | Steps: 319 | Worker: 0\n",
            "Episode: 973 | Moving Average Reward: 219 | Episode Reward: 292 | Loss: 4.362 | Steps: 293 | Worker: 0\n",
            "Episode: 974 | Moving Average Reward: 219 | Episode Reward: 284 | Loss: 4.223 | Steps: 285 | Worker: 1\n",
            "Episode: 975 | Moving Average Reward: 219 | Episode Reward: 209 | Loss: 5.822 | Steps: 210 | Worker: 0\n",
            "Episode: 976 | Moving Average Reward: 220 | Episode Reward: 307 | Loss: 3.545 | Steps: 308 | Worker: 1\n",
            "Episode: 977 | Moving Average Reward: 220 | Episode Reward: 254 | Loss: 4.255 | Steps: 255 | Worker: 1\n",
            "Episode: 978 | Moving Average Reward: 223 | Episode Reward: 462 | Loss: 2.703 | Steps: 463 | Worker: 0\n",
            "Episode: 979 | Moving Average Reward: 223 | Episode Reward: 229 | Loss: 4.386 | Steps: 230 | Worker: 1\n",
            "Episode: 980 | Moving Average Reward: 223 | Episode Reward: 259 | Loss: 4.79 | Steps: 260 | Worker: 0\n",
            "Episode: 981 | Moving Average Reward: 224 | Episode Reward: 348 | Loss: 4.315 | Steps: 349 | Worker: 1\n",
            "Episode: 982 | Moving Average Reward: 225 | Episode Reward: 256 | Loss: 5.076 | Steps: 257 | Worker: 0\n",
            "Episode: 983 | Moving Average Reward: 225 | Episode Reward: 237 | Loss: 4.33 | Steps: 238 | Worker: 0\n",
            "Episode: 984 | Moving Average Reward: 226 | Episode Reward: 319 | Loss: 2.861 | Steps: 320 | Worker: 1\n",
            "Episode: 985 | Moving Average Reward: 226 | Episode Reward: 236 | Loss: 4.456 | Steps: 237 | Worker: 1\n",
            "Episode: 986 | Moving Average Reward: 229 | Episode Reward: 525 | Loss: 2.534 | Steps: 526 | Worker: 0\n",
            "Episode: 987 | Moving Average Reward: 230 | Episode Reward: 345 | Loss: 3.622 | Steps: 346 | Worker: 1\n",
            "Episode: 988 | Moving Average Reward: 231 | Episode Reward: 291 | Loss: 4.041 | Steps: 292 | Worker: 1\n",
            "Episode: 989 | Moving Average Reward: 232 | Episode Reward: 421 | Loss: 2.958 | Steps: 422 | Worker: 0\n",
            "Episode: 990 | Moving Average Reward: 232 | Episode Reward: 209 | Loss: 5.01 | Steps: 210 | Worker: 0\n",
            "Episode: 991 | Moving Average Reward: 232 | Episode Reward: 242 | Loss: 5.239 | Steps: 243 | Worker: 1\n",
            "Episode: 992 | Moving Average Reward: 232 | Episode Reward: 218 | Loss: 4.815 | Steps: 219 | Worker: 0\n",
            "Episode: 993 | Moving Average Reward: 232 | Episode Reward: 239 | Loss: 3.816 | Steps: 240 | Worker: 1\n",
            "Episode: 994 | Moving Average Reward: 231 | Episode Reward: 132 | Loss: 11.113 | Steps: 133 | Worker: 1\n",
            "Episode: 995 | Moving Average Reward: 231 | Episode Reward: 239 | Loss: 3.926 | Steps: 240 | Worker: 0\n",
            "Episode: 996 | Moving Average Reward: 233 | Episode Reward: 363 | Loss: 3.807 | Steps: 364 | Worker: 0\n",
            "Episode: 997 | Moving Average Reward: 235 | Episode Reward: 452 | Loss: 2.595 | Steps: 453 | Worker: 1\n",
            "Episode: 998 | Moving Average Reward: 234 | Episode Reward: 188 | Loss: 5.782 | Steps: 189 | Worker: 0\n",
            "Episode: 999 | Moving Average Reward: 234 | Episode Reward: 196 | Loss: 7.09 | Steps: 197 | Worker: 1\n",
            "Episode: 1000 | Moving Average Reward: 235 | Episode Reward: 391 | Loss: 3.205 | Steps: 392 | Worker: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xd4HOW59/HvrWZbcpFky3KTLXfj\nbmOMTQm9E0oONSGYhAPkDSSBwEng5JxUkpByICEhBBIIhNATkhBwaMb04opt3HDvtiRLltXr/f6x\nYyGDZK1krVa7+n2ua6/deWZmdQ9jdOuZp5m7IyIi8kkJ0Q5AREQ6JyUIERFpkhKEiIg0SQlCRESa\npAQhIiJNUoIQEZEmKUGIiEiTlCBERKRJShAiItKkpGgHcDj69evnubm50Q5DRCSmLF68uMDds1o6\nLqYTRG5uLosWLYp2GCIiMcXMtoRznB4xiYhIk5QgRESkSUoQIiLSJCUIERFpkhKEiIg0SQlCRESa\npAQhIiJNUoIQEWmDxVsKmb8mj3hetjmmB8qJiETLlQ8soKy6jpm5mTx6zdEkJ8bf39vxd0UiIhFW\nU1dPWXUdAAs2F/KvZTujHFFkKEGIiLRSaWUtAN89dzwj+qXxyHthzVwRc5QgRERaqbwmVHtI65bI\nlbOHsXTrPj7cURzlqNqfEoSISCtVBI+XeqQkcf7UwSQYvLRqT5Sjan9KECIirdSQIJITyUhL4chh\nGcxbrQQhItLlVQSPmFJTEgE45YhsVu7cz9a95dEMq90pQYiItFJ5daiRukeQIC6YOpikBOPP726O\nXlARoAQhItIKRWXV3PL0MiD0iAlgQJ/unDlxAE8t2taQPOKBEoSISCtcev+7FJRWA9CvZ7eG8jnH\n5LK/spY/vb05SpG1v4glCDPLMbP5ZrbKzFaa2TeC8u+b2Q4z+yB4nd3onNvMbL2ZrTWzMyIVm4hI\nW320p7Thc1avjxPEjGEZnHpENr9+ZR2FZdXRCK3dRbIGUQvc7O7jgVnA9WY2Pth3l7tPDV5zAYJ9\nlwETgDOB35lZYgTjExFplf2VNQDcdOoYln339IP2mRk3nDyK6rp63tlQEI3w2l3EEoS773L3JcHn\nEmA1MPgQp5wPPOHuVe6+CVgPzIxUfCIirbW7uBKA4Vlp9ElN/tT+8QN70y0pgQWbCjs6tIjokDYI\nM8sFpgHvB0U3mNlyM3vQzDKCssHAtkanbaeJhGJm15rZIjNblJ+fH8GoRUQOtr8iVINI7/Hp5ACQ\nkpTAqeOzeWbJDvbsr+zI0CIi4gnCzHoCfwNudPf9wL3ASGAqsAv4v9Z8n7vf7+4z3H1GVlZWu8cr\nItKc0qpQD6W0bs1PhP2lY3Ipq67lwnvepjhIKLEqognCzJIJJYdH3f0ZAHff4+517l4P/IGPHyPt\nAHIanT4kKBMR6RTKqkID5Hp1bz5BzMjN5IE5M9hTUsXsn85jfV5ps8d2dpHsxWTAA8Bqd7+zUfnA\nRoddCHwYfH4WuMzMupnZcGA0sCBS8YmItFZpVahGcKgaBMDJ47K5/sSRlFfXcf5v3+qI0CIikjWI\nY4EvAid/okvrz81shZktB04CbgJw95XAU8Aq4AXgenevi2B8IiKtUhrUIHq2kCAAvnrSKNJSEimr\nrovZKTgitqKcu78FWBO75h7inB8DP45UTCIih6OwrIrEBCMtpeUe+N2TE5l384kc//NXefDtTXz/\nvAkdEGH70khqEZEwrdq5n9H9e5IU5vKiA/p05/ypg3li4VaKYnDwnBKEiEiYNu8tZ2T/nq065z+P\nH05lTT33vbExQlFFjhKEiEgY3J0d+yoYkt6jVeeNG9Cb86YM4i/vbaGsKrYm8lOCEBEJw6tr8qiu\nrWdIRusSBMCcY4ZRWlXL3a+ui0BkkaMEISLSgm2F5Vz98CIAzp40sIWjP+3IYZmcM3kgj723Naam\nA1eCEBFpwa/nhf7y//opo+nbaIrv1rj6uOGUVNVy3+uH1xbx0NubOPNXb1BQWnVY3xMOJQgRkRbk\nlVTRNy2Fr544ss3fMX1oBmdPGsCDb22isqb1Q7xKq2qpravnsQVbWbO7hLte/qjNsYQrYuMgRETi\nxd7SKqbmpNM9+fBWILj0qKHMXbGbN9cVcNr47LDPq6yp49g7Xm2Y22lU/578zznjWzjr8KkGISLS\ngoLSKjLTUg77e44Z2ZeM1GSeXbazVeet2FF80MR/N506pmE97EhSghARaUZBaRWX3vcue/ZXMaxv\n6mF/X3JiAp+bPoS5K3a1ajrwR97dQs9uSbx968k8/ZXZnDVxwGHHEg4lCBGRZjyzZDvvB4v/HDGw\nd7t85+Uzc6ird174cHdYx7s789fmcc6kgQxO78FRuZkkJDQ1i1H7U4IQEWnG5r3lpKUk8ucvz+Sk\nsf3b5TtH9e/F6P49mbtiV1jH7yuvoaSyljEDerXLz28NJQgRkWas31PKyP49+cyYrHb9q/3sSQNZ\nsLmQ/JKWu6o+vnArADltGKB3uJQgRESaUFZVy4LNhRw7ql+7f/fZkwbiTou1iLfWFfDzF9Yyol8a\nx4/u+BU0lSBERJqwvzLUaygn4/Abpz9pTHZPJg3uw29eXUdVbdNjIvaWVnH786vom5bCk9fN7pBe\nS5/UbII4sKhPc6+ODFJEpKOVNaw/3f6/mM2Mb505loLSap79oOkur88t38Wa3SV8/ZTRZPVq2+jt\nw3WogXLnBu/XB++PBO9fiFw4IiKdQ1krVo9ri2NH9mPykD7c/vxqThibRf9e3Q/av3hLEb27J3Hl\n7GER+fnhaLYG4e5b3H0LcJq7f8vdVwSvW4HTOy5EEZGO93ENIjIJIiHBuOvSqRRX1DDzx/NYs3t/\nw76/vLeFZ5ft5NTx2Zh1TJfWJmMM4xgzs2MbbRwT5nkiIjGjurb+oFXfVu0K/cJOS4ncjEQjs3ry\n9ZNHAXDmr97k1Dtf52+Lt/OzF9aQnGjcdtYREfvZ4Qjnyr8M/MnM+gTb+4IyEZGYl1dSyRMLtnFn\nMPndSWOzOGfyIG5/fjUAfXse/hQbh/LN08cyeUg6//nnRazPK+Xmp5cB8I/rj41a28MBh0wQZpYA\njHL3KQcShLsXd0hkIiLt5MMdxfxr2U5uOWMsyYkJbCss5411+Rw3qh8n/OK1g46dvzaf+WvzAfjf\nc8czqJUryLXFqeOz+ej2s3h51R5+Mnc1/zF9MFNz0iP+c1tyyATh7vVm9i3gKSUGEYlVtzy9jDW7\nS+iRksiNp47h239bzjsb9jbsz+2byqPXzGJweg/++OZGnly4jWuOH8HFM4Z0WIwpSQmcM3kg50xu\n/YJEkWLufugDzO4ACoAngbID5e5eGNnQWjZjxgxftGhRtMMQkU7qzpc/YlhmKne+/BE79lUAMHFw\nbz7cEWpfmJmbyZXHDOPcyYOiGWaHM7PF7j6jpePCaYO4NHi/vlGZAyPaEpiISEdwd+6e9/Ea0Jcd\nlcOy7cUNyeEvVx/NcaPbf5R0PGkxQbj78I4IRESkPe2vPHjt59SUJP44ZwZbCsrI7ZfWIW0LsS6s\n/ltmNhEYDzSM5HD3P0cqKBGRtiqprOHhdzZTVVsPwO0XTOTNdfmcN3UQg9N7MFiJIWwtJggz+x5w\nIqEEMRc4C3gLUIIQkU6jqraOV1blsS6vhF+98vGjpVOO6M8Vs6I3GjmWhVODuAiYAix19y+ZWTbw\nl8iGJSISPnfn2j8v5vWP8hvKThufzWVH5TCwj2oMbRVOgqgIurvWmllvIA/IiXBcIiItqq93Vu/e\nz8W/f5fy6jp6JCcysn8aFx+Zw5xjcqMdXswLJ0EsMrN04A/AYqAUeDeiUYmItGDxlkKu+tNCyqvr\nqKsPdde/94rpnNhOK79JeL2Yvhp8/L2ZvQD0dvcWp/s2sxxC7RTZhLrF3u/uvzazTEJjKnKBzcAl\n7l5koRmpfg2cDZQDV7n7ktZfkojEO3fnp3PXUBL0VDpmZF9+efEU9UxqZ+E0Uj8CvAG86e5rWvHd\ntcDN7r7EzHoBi83sZeAqYJ6732FmtwK3At8m1Pg9OngdDdwbvIuIHKS4ooZFW4q46phcenZLYs4x\nuVGftygehTMr64PAQOA3ZrbRzP5mZt9o6SR333WgBuDuJcBqYDBwPvBwcNjDwAXB5/OBP3vIe0C6\nmXWeMeci0mnsKw+t9jZpcB9uOWOskkOEhPOIab6ZvQEcBZwEfAWYQOhxUFjMLBeYBrwPZLv7gYVY\ndxN6BAWh5LGt0Wnbg7KDFm01s2uBawGGDh0abggicWV/ZQ2pyYm8vGoPZdV1DMnowawRfaMdVocp\nKg9Ny52RlhzlSOJbOI+Y5gFphBqm3wSOcve8cH+AmfUE/gbc6O77Gy9+4e5uZoeeDOoT3P1+4H4I\nzcXUmnNF4sEj727mf/+58lPl933xSM6YMKDjA4qCfRWhGkSfHpGdirurC+cR03KgGpgITAYmmllY\nLUFmlkwoOTzq7s8ExXsOPDoK3g8kmx0c3H12SFAmIoE3Psr/VHK4+/JpZPfuxg//tYr6+q7xN9O/\nV+wiMcEYkqFG6UgK5xHTTQBBQ/NVwJ+AAcAhH/oFvZIeAFa7+52Ndj0LzAHuCN7/2aj8BjN7glDj\ndHGjR1EiAvxr2U769Ehm4XdO5YRfzCcjNYXzpoRmIv3640t5ZukOLjqy46aojqTiihrW7i4hJ7MH\nRWU1LNu+j4fe3sz6/FLq6p3rThhBdu/uLX+RtFk4j5huAI4HjiTULfVBQo+aWnIs8EVghZl9EJT9\nN6HE8JSZXQ1sAS4J9s0l1MV1PaFurl8K+ypEugB35+31BRw7qi8pSQm89l8ncmC2/tPHZ5OT2YOf\nvbCG86YMIiUpdlcF3lxQxgNvbeKR97Y0e8xRuRncdOqYDoyqawpnoFx34E5gsbvXtnTwAe7+FtDc\natunNHG8c/CU4iLSSH5pFTuLK7kmNxOAbkmJDfu6Jyfyw/Mm8qWHFvLiyt18dkrsrm/w96U7GpLD\nlCF9mDWyL89+sJPUlETOnzqYLxw9lL491WupI4TziOmXZnYcodrAn8wsC+jp7psiHp2INNi5rxKA\nnIzUJvefMCaL3L6p/PbV9Zw7eSCNO4TEkv2VNfTslsQH3z2NxATDzLjtrCOiHVaX1GI9NJjN9dvA\nbUFRMpqsT6TD7SgKrYg2ML3p5+4JCcbVx49g7Z6ShkVxYlFJZS29uieRlJgQs0kuXoTzoPJC4DyC\n5UbdfSfQK5JBicinvbU+n57dkhiZ1bPZY04fn0335AQ++9u3eKPRzKaxpKSyhl7dw1qqRiIsnARR\nHbQPOICZpUU2JBFpyoodxUwflkH35MRmj8nu3Z0HrzoKM/ja40vZmF/agRF+WnVtPX9dvJ2K6rqw\nzwnVIDQArjMIJ0E8ZWb3EZr64hrgFUIzu4pIB6mvd9bnlTK6f/O1hwOOGdmP1285icQE4zt//7AD\nomveM0u2c8vTy/jGE0sbZlxtye79lWSmaQBcZ9BignD3XwJ/JTTgbSzwXXf/TaQDE5GPbS+qoLKm\nPqwEATC0bypXHzecdzfuZf7asCc+aHevrM4jJSmBl1bt4aYnPzhkTaK0qpYL7nmbjfllTB+a0YFR\nSnMOmSDMLNHM5rv7y+7+X+5+i7u/3FHBiUjIS6t2AzB+UO+wz7n6uOGMG9CLG5/4gFfX7IlUaAfZ\nXlTOc8t3UlRWTU1dPe9t3MtFRw7hayeP4tllO7nywfeprGk6ScxbvYcPtu0D4PQJ2U0eIx3rkAnC\n3euAejPr00HxiMgn/OLFNdz+/GqmDU1n0uDw/1fsnpzIbz8/jeKKGr780KJDDjxrbMGmQsqrwx7y\ndJCbn1rGDY8tZdqPXuaMu96gtKqWaTnp3Hz6WG47axwLNxcx4/ZXKA5mY21sze4SAObdfMIhG+Kl\n44TTBlFKaDT0A2Z294FXpAMTEaitq+ee+RsA+NKxw1vd7XNU/148cvVMAG5/bhXbi8oPeXxRWTWX\n3PcuJ/7iNYorPv1L/FDW7Snh/U2FDdsbC8oAmJKTDsB1J4zkytnDKK2q5fbnVzUc9+LK3Ty1aBtP\nLdzGhEG9lRw6kXD6kj0TvESkg7248uNHQ2dPbNtMrcePzuLtW0/m5F++xn8+vIg/zpnBkGYG2xUG\n02jnlVQx5Qcv8cVZw/j2WePo2a3lXxW/emUdZjDvmyfQLTmRrz22hKra+oN+4f/w/In07JbE717b\nwMnj+nPGhAFc98jihv23XzCxTdcokRHOSOqHWzpGRCLjiYVbGZqZyvxbTiQxoe2Dxgan9+AXF0/h\npic/4IbHlvL0V2aTnBh6gODuPL5gG2dOHMD+oNYwMzeTBZsLeeS9LWzIL+WhL81sdn6nHfsq+N4/\nV/LK6j2cM2kgI4KE8MxXj23y+K+fMprX1ubz/x5dwsisUK/5lMQEPjtlEKd3kenKY0Xszugl0gWs\nzyvlyGEZh5UcDjhvyiDuvmwaH2zbxx3/XtMwNfia3SX8999XMP1HL7NiRzEA3z5rLJvvOIdbzxrH\nOxv28pO5q5v8zrySSk6/83VeWR2q6UzJabmNpHtyInddOpWczB5syA89hnriuln83yVT2uU6pf1o\nuKJIJ1VeXcuu4kpG9Gu/sannTB7I35dm88Bbm3hu+U5G9+/FW+sLGvZ/N1hroncwUO0rJ4zkoz0l\nPPTOZvJLq7j1zHHkZH78eGrlzv2UVdfx84smM3FQH0b2Dy/WsQN68erNJ/Luhr0s2FTI5FY0vkvH\nCbsGYWa9gzUhRKQDbAoaeYdnte/kBb/9/DR++rlJDMtMo6C0CoBBfbrz68umNhwzoM/H8z395MJJ\nXDJjCM8v38XxP5/P/W9saNi3YnuoxnH86H6MH9T7oBlmW5KcmMBnxmRxyxljSUrUw4zOyNwPPbrR\nzI4itAZEL0LTd+8Dvuzuiw95YgeYMWOGL1q0KNphiLS77UXlXP6H99hWWMFLN32GMdmR+9ustq7+\noF/Q7t5kb6mlW4v4ydzVLNxcBEByolFT5/RNS2HBd07V46EYYmaL3X1GS8eFk7YfAL7q7rnuPozQ\nmg1/OtwARaRp2wrLOe5n89lWWMFlR+VENDkAn/rrvbmutNOGZvDYNbO46phcADJSUzgqN4OnvzJb\nySFOhdMGUefuDSvIuftbZta2UTQi0qIDYwm+csJIbj1rXJSjOVhyYgLfP28C3zx9TEM7hcSvcBLE\n68FkfY8TmtH1UuA1M5sO4O5LIhifSJewc18F/+/RJdTU1rNqV2gth6+eNDLKUTVPyaFrCCdBTAne\nv/eJ8mmEEsbJ7RqRSBdTWlXLN55YysodxdQGXU+/dvIo/RKWqAtnoNxJHRGISFdUVlXLhfe8zbq8\nUu66dAqzR/TjtbV5XDwjJ9qhiYS15Gh2MA/Tv4Pt8WZ2deRDE4l/c1fsYl1eKb++bCoXThvCgD7d\nuWzmUDX6SqcQTi+mh4AXgUHB9kfAjZEKSKSrqKt3/r50B6kpiXx28qCWTxDpYOEkiH7u/hRQD+Du\ntUD46weKSJPumb+edzbsJTMthQTVGKQTCidBlJlZXz5ek3oWUBzRqETinLvzwoehRYB+cdGUFo4W\niY5wejF9E3gWGGlmbwNZwEURjUokzv1r+S5W7drPjy6YyOyRfaMdjkiTwunFtMTMTiC0HrUBa929\ndSuJiEiDD3cU8/XHlzJ5SB8+P3NotMMRaVZYs7kG7Q4rIxyLSJewcHNopPT3z5ug3krSqWkKRZEO\nVlRWjRlMGZIe7VBEDkkJQqSDFZZXk94jWbUH6fTCGShnZnaFmX032B5qZjMjH5pI/Kmrd579YCcZ\nqSnRDkWkReHUIH4HzAYuD7ZLgHtaOsnMHjSzPDP7sFHZ981sh5l9ELzObrTvNjNbb2ZrzeyMVl6H\nSEz4+9Id7K+s5YJpg6MdikiLwkkQR7v79UAlgLsXAeH8+fMQcGYT5Xe5+9TgNRdC03cAlwETgnN+\nZ2bhL00lEgPcnTv+vYaJg3tzw0mjoh2OSIvCSRA1wS/rAwPlsghGVR+Ku78BFIYZx/nAE+5e5e6b\ngPWAHmNJXNlUUEZBaRVXHD1MI6clJoSTIO4G/g70N7MfA28BPzmMn3mDmS0PHkFlBGWDgW2Njtke\nlH2KmV1rZovMbFF+fv5hhCHSsZZs3QfA9GEZLRwp0jm0mCDc/VHgW8BPgV3ABe7+dBt/3r3ASGBq\n8F3/19ovcPf73X2Gu8/IyspqYxgiHe+llbvpm5bCqKye0Q5FJCwtDpQzs0wgj9CKcgfKktsymtrd\n9zT6jj8AzwWbO4DGE+APCcpE4sLOfRW8snoP150wUo+XJGaE84hpCZBPaJrvdcHnzWa2xMyObM0P\nM7OBjTYvBA70cHoWuMzMupnZcGA0sKA13y3SWbk7P/zXKsxMU2tITAlnqo2Xgb+6+4sAZnY68B/A\nnwh1gT26qZPM7HHgRKCfmW0ntGTpiWY2lVCD92bgOgB3X2lmTwGrgFrgenfXlOISF95YV8ALK3dz\n61njyMlMjXY4ImELJ0HMcvdrDmy4+0tm9kt3v87MujV3krtf3kTxA4c4/sfAj8OIRySmvPlRPilJ\nCXzp2NxohyLSKuEkiF1m9m3giWD7UmBP0PW1xe6uIl3doi1FTB7ch25JGtojsSWcNojPE2o0/kfw\nGhqUJQKXRC40kdjn7qzatZ+pOZqYT2JPOOtBFABfa2b3+vYNRyS+FJZVU11bz5CMHtEORaTVwunm\nmkVoHMQEoPuBcnc/OYJxicSFNbtLABiYrgQhsSecR0yPAmuA4cAPCPU+WhjBmETixhMLt9EtKUGP\nmCQmhZMg+rr7A0CNu7/u7l8GVHsQCcO7G/Zy7uRBZPfu3vLBIp1MOL2YDoyY3mVm5wA7gczIhSQS\nH4rKqikorWJUf02tIbEpnARxu5n1AW4GfgP0Bm6KaFQiceDR97cA6PGSxKxDJohgrMNod38OKAZO\n6pCoROLAK6vzmDi4N7NH9o12KCJtcsg2iGC6i6ZGRIvIIeyvrGHZ9n2cekR2tEMRabNwHjG9bWa/\nBZ4Eyg4UuvuSiEUlEuO27i3HHcYN6B3tUETaLJwEMTV4/2GjMkc9mUSatbWwHEAD5CSmhTOSWu0O\nIq3wzvoCvvroEnp3T2KkFgeSGNbiOAgzyzazB8zs38H2eDO7OvKhicSmu19dB8CvL59GjxRN0Cex\nK5yBcg8BLwKDgu2PgBsjFZBILFu3p4T3NhZy06ljOGls/2iHI3JYwkkQ/dz9KYKpvd29FtBiPiJN\nWLsnNPfS6RPUe0liXzgJoszM+hJqmMbMZhEaEyEin7CtsAJAK8dJXAinF9PNhNaMHmlmbwNZwEUR\njUokRm0rKicjNZme3cL5X0ukcwunF9NiMzsBGAsYsNbda1o4TaTL2VtaxWPvb412GCLtJpxeTMsJ\nrQdR6e4fKjmING3uil0AZPdudql2kZgSThvEZ4Fa4CkzW2hmt5jZ0AjHJRJznli4jX49U3j9vzR0\nSOJDiwnC3be4+8/d/UhCa1FPBjZFPDKRGFJeXcva3SVcPCOH7ska+yDxIayWNDMbBlwavOoIPXIS\nkcCra/KorXdOHJMV7VBE2k04a1K/DyQDTwMXu/vGiEclEmMWbS4iNSWRI4dlRDsUkXYTTg3iSndf\nG/FIRGLYkq1FTB7Sh6TEcJr1RGJDON1c1wZLjU4Aujcq/2HzZ4l0HSWVNazauZ/rThgR7VBE2lU4\n3Vx/T6jt4WuExkFcDAyLcFwiMePdDXuprXeOG6X2B4kv4dSHj3H3K4Eid/8BMBsYE9mwRGLHuxv3\n0iNZ7Q8Sf8JJEBXBe7mZDQJqgIGRC0kktmzIL2Nk/zRSktT+IPElnH/Rz5lZOvALYAmwGXispZPM\n7EEzyzOzDxuVZZrZy2a2LnjPCMrNzO42s/VmttzMprftckQ6lruzbk8Jw/tpYSCJP+EMlPuRu+9z\n978RansY5+7fDeO7HwLO/ETZrcA8dx8NzAu2Ac4CRgeva4F7wwtfJLpe/yifXcWVHDOyb7RDEWl3\nraoTu3uVu4c11be7vwEUfqL4fODh4PPDwAWNyv/sIe8B6Wamx1jSqbk7d738EUMzU7lw2uBohyPS\n7jr6oWm2u+8KPu8GDqyqMhjY1ui47UGZSKe1Ib+MZduLufq44ZpeQ+JS1FrV3N0JFiFqDTO71swW\nmdmi/Pz8CEQmEp5Fm0MV5ONG94tyJCKREc5UG001GBcDW4LlR1tjj5kNdPddwSOkvKB8B5DT6Lgh\nQdmnuPv9wP0AM2bMaHWCEWkP+SVV/Oi5VWSmpTCiX1q0wxGJiHBqEL8D3iP0S/kPwLuE5mVaa2an\nt/LnPQvMCT7PAf7ZqPzKoDfTLKC40aMokU7nxieXUlZdx5zZuZhZtMMRiYhwEsROYJq7zwim/J4G\nbAROA37e3Elm9jihZDLWzLab2dXAHcBpZrYOODXYBpgbfOd6Qknoq228HpEOsau4EoDrTxoZ5UhE\nIiecyfrGuPvKAxvuvsrMxrn7xkP95eTulzez65QmjnXg+jBiEYk6d6egpIorZg3V5HwS18JJECvN\n7F7giWD7UmCVmXUjNKpapEspKq9hf2UtuX3V9iDxLZw/f64i9OjnxuC1MSirAbS2onQ5mwpKARiR\npQQh8S2c6b4rgP8LXp9U2u4RiXRymwrKAVSDkLgXTjfXY4HvE5pmo+F4d9fk99Ilrc8rJSnByMlM\njXYoIhEVThvEA8BNwGJC61GLdFk1dfW8tHI3Ewb3IVkN1BLnwkkQxe7+74hHItLJ1dU7D7+zmY0F\nZfzkwknRDkck4sJJEPPN7BfAM0DVgUJ3XxKxqEQ6oedX7OL251cD8LnpmipM4l84CeLo4H1GozIH\nTm7/cEQ6r+Xb9gFw3WdGaHI+6RLC6cWkrqzS5a3etZ8/vrWJoZmp3Hb2EdEOR6RDNJsgzOwKd/+L\nmX2zqf3ufmfkwhLpXJZuDdUe/lvJQbqQQ9UgDnTy7tURgYh0Zos2F9I9OYHTx2e3fLBInGg2Qbj7\nfcHH37m7Fl6QLumZJdu565WP2FZYwTXHDychQTO3StcRTiP122a2GXgSeMbdiyIbkkjnsKu4gm8+\ntQyAcyYN5JYzxkY5IpGO1eKtubjUAAAPsElEQVRIH3cfA/wPMAFYbGbPmdkVEY9MJMpe+HA3AD++\ncCL3fGE63ZLUc0m6lrCGgrr7Anf/JjATKAQejmhUIp3Av5bt5IiBvfnC0cOiHYpIVLSYIMyst5nN\nMbN/A+8AuwglCpG4tb2onCVb93Hu5IHRDkUkasJpg1gG/AP4obu/G+F4RKJuX3k1x/1sPgCfnTwo\nytGIRE84CWKEu7uZ9TSznu6uKb4lrj22YCsARw/PZGhfzdgqXVc4bRATzGwpsJLQSnKLzWxihOMS\niZq/Ld7OUbkZPHHtrGiHIhJV4SSI+4Fvuvswdx8K3ByUicSVvP2VXHDP22zIL+OsiQM51JrrIl1B\nOI+Y0tx9/oENd3/NzLSUlsSVN9fl88UHFjRsn6YR0yJhJYiNZva/wCPB9hWE1qUWiQsV1XXc9swK\nAIZk9OD3Vxyp1eJECC9BfBn4AaH1IADeDMpE4sKf3tnE9qIKfveF6Zw5YYCm0xAJhDPddxHw9Q6I\nRaTDFVfUcO9rG5g8pA9nT9KYB5HGDjXd97OHOtHdz2v/cEQ61hMLtlJSWcv3z5sQ7VBEOp1D1SBm\nA9uAx4H3AdW7Je4s2lLEqP49mT40I9qhiHQ6h0oQA4DTgMuBzwPPA4+7+8qOCEykI+wurmRweo9o\nhyHSKTU7DsLd69z9BXefA8wC1gOvmdkNHRadSASVVdWyZW8ZA3p3j3YoIp3SIRupzawbcA6hWkQu\ncDfw98iHJRJ5v3ttPfsrazl3ihqnRZpyqEbqPwMTgbnAD9z9ww6LSiTC1u4u4Z75G5g4uDfHj86K\ndjgindKhahBXAGXAN4CvN5p2wAB3995t/aHBCnUlQB1Q6+4zzCyT0Kp1ucBm4BKtXieR4O7c/vwq\nAE4ZpxHTIs051JrUYS0mdBhOcveCRtu3AvPc/Q4zuzXY/naEY5AuZunWIi7+/bvU1jvfOGU0N502\nJtohiXRakU4CrXE+H69U9zBwQRRjkTj18Dubqa13vnXmWL5xyuhohyPSqUUrQTjwUjB1+LVBWba7\n7wo+7wZU95d2ta+8mrkf7ubK2cP46omjNKWGSAvCmYspEo5z9x1m1h942czWNN4ZLFDkTZ0YJJRr\nAYYOHRr5SCVuPLVoG9W19Xz+aP27EQlHVGoQ7r4jeM8j1G12JrDHzAYCBO95zZx7v7vPcPcZWVnq\nfSLh+evi7fxk7hpGZKUxbkCb+1eIdCkdXoMI1pJIcPeS4PPpwA+BZ4E5wB3B+z87OjaJH48v2MrW\nwnJ27qtg7e4S1uwuAeA/pg+JcmQisSMaj5iygb8H3WaTgMfc/QUzWwg8ZWZXA1uAS6IQm8SBv7y3\nhf/5x8fDdoZk9GBEvzR+8/lpTBjUJ4qRicSWDk8Q7r4RmNJE+V7glI6OR2LfOxsKeGnlHpZv30dB\naTVbC8sZ0Ls7v7x4CumpyUwY1FvLh4q0QbQaqUXarKyqluXbi5m/NtRMdf8bBy9wODa7F49eczT9\nenaLRngicUMJQmJKSWUNl973Hqt27W8oO2lsFj+/aAo9uyVR505KYgIpSZ1piI9IbFKCkJgwb/Ue\n7nz5I1buDCWGqTnpnDlxAEflZjJ9aLoeIYlEgBKEdCpLthbx1roChvVN5d8rdvP2+gKyendjY34Z\nAIP6dOdz04dw1bG5eoQkEmFKENIpVNXW8ewHO/mvvy4/qHxweg9KKmuZmpPOHf8xSWMYRDqQEoR0\nuLp6J7+kitRuiTz63lZeW5vH+5sKAejdPYlHrj6a/ZU11NU7J47tH+VoRbouJQiJuIWbC3n8/a3U\nuzOsbxq/nrfuU8eMye7JF44exmUzc+iWlBiFKEXkk5QgpM127Kugf69uJCeGegyVV9eyYnsxhWWh\nsQh5JVVsKyzntbX5VNfVN5yXkZrM2ZMGUlxRwxdnDWPsgF6kp6ZE6zJEpBlKENIqZVW1rNhRzLPL\ndvLY+1tJMEhOTKCqtp4Eg/pPTLGYYDAlJ507L5lKTkYPduyrICcjVTOpisQAJQgJS3F5DT/410qe\nWbrjoPL/PH4EJZW11NTVk5mWwoxhGaSmJDFuYC+SEoyUpARSUz7+Zzasb1pHhy4ibaQEIQ2Wbi0i\nv6SKbUUVFJVV0793N8qq6sgvqWLemj1s2VvOzNxMPjd9MEcOy2BU/54afyASx5QguhB3Z8e+Cron\nJ5KRmkJZ0Gawr7yGjLRkPv+H9w95/k8/N4nLZ2otBZGuQgkijpVU1lBfD/e+voFl2/axeEvRQY3F\nTbn2MyO4fOZQcjJ6kFdSRYIZfXokk5xoJCVq+gqRrkQJIs7U1zsfbN/HUwu38cTCbQ3luX1TGZ3d\nk0tm5FBTV8+G/DL69Eimf69uHDksg8KyajLTUpiSk95wzqD0HtG4BBHpJJQgYtCWvWWs3lXC0q1F\n9O/dnXV7Sli8pYjaeidvfyVl1XUAJCcaF04bzGfGZHHu5EFRjlpEYo0SRAyorKnjpVV7eGd9AYu3\nFLEur/Sg/T2SExk/qDd901I4blQ/pg1NZ1jfNMYN6EVaN91iEWkb/fbopHYVV/DqmjwWby7ig+37\nGiarmzS4DzeeOprpQzPIyUwlLSWRvj27kahxBSLSzpQgosDdWbJ1H8UV1SQnJpCUkEBKklFSWcuG\n/DLmrd7D+5sKqat3EhOMMdm9+O654/ni7GENo5ZFRCJNCSLC9pVX8+Dbm1m0uZCKmjpSEhPYWVzB\ntsKKZs9JS0nktCOyueYzI5gwqDfdkzU3kYh0PCWIdlJRXcfesiqKK2rYs7+S19bms2BTIWv3lOAe\n6kU0sE8PquvqGZnVkzmzc5k+LIP6eqe6rp7aulBtIbt3dwan96BHipKCiESXEsRhqKmr550Ne/nj\nmxt5c13BQfsSE4yjh2cyZ3Yun50ykCOHZUYpShGRtlGCaMGmgjI25pdSUVNHYVk1mwvK2VdRzf6K\nWhZtKWRfeQ0JBhdMHURiQgJTc/owOrsXY7N7kZGmGUpFJHZ1+QRRUlnD2+v38uLK3azcWUyfHsn0\n6p7MpoIyCsuqKa6oOeh4M+jTI5nsXt05YUwWJ4zJ4pRx2fRJTY7SFYiIREaXTBDz1+Txv//8kNKq\nWvaVhxJAYoIxY1gGCWbsLq4kIzWZY0f1ZWRWT3L7pjEwvTvpPVLI6tWNBEOT1IlI3OuSCSI9NZmZ\nwzNJSjAGpfdgZm4m04dlqLeQiEgjXTJBTBuawbShGdEOQ0SkU9OoKxERaZIShIiINEkJQkREmqQE\nISIiTep0CcLMzjSztWa23sxujXY8IiJdVadKEGaWCNwDnAWMBy43s/HRjUpEpGvqVAkCmAmsd/eN\n7l4NPAGcH+WYRES6pM6WIAYD2xptbw/KRESkg8XcQDkzuxa4NtgsNbO1bfyqfkBBi0fFF11z16Br\n7hoO55qHhXNQZ0sQO4CcRttDgrIG7n4/cP/h/iAzW+TuMw73e2KJrrlr0DV3DR1xzZ3tEdNCYLSZ\nDTezFOAy4NkoxyQi0iV1qhqEu9ea2Q3Ai0Ai8KC7r4xyWCIiXVKnShAA7j4XmNsBP+qwH1PFIF1z\n16Br7hoifs3m7pH+GSIiEoM6WxuEiIh0El0yQcTrdB5mlmNm881slZmtNLNvBOWZZvayma0L3jOC\ncjOzu4P/DsvNbHp0r6BtzCzRzJaa2XPB9nAzez+4rieDDg+YWbdge32wPzeacR8OM0s3s7+a2Roz\nW21ms+P5PpvZTcG/6Q/N7HEz6x6P99nMHjSzPDP7sFFZq++rmc0Jjl9nZnPaGk+XSxBxPp1HLXCz\nu48HZgHXB9d2KzDP3UcD84JtCP03GB28rgXu7fiQ28U3gNWNtn8G3OXuo4Ai4Oqg/GqgKCi/Kzgu\nVv0aeMHdxwFTCF1/XN5nMxsMfB2Y4e4TCXVguYz4vM8PAWd+oqxV99XMMoHvAUcTmp3ieweSSqu5\ne5d6AbOBFxtt3wbcFu24InSt/wROA9YCA4OygcDa4PN9wOWNjm84LlZehMbKzANOBp4DjNDgoaRP\n3m9CveNmB5+TguMs2tfQhmvuA2z6ZOzxep/5eIaFzOC+PQecEa/3GcgFPmzrfQUuB+5rVH7Qca15\ndbkaBF1kOo+gWj0NeB/Idvddwa7dQHbwOR7+W/wK+BZQH2z3Bfa5e22w3fiaGq432F8cHB9rhgP5\nwJ+CR2t/NLM04vQ+u/sO4JfAVmAXofu2mPi/zwe09r622/3uigki7plZT+BvwI3uvr/xPg/9SREX\nXdfM7Fwgz90XRzuWDpYETAfudfdpQBkfP3YA4u4+ZxCatHM4MAhI49OPYbqEjr6vXTFBtDidRywz\ns2RCyeFRd38mKN5jZgOD/QOBvKA81v9bHAucZ2abCc38ezKhZ/PpZnZgjE/ja2q43mB/H2BvRwbc\nTrYD2939/WD7r4QSRrze51OBTe6e7+41wDOE7n283+cDWntf2+1+d8UEEbfTeZiZAQ8Aq939zka7\nngUO9GSYQ6ht4kD5lUFviFlAcaOqbKfn7re5+xB3zyV0H1919y8A84GLgsM+eb0H/jtcFBwfc39l\nu/tuYJuZjQ2KTgFWEaf3mdCjpVlmlhr8Gz9wvXF9nxtp7X19ETjdzDKC2tfpQVnrRbtBJkqNQGcD\nHwEbgO9EO552vK7jCFU/lwMfBK+zCT1/nQesA14BMoPjjVCPrg3ACkK9RKJ+HW289hOB54LPI4AF\nwHrgaaBbUN492F4f7B8R7bgP43qnAouCe/0PICOe7zPwA2AN8CHwCNAtHu8z8DihdpYaQjXFq9ty\nX4EvB9e/HvhSW+PRSGoREWlSV3zEJCIiYVCCEBGRJilBiIhIk5QgRESkSUoQIiLSJCUIkVYys+8E\nM4suN7MPzOxoM7vRzFKjHZtIe1I3V5FWMLPZwJ3Aie5eZWb9gBTgHUL90AuiGqBIO1INQqR1BgIF\n7l4FECSEiwjNETTfzOYDmNnpZvaumS0xs6eD+bEws81m9nMzW2FmC8xsVLQuRKQlShAirfMSkGNm\nH5nZ78zsBHe/G9gJnOTuJwW1iv8BTnX36YRGPH+z0XcUu/sk4LeEZqMV6ZSSWj5ERA5w91IzOxI4\nHjgJeNI+vSrhLEKLUb0dmjqIFODdRvsfb/R+V2QjFmk7JQiRVnL3OuA14DUzW8HHE6kdYMDL7n55\nc1/RzGeRTkWPmERawczGmtnoRkVTgS1ACdArKHsPOPZA+4KZpZnZmEbnXNrovXHNQqRTUQ1CpHV6\nAr8xs3RCa4CvJ7Qe8OXAC2a2M2iHuAp43My6Bef9D6EZhAEyzGw5UBWcJ9IpqZurSAcKFjdSd1iJ\nCXrEJCIiTVINQkREmqQahIiINEkJQkREmqQEISIiTVKCEBGRJilBiIhIk5QgRESkSf8fL74cGf1P\nFCUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_zSz-cdJem4",
        "colab_type": "text"
      },
      "source": [
        "After training, let's run `play()` to see how the model works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQeGR5sOtnzZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4150
        },
        "outputId": "20fcd956-20b8-4a51-c8d2-8d21d7fb413e"
      },
      "source": [
        "if master is None:\n",
        "  master = MasterAgent()\n",
        "master.play()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading model from: /tmp/model_CartPole-v0.h5\n",
            "0. Reward: 1.0, action: 0\n",
            "1. Reward: 2.0, action: 1\n",
            "2. Reward: 3.0, action: 0\n",
            "3. Reward: 4.0, action: 1\n",
            "4. Reward: 5.0, action: 0\n",
            "5. Reward: 6.0, action: 1\n",
            "6. Reward: 7.0, action: 0\n",
            "7. Reward: 8.0, action: 1\n",
            "8. Reward: 9.0, action: 0\n",
            "9. Reward: 10.0, action: 1\n",
            "10. Reward: 11.0, action: 0\n",
            "11. Reward: 12.0, action: 1\n",
            "12. Reward: 13.0, action: 0\n",
            "13. Reward: 14.0, action: 1\n",
            "14. Reward: 15.0, action: 0\n",
            "15. Reward: 16.0, action: 1\n",
            "16. Reward: 17.0, action: 0\n",
            "17. Reward: 18.0, action: 1\n",
            "18. Reward: 19.0, action: 0\n",
            "19. Reward: 20.0, action: 1\n",
            "20. Reward: 21.0, action: 0\n",
            "21. Reward: 22.0, action: 1\n",
            "22. Reward: 23.0, action: 0\n",
            "23. Reward: 24.0, action: 1\n",
            "24. Reward: 25.0, action: 0\n",
            "25. Reward: 26.0, action: 1\n",
            "26. Reward: 27.0, action: 0\n",
            "27. Reward: 28.0, action: 0\n",
            "28. Reward: 29.0, action: 1\n",
            "29. Reward: 30.0, action: 0\n",
            "30. Reward: 31.0, action: 1\n",
            "31. Reward: 32.0, action: 0\n",
            "32. Reward: 33.0, action: 1\n",
            "33. Reward: 34.0, action: 0\n",
            "34. Reward: 35.0, action: 1\n",
            "35. Reward: 36.0, action: 1\n",
            "36. Reward: 37.0, action: 0\n",
            "37. Reward: 38.0, action: 1\n",
            "38. Reward: 39.0, action: 0\n",
            "39. Reward: 40.0, action: 1\n",
            "40. Reward: 41.0, action: 0\n",
            "41. Reward: 42.0, action: 0\n",
            "42. Reward: 43.0, action: 1\n",
            "43. Reward: 44.0, action: 1\n",
            "44. Reward: 45.0, action: 0\n",
            "45. Reward: 46.0, action: 1\n",
            "46. Reward: 47.0, action: 0\n",
            "47. Reward: 48.0, action: 1\n",
            "48. Reward: 49.0, action: 0\n",
            "49. Reward: 50.0, action: 0\n",
            "50. Reward: 51.0, action: 1\n",
            "51. Reward: 52.0, action: 1\n",
            "52. Reward: 53.0, action: 0\n",
            "53. Reward: 54.0, action: 1\n",
            "54. Reward: 55.0, action: 0\n",
            "55. Reward: 56.0, action: 0\n",
            "56. Reward: 57.0, action: 1\n",
            "57. Reward: 58.0, action: 1\n",
            "58. Reward: 59.0, action: 0\n",
            "59. Reward: 60.0, action: 0\n",
            "60. Reward: 61.0, action: 1\n",
            "61. Reward: 62.0, action: 1\n",
            "62. Reward: 63.0, action: 0\n",
            "63. Reward: 64.0, action: 1\n",
            "64. Reward: 65.0, action: 0\n",
            "65. Reward: 66.0, action: 1\n",
            "66. Reward: 67.0, action: 0\n",
            "67. Reward: 68.0, action: 1\n",
            "68. Reward: 69.0, action: 0\n",
            "69. Reward: 70.0, action: 0\n",
            "70. Reward: 71.0, action: 1\n",
            "71. Reward: 72.0, action: 0\n",
            "72. Reward: 73.0, action: 1\n",
            "73. Reward: 74.0, action: 1\n",
            "74. Reward: 75.0, action: 0\n",
            "75. Reward: 76.0, action: 1\n",
            "76. Reward: 77.0, action: 0\n",
            "77. Reward: 78.0, action: 1\n",
            "78. Reward: 79.0, action: 0\n",
            "79. Reward: 80.0, action: 0\n",
            "80. Reward: 81.0, action: 1\n",
            "81. Reward: 82.0, action: 1\n",
            "82. Reward: 83.0, action: 0\n",
            "83. Reward: 84.0, action: 0\n",
            "84. Reward: 85.0, action: 1\n",
            "85. Reward: 86.0, action: 1\n",
            "86. Reward: 87.0, action: 0\n",
            "87. Reward: 88.0, action: 1\n",
            "88. Reward: 89.0, action: 0\n",
            "89. Reward: 90.0, action: 1\n",
            "90. Reward: 91.0, action: 0\n",
            "91. Reward: 92.0, action: 0\n",
            "92. Reward: 93.0, action: 1\n",
            "93. Reward: 94.0, action: 0\n",
            "94. Reward: 95.0, action: 1\n",
            "95. Reward: 96.0, action: 1\n",
            "96. Reward: 97.0, action: 0\n",
            "97. Reward: 98.0, action: 1\n",
            "98. Reward: 99.0, action: 0\n",
            "99. Reward: 100.0, action: 1\n",
            "100. Reward: 101.0, action: 0\n",
            "101. Reward: 102.0, action: 1\n",
            "102. Reward: 103.0, action: 0\n",
            "103. Reward: 104.0, action: 0\n",
            "104. Reward: 105.0, action: 1\n",
            "105. Reward: 106.0, action: 0\n",
            "106. Reward: 107.0, action: 1\n",
            "107. Reward: 108.0, action: 1\n",
            "108. Reward: 109.0, action: 0\n",
            "109. Reward: 110.0, action: 1\n",
            "110. Reward: 111.0, action: 0\n",
            "111. Reward: 112.0, action: 0\n",
            "112. Reward: 113.0, action: 1\n",
            "113. Reward: 114.0, action: 1\n",
            "114. Reward: 115.0, action: 0\n",
            "115. Reward: 116.0, action: 0\n",
            "116. Reward: 117.0, action: 1\n",
            "117. Reward: 118.0, action: 1\n",
            "118. Reward: 119.0, action: 0\n",
            "119. Reward: 120.0, action: 1\n",
            "120. Reward: 121.0, action: 0\n",
            "121. Reward: 122.0, action: 0\n",
            "122. Reward: 123.0, action: 1\n",
            "123. Reward: 124.0, action: 1\n",
            "124. Reward: 125.0, action: 0\n",
            "125. Reward: 126.0, action: 0\n",
            "126. Reward: 127.0, action: 1\n",
            "127. Reward: 128.0, action: 1\n",
            "128. Reward: 129.0, action: 0\n",
            "129. Reward: 130.0, action: 1\n",
            "130. Reward: 131.0, action: 0\n",
            "131. Reward: 132.0, action: 0\n",
            "132. Reward: 133.0, action: 1\n",
            "133. Reward: 134.0, action: 1\n",
            "134. Reward: 135.0, action: 0\n",
            "135. Reward: 136.0, action: 0\n",
            "136. Reward: 137.0, action: 1\n",
            "137. Reward: 138.0, action: 1\n",
            "138. Reward: 139.0, action: 0\n",
            "139. Reward: 140.0, action: 1\n",
            "140. Reward: 141.0, action: 0\n",
            "141. Reward: 142.0, action: 0\n",
            "142. Reward: 143.0, action: 1\n",
            "143. Reward: 144.0, action: 1\n",
            "144. Reward: 145.0, action: 0\n",
            "145. Reward: 146.0, action: 0\n",
            "146. Reward: 147.0, action: 1\n",
            "147. Reward: 148.0, action: 1\n",
            "148. Reward: 149.0, action: 0\n",
            "149. Reward: 150.0, action: 0\n",
            "150. Reward: 151.0, action: 1\n",
            "151. Reward: 152.0, action: 1\n",
            "152. Reward: 153.0, action: 0\n",
            "153. Reward: 154.0, action: 1\n",
            "154. Reward: 155.0, action: 0\n",
            "155. Reward: 156.0, action: 0\n",
            "156. Reward: 157.0, action: 1\n",
            "157. Reward: 158.0, action: 0\n",
            "158. Reward: 159.0, action: 1\n",
            "159. Reward: 160.0, action: 1\n",
            "160. Reward: 161.0, action: 0\n",
            "161. Reward: 162.0, action: 1\n",
            "162. Reward: 163.0, action: 0\n",
            "163. Reward: 164.0, action: 0\n",
            "164. Reward: 165.0, action: 1\n",
            "165. Reward: 166.0, action: 1\n",
            "166. Reward: 167.0, action: 0\n",
            "167. Reward: 168.0, action: 1\n",
            "168. Reward: 169.0, action: 0\n",
            "169. Reward: 170.0, action: 0\n",
            "170. Reward: 171.0, action: 1\n",
            "171. Reward: 172.0, action: 0\n",
            "172. Reward: 173.0, action: 1\n",
            "173. Reward: 174.0, action: 1\n",
            "174. Reward: 175.0, action: 0\n",
            "175. Reward: 176.0, action: 0\n",
            "176. Reward: 177.0, action: 1\n",
            "177. Reward: 178.0, action: 1\n",
            "178. Reward: 179.0, action: 0\n",
            "179. Reward: 180.0, action: 1\n",
            "180. Reward: 181.0, action: 0\n",
            "181. Reward: 182.0, action: 0\n",
            "182. Reward: 183.0, action: 1\n",
            "183. Reward: 184.0, action: 0\n",
            "184. Reward: 185.0, action: 1\n",
            "185. Reward: 186.0, action: 1\n",
            "186. Reward: 187.0, action: 0\n",
            "187. Reward: 188.0, action: 1\n",
            "188. Reward: 189.0, action: 0\n",
            "189. Reward: 190.0, action: 0\n",
            "190. Reward: 191.0, action: 1\n",
            "191. Reward: 192.0, action: 0\n",
            "192. Reward: 193.0, action: 1\n",
            "193. Reward: 194.0, action: 1\n",
            "194. Reward: 195.0, action: 0\n",
            "195. Reward: 196.0, action: 1\n",
            "196. Reward: 197.0, action: 0\n",
            "197. Reward: 198.0, action: 0\n",
            "198. Reward: 199.0, action: 1\n",
            "199. Reward: 200.0, action: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAr0AU4DQaDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}